<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Dabble</title>
      <link href="posts/c1e1.html"/>
      <url>posts/c1e1.html</url>
      
        <content type="html"><![CDATA[<h2 id="思想回顾"><a href="#思想回顾" class="headerlink" title="思想回顾"></a>思想回顾</h2><h3 id="模型的概述"><a href="#模型的概述" class="headerlink" title="模型的概述"></a>模型的概述</h3><p>前提假设：只读工作负载。即不支持插入、更新、删除工作负载。针对的也是一维有序数据集。</p><img src="/posts/模型对比图.png" alt="模型对比图" style="zoom:50%;"><p>B树 索引最终寻到磁盘的某个块，因为数据是以磁盘块为单位存储的。要找记录的最终位置，可能需要遍历磁盘页，如果块内存储有序，则使用偏移量即可定位到记录实际位置。</p><p>近似的，当把索引看成模型后，由于机器学习模型预测的往往不精确，因此需要在训练时记录模型的最大、最小误差，最终在误差范围内采用二分查找确定记录的准确位置。</p><hr><h3 id="层次结构模型-RMI"><a href="#层次结构模型-RMI" class="headerlink" title="层次结构模型-RMI"></a>层次结构模型-RMI</h3><p><strong>层次：下一层各模型带训练的数据集取决于对上一层模型的预测值的划分规则</strong>。</p><p><strong>相较于 B树 与 B+树 采用 if 语句 对数据集进行划分，RMI 模型采用 ML与区间划分方式均等划分数据集</strong>。</p><img src="/posts/模型示例图.png" alt="模型示例图" style="zoom:50%;"><p>如：第1层给出一个0~N的预测，假设第2层有K个模型，则第1层预测在范围<code> (0,N/K)</code> 的记录将交给 第2层 的模型1 进一步预测，落在 <code>(N/K+1,2×N/K)</code> 的记录将 由第2层的模型2 进一步预测，以此类推。</p><p>类似B树，RMI 的每层模型将会进一步缩小搜索范围，<code>每一层被选择的模型接受相同键值的输入</code>，<code>并根据输出选择下一层的模型</code>，直到最后一层给出记录位置的最终预测（通常还会根据最大误差进行二分搜索来确定记录的精确位置）。</p><hr><h2 id="Dabble-模型"><a href="#Dabble-模型" class="headerlink" title="Dabble 模型"></a>Dabble 模型</h2><p><strong>前提假设：单维数据索引，即数据集为内存中的有序数组</strong>。</p><img src="/posts/Dabble模型示意图.png" alt="Dabble模型示意图" style="zoom: 50%;"><p><strong>模型四部分</strong>：</p><ol><li>首先，利用聚类方法对数据集进行预处理，通过 K-Means 聚类，将数据集按照其分布规律划分为不相交的区域；</li><li>初始化 K个 神经网络模型，分别对上一步得到的 K个数据区域进行训练，得到 K个学习索引模型；</li><li>当数据查询时，通过模型选择器选出对应的模型，并预测出键对应的记录位置，并通过二分查找在误差范围内找到最终的位置；</li><li>插入新的数据时，数据首先放入一个 B 树的缓存中，当缓存达到一定的阈值后，将缓存中的数据一次性归并到数据集中，并对相应的模型进行重新训练。</li></ol><p><strong>具体的</strong>：对于每一个数据区域，分别利用神经网络对其进行训练，使网络能够对该数据区域的分布得到一个比较好的拟合【过拟合】；在神经网络训练阶段，重点关注访问频率高的热点数据，从而使神经网络对这些数据的预测精度更高。</p><p>针对数据插入问题，借鉴  LSM 树中的延迟更新机制，在内存中开辟一块缓存用来存放新插入的数据，当缓存溢出时，一次性将数据进行插入；针对索引更新问题，通过模型解耦的方式缓解索引更新带来的消耗。</p><p>当数据插入时，只需要重新训练有数据插入的那个数据区域对应的模型即可，不需要对整个 Dabble 模型重新训练，从而提高了模型的可扩展性。</p><hr><h4 id="数据空间划分"><a href="#数据空间划分" class="headerlink" title="数据空间划分"></a>数据空间划分</h4><p>目的：使用 ML 模型尽可能学习到数据分布特点，从而充分拟合数据使得查的既快又准，内存占用还少。</p><p>客观事实：现实世界中，数据分布较为复杂，很难用通用的模型较好地学习到数据分布特点。</p><img src="/posts/数据集分布图.png" alt="数据集分布图" style="zoom:50%;"><p>图 4(a)展示了一个日志记录数据集的键-值分布，其中，键(key)是每一条记录的生成时间，即时间戳<br>(timestamp)；值(value)代表了每一条记录的存储位置。通过图中可以看到：键的分布是有区域性的，反映在日志记<br>录数据集上可以理解为对数据集的访问具有时间属性，<strong>在不同时间段数据的访问频率是不一样的</strong>；其次，从宏观<br>角度看，图 4(a)中的数据分布是比较杂乱的，呈现出一种非线性特点。但是，把图4(a)中的某一局部信息放大来看， 可以发现局部区域数据呈现较好的线性分布，因此最直观的想法是利用简单函数对每一个区域分段学习拟合。但是，<strong>如果不对数据进行预处理</strong>，直接对整个数据集进行学习，训练出的模型预测精度往往比较低。</p><p>图 4(b)中展示了经过聚类算法处理后得到的数据划分：整个数据集按照分布特点划分成了K个区域{D<del>1</del>，D<del>2</del>，…，D<del>K</del>}，数据区域间不相交且大小有序，即 <strong>第 i+1 个数据区域中的数据比第 i 个数据区域中的数据都要大</strong>。</p><p>假设训练数据集是 {x^(1)^，x^(2)^，…，x^(n)^}，x^(i)^ 代表了第 i 个数据记录，c^(i)^代表了训练数据 i 与 K 个类中最近的那个类，质心 u<del>j</del> 代表了每一个簇中样本的中心点。输入的应该是<code>&lt;key,position&gt;</code> 。</p><p>算法1：数据集聚类过程。</p><p>初始化：随机选取 K 个数据点作为聚类之心${u_{1},u_{2},\ldots,u_{K}}$。</p><p>输出：K 个数据区域。 </p><blockquote><ol><li>重复以下过程，直至模型训练收敛 { </li><li>对于每一个数据 i，计算其属于的类 c^(i)^:</li><li>$c^{(i)}=\arg\min_j\parallel x^{(i)}-u_j\parallel^2;$</li><li>对于每一个类 j，重新计算该类的质心：</li><li>$u_j=\frac{\sum_{i=1}^K1{c^{(i)}=j}x^{(i)}}{\sum_{i=1}^K1{c^{(i)}=j}};$  ===&gt; $u_j=\frac{\sum_{i=1}^n1{c^{(i)}=j}x^{(i)}}{\sum_{i=1}^n1{c^{(i)}=j}};$  </li></ol><p>}</p></blockquote><p>存在的制约关系：K 值越大，会使神经网络模型对应的数据区域越小，数据分布越一致，模型拟合效果越好。但是 K 值过大会使模型的数目过多，从而使模型选择器在选择某个数据查询对应的模型时花费时间过多。</p><hr><h4 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h4><p>数据集应已被划分为：K个不相交的数据区域，每个数据区域内数据分布近似一致。</p><p>采用神经网络模型分别对各自的数据区域进行训练。</p><p>模型的输入：数据记录的 key；对应的训练标签：该 key 的 position。</p><img src="/posts/Dabble模型详细图.png" alt="Dabble模型详细图" style="zoom:50%;"><p>如图5 所示，模型的输入与输出均为一维的标量类型。其中，神经网络模型采用两层架构，即：在输入层与输出层中间只加入一层隐藏层的神经元，并且在隐藏层后面加一层 ReLU 激活函数用来做非线性变换。【由于数据的局部区域分布依然可能存在非线性的特征，因此需要引入激活函数来学习非线性特征】</p><p>最终目标：模型预测出来的位置与数据记录对应的位置尽可能接近【过拟合】。</p><p>结合客户端对数据的访问请求往往呈现某种规律，如齐夫定律(Zipf’s law)。引入热点数据访问。</p><p>定义损失函数如下：$L_i=\sum_{(x,y)}\lambda_x\cdot(f_i(x)-y)^2$，其中，x、y 分别代表训练数据的 key 以及对应的存储位置，f<del>i</del>(x) 代表第 i 个训练模型 (i 取值为1<del>K)， L</del>i~ 为第 i 个模型的损失函数。$\lambda_x$ 代表了单个数据键 x 的访问热度，每一个数据键 x 对应的 $\lambda_x$ 值不同， 该值可以通过对 x 的历史访问数据计算得出。【类似词频统计】</p><p>【相当于，在计算损失时，预测值与真实值之差的平方又分别乘了个权重参数】</p><p>当 K个神经网络模型 f<del>i</del>(x)，1&lt;=i&lt;=K 训练完成后，每一个模型都会对应一个误差值 (MaxError)，这个值代表了 模型 f<del>i</del>(x) 在第i个数据区域最大的预测误差。</p><p>$\mathrm{MaxError}<em>i=\max</em>{x\in D_i}|f_i(x)-y(x)|$ </p><p>其中：</p><ul><li>f<del>i</del>(x) 是第 i 个神经网络模型对输入 x 的预测输出；</li><li>y(x) 是输入 x 对应的真实标签或值；</li><li>D<del>i</del>  是第 i 个数据区域，其中包含了用于评估模型性能的数据样本集；</li><li>MaxError<del>i</del>  是在第 i 个数据区域上模型 f<del>i</del>(x) 的最大预测误差。</li></ul><hr><h4 id="数据查询"><a href="#数据查询" class="headerlink" title="数据查询"></a>数据查询</h4><p>经过聚类得到的 K个数据区域之间大小有序，并且每个区域都有对应的键值范围(MinKey， MaxKey)。</p><p>当一个查询到来时，首先通过模型选择器比较判断键的大小【key与区域键值范围进行比较】，从而找到该查询对应的索引模型 f<del>i</del>(x)，然后利用 f<del>i</del>(x) 计算出该数据记录所对应的存储位置 y。此时得到的位置是一个近似位置，可用二分查找算法，在 [y−MaxError，y+MaxError] 范围内查找数据的最终位置，从而完成查询操作。</p><p>另外，为了提高查询效率，<strong>Dabble 模型会在服务器端缓存(cache) 近期访问的数据以及热点数据</strong>。所以每当查询到来，首先查询系统缓存：如果数据命中，则直接返回结果；否则，访问 Dabble索引进行数据的查询。</p><p>为了防止缓存与真实数据的不一致性，可采用租赁机制。在租赁有效期间，缓存数据有效，且该缓存对应的数据更新被锁定【不可更新】；租赁失效后，如果想要重新缓存该部分数据，需要重新向系统发出租赁申请。</p><hr><h4 id="索引更新"><a href="#索引更新" class="headerlink" title="索引更新"></a>索引更新</h4><p>学习型索引存在的问题：新数据的不断插入，会引起数据分布的改变。在这种情况下，之前训练的神经网络模型将出现预测位置偏移的情况，并且会随着数据分布改变的程度不断加大。因此，朴素的想法是在新数据集上重新训练模型，但这个耗费是非常大的。</p><h5 id="模型解耦"><a href="#模型解耦" class="headerlink" title="模型解耦"></a>模型解耦</h5><p>当新查询到来的时候，经过模型选择器选择模型 f<del>i</del>(x)，计算得到 键x 对应的位置，并在误差范围内进行查找，从而获得最终的数据。在数据预处理阶段，我们通过聚类方法把数据集分割成 K个区域， 那么能不能在数据插入的时候，只更新当前的模型 f<del>i</del>(x)？这样，其余K−1个模型可保持不变，从而将更新带来的消耗尽可能降低。</p><p>受到程序设计领域模型解耦思想的启发，我们可以在模型的预测部分加一层中间层，使模型之间相互独立，<br>这样每个模型预测出来的位置变为这个键在该模型对应的数据区域的位置。在这种情况下，如图 5 所示，模型<br>f<del>i</del>(x) 的输出值 y 代表的是键 x 在第 i 个数据区域 D<del>i</del> 中的位置。令 |D<del>i</del>| 代表第 i 个数据区域的大小，那么键 x 真实的<br>预测位置可以通过如下计算得到：$pos=\sum_{t=1}^{i-1}\mid D_t\mid+y\text{ ,其中},\sum_{t=1}^{i-1}\mid D_t\mid $ 代表了前 i−1 个数据区域包含的数据量，该累计值再加上 y，得到键 x 真正的预测存储位置。通过这种方式，我们只需在聚类以及后续的索引更新过程中记录下每个数据区域的大小，即可方便地通过中间层得到查询的预测位置。可以看到：通过加了一层中间层，模型 f<del>i</del>(x) 由预测键 x 在整个数据集中的位置变为预测在 区域|D<del>i</del>|中 的相对位置。</p><p>在这种情况下，新插入的数据只会影响数据区域 |D<del>i</del>| 中的数据分布，其他模型依然可以通过预测出来的数据键的相对位置加上位于前面的所有数据区域的大小得到数据键的真正位置。模型之间互不影响，相互独立，为后续的数据插入过程提供了高效的解决方法。另外，这里提到的中间层只是一个虚拟上的概念，并没有真正改变数据的存储位置，因此不会影响数据的分布。与 ASLM[24]、AIDEL[23]不同，Dabble 模型最终输出的 pos 代表在整个数据集的真实位置。因此，Dabble 模型既适用于存储在连续内存中的数据集，也可应用于将数据区域分块存储的场景。</p><p>个人理解模型解耦：输入 key 不变，对应的标签 position 变为 相对位置【实际区域区间数组下标】。</p><p>模型最终的二分搜索区间：[pos-MaxError,pos+MaxError]。会越界相对数据区域，但实际数据还是连续存储。</p><hr><h5 id="数据更新"><a href="#数据更新" class="headerlink" title="数据更新"></a>数据更新</h5><p>通过模型解耦，模型 f<del>i</del>(x)，1≤i≤K 之间相互独立，当插入数据的时候，只需要重新训练对应的神经网络模型即可，而不需要在全部数据集上进行训练。但是，如果插入操作比较多，对模型频繁地重新训练将会带来非常大的代价，严重影响系统的响应速度。【<strong>为一个值的插入就重新训练模型肯定很亏，所以得积攒一点—缓存积攒</strong>】</p><p>借鉴 LSM Tree 思想：首先，在内存中开辟一块缓存(buffer)，并将其分为K个部分，作为K 个模型的缓冲区。当有新数据插入时，首先通过模型选择器获得新数据应该存储的数据区域，然后直接将该条数据放入到对应的缓存块中。当某个缓存块达到阈值时，将其中的数据归并到真正的数据区域中，并对相应的模型  f<del>i</del>(x) 重新训练，使其能够拟合当前的数据分布。</p><p>在这种情况下，当某个数据区域由于数据分布过于复杂， 导致一个神经网络模型无法较好地学习时，对该数据区域进行分裂操作，此时，整个数据集合变为 K+1 个区域。【如何判定？损失过大，大于给定阈值？】</p><p>注意：如第2.1节提到的那样，把数据集划分为区域，只是逻辑上的行为，数据集依旧是作为一个整体存储。</p><p>引入数据更新机制后，对于数据查询请求，需要首先在第2.3 节中提到的系统缓存进行查找，如果失败，则在数据缓存块中进行查找：如果查找成功，则直接返回结果；否则，使用 Dabble 模型进行查找。为了提高在缓存中的查询速度，缓存中的数据可以按照B树方式进行组织。</p><hr><h4 id="实验结论"><a href="#实验结论" class="headerlink" title="实验结论"></a>实验结论</h4><p>K 值越大，模型收敛的速度越快。原因在于：K 值越大，每一个数据区域所包含的数据记<br>录越少，因此模型在小的数据规模上学习速度更快。</p><p>K 值决定了数据集划分的个数，显然，K 值越大，数据集划分的越细，从而单个神经网络模型在对应的数据区域上学习效果越好，预测越精准。虽然 K 值越大使神经网络学习效果越好，但是当 K 过大时，会由于模型数目过多，使得模型选择器在选择某个查询对应的模型时花费的时间增加【应该是线性判断的，所以个数多就慢了】。因此，对于某个特定数据集，存在某个 K 值使模型性能达到最优。另外，数据集的数据分布不同，数据规模大小不一样，最优性能对应的 K 值会不同。</p><hr><p>================================================================================================================================================================</p><p>创新想法：数据分布划分—-使用Zorder空间填充曲线进行编码分区—上下限不好抓取。</p><p>================================================================================================================================================================</p><p>分完堆就可以采用  神经网络进行拟合了。。</p><p>此处的 缓存：可以使用 并发的 BlinkHash 维护———–数据量有限，不太适合，再想想。</p><p>================================================================================================================================================================</p><p>如果不分堆，现有的方法有：分段线性拟合。分段有的采用贪心算法实现数据集的划分。</p><p>================================================================================================================================================================</p><p>优化点：数据集划分、模型架构、并发结构、多维数据、支持插入、支持更新删除。</p><p>================================================================================================================================================================</p><hr>]]></content>
      
      
      <categories>
          
          <category> 学习型索引 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习型索引 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LearnedIndex-03</title>
      <link href="posts/4082.html"/>
      <url>posts/4082.html</url>
      
        <content type="html"><![CDATA[<h1 id="数据库中的-B-树与-B-树"><a href="#数据库中的-B-树与-B-树" class="headerlink" title="数据库中的 B 树与 B+ 树"></a>数据库中的 B 树与 B+ 树</h1><p><strong>参考资料</strong>：</p><ul><li><a href="https://www.youtube.com/watch?v=aZjYr87r1b8">https://www.youtube.com/watch?v=aZjYr87r1b8</a></li></ul><h2 id="1-主要内容"><a href="#1-主要内容" class="headerlink" title="1. 主要内容"></a>1. 主要内容</h2><p>B 树与 B+ 树是一类非常重要的数据结构，它们被广泛应用于各种数据库中。为了更好地理解这类数据结构，我们将逐一讨论以下内容：</p><ol><li>磁盘结构</li><li>数据是如何存储在磁盘上的</li><li>什么是索引</li><li>什么是多级索引</li><li>多路查找树</li><li>B 树</li><li>B 树中的插入与删除</li><li>B+ 树</li></ol><h2 id="2-磁盘结构"><a href="#2-磁盘结构" class="headerlink" title="2. 磁盘结构"></a>2. 磁盘结构</h2><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-01-02-WX20210102-105134%402x.png" alt="img"></p><p>我们来看一下 <strong>磁盘 (disk)</strong> 的结构：一个典型的磁盘驱动器由一个或多个 <strong>盘片 (platter)</strong> 组成，它们以一个固定的速度围绕一个共同的 <strong>主轴 (spindle)</strong> 旋转。每个盘片表面覆盖着一层可磁化的物质。驱动器通过 <strong>磁臂 (arm)</strong> 末尾的 <strong>磁头 (head)</strong> 来读/写盘片。</p><p>盘片在 <strong>逻辑上 (而非物理上)</strong> 被划分为一系列的同心环状区域，数据就存储在这样的同心圆环上面，这些同心圆环被称为 **磁道 (track)**。每个盘面可以划分多个磁道，最外圈的磁道是 0 号磁道，向圆心增长依次为 1 号磁道、2 号磁道……磁盘的数据存放就是从最外圈开始的。</p><p>根据硬盘的规格不同，磁道数可以从几百到成千上万不等。每个磁道可以存储几个 Kb 的数据，但是计算机不必要每次都读写这么多数据。因此，再把每个磁道划分为若干个弧段，每个弧段就是一个 **扇区 (sector)**。</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-01-02-WX20210102-131058%402x.png" alt="img"></p><p>一个盘片被划分为许多磁道和扇区，一个磁道和一个扇区相交的区域称为一个 **块 (block)**。因此，磁盘上的任意一个块都可以通过其对应的磁道编号和扇区编号来寻址，也就是说，磁盘上的块地址格式由磁道编号和扇区编号组成：</p><p>块地址 = (磁道编号, 扇区编号)</p><p>块是硬盘上存储的物理单位。出于稳定性考虑，通常一个块存储 512 字节的数据，但是实际上其容量可以是任意大小，具体取决于磁盘制造商和磁盘型号。</p><p>这里，我们假设每个块的容量为 512 字节。当我们从磁盘上读取或写入数据时，我们总是以块为单位进行读/写。如果现在我们读取一个 512 字节的块，假设其中第一个字节的地址为 0，最后一个字节的地址为 511，那么其中每个字节都有其各自的地址，我们称之为 **偏移量 (offset)**。</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-01-02-WX20210102-121031%402x.png" alt="img"></p><p>假设磁盘上的每个块的第一个和最后一个字节的偏移量都分别为 0 和 511。因此，我们只需要知道 <strong>磁道编号</strong>、<strong>扇区编号</strong> 和 <strong>偏移量</strong> 这三个信息就可以定位到磁盘上的任意一个字节：首先，利用磁道编号和扇区编号定位到该字节所在的块；然后，在块内通过偏移量定位到该字节。</p><p>正常情况下，我们可以通过盘片的旋转来选择扇区，通过磁头的轴向移动来选择磁道，也就是说，我们可以通过旋转盘片和移动磁头来定位到某个块，而数据总是以块的形式存储在磁盘上的。</p><p>现在，让我们来看一下内存，或者说 RAM (注：RAM 是一种存储技术，其类型为主存)，我们所有的程序都运行在内存中。假设我们的程序需要访问磁盘上的数据，我们需要将这些数据从磁盘上读取到内存中，然后我们的程序才能对其进行访问，而一旦我们的程序得到了某些结果，我们再将这些结果写入到磁盘。所以，数据处理无法直接在磁盘上进行，数据需要被读入内存中然后才能被程序访问。</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-01-02-WX20210102-141040%402x.png" alt="img"></p><p>内存中的数据可以被程序直接访问，我们将其称为 **数据结构 (data structure)**。而在磁盘上高效组织数据使得其能够以一种简单方式被利用的系统被称为 **数据库管理系统 (DBMS)**。</p><h2 id="3-数据是如何存储在磁盘上的"><a href="#3-数据是如何存储在磁盘上的" class="headerlink" title="3. 数据是如何存储在磁盘上的"></a>3. 数据是如何存储在磁盘上的</h2><p>现在，我们来看一下磁盘上的数据是如何以数据库的形式被组织起来的。</p><p>例子：这里我们有一个数据库表，其中包含一些 <strong>列 (columns)</strong> 和 **行 (rows)**，假设这里我们有 100 行数据。表的结构如左图所示，一共有 5 列属性，总的大小为 128 字节，即表中的每一行大小为 128 字节。</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-01-02-WX20210102-140156%402x.png" alt="img"></p><p>现在我们希望将该表存储在磁盘上，每个块中可以存储的记录条数为：</p><p>number of records/block=512128=4</p><p>那么存储总共 100 条记录需要的块的数量为：</p><p>number of blocks for database table=1004=25</p><p>所以，在磁盘上存储该数据库表总共需要 25 个块。</p><p>现在，假设我们已经将该数据库表存储在了磁盘上的 25 个块中。想象一下，现在我们希望写一条查询语句来查询其中某条特定记录，所需要的搜索次数是多少？因为所查询的记录可能在表中的任意位置，所以我们可能需要搜索整个数据库表，总共需要访问磁盘上最多 25 个块。我们当然可以一个块一个块地逐一访问并检查其中是否包含所查询的记录，但这样可能需要访问多达 25 个块。</p><h2 id="4-索引"><a href="#4-索引" class="headerlink" title="4. 索引"></a>4. 索引</h2><p>我们可以缩减这种查询的时间开销吗？</p><p>答案是可以。现在我们将为该数据库表构建 **索引 (index)**。在索引中，我们将存储一个 **键 (key)**，即员工 id，以及一个指向相应记录的 **指针 (pointer)**。因此，对于磁盘上的块中的每条记录，我们都有一个指向该记录的指针，我们称之为 **记录指针 (record pointer)**。每条记录在索引中都有一个对应的 **条目 (entry)**，我们将这种索引称为 **密集索引 (tense index)**。</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-01-02-WX20210102-144054%402x.png" alt="img"></p><p>那么，索引存储在哪里呢？</p><p>同样，我们将索引也存储在磁盘上。我们假设每个 id 大小为 10 个字节，每个指针大小为 6 个字节，因此索引中的每个条目大小为 16 字节。所以，每个块可存储的索引条目数量为：</p><p>number of entries/block=51216=32</p><p>那么，存储 100 条记录的索引所需要的块的数量为：</p><p>number of blocks for index=10032=3.125</p><p>所以我们总共需要 4 个块来存储这 100 条记录的索引。</p><p>现在，当我们需要搜索任意一条记录时，我们可以直接在索引中搜索，因此查找该记录对应的索引条目最多需要访问 4 个块。当我们在索引中找到该记录的 key 后，我们就可以直接访问其记录所在的块。所以，现在查询任意一条记录最多需要访问 4 个索引块加上 1 个数据库表块，总共只需要访问 5 个块。这就是构建索引的好处。</p><h2 id="5-多级索引"><a href="#5-多级索引" class="headerlink" title="5. 多级索引"></a>5. 多级索引</h2><p>现在，假设我们的记录不是 100 条，而是 1000 条，这意味着数据库表的存储需要 250 个块，而相应地，索引存储需要 32 个块。这样一来，索引内的搜索开销也就大大增加了。</p><p>那么，我们可以在索引之上再构建一层索引吗？</p><p>答案是可以。我们应该如何构建二级索引的条目呢？仍然和之前一样为一级索引的每一个条目构建一条索引吗？并不是，我们知道对于一级索引，每个块可以存储 32 个条目，所以我们可以每隔 32 条记录构建一个二级索引条目，我们将这种索引称为 **稀疏索引 (sparse index)**。</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-01-02-WX20210102-151634%402x.png" alt="img"></p><p>我们知道索引中的每个条目大小为 16 字节，每个块可存储的索引条目数量为 32。而二级索引一共的条目数量为：</p><p>number of entries for 2nd index=100032=31.25</p><p>所以，二级索引的条目数量为 32，存储所需的块数量为：</p><p>number of blocks for 2nd index=3232=1</p><p>所以，二级索引的存储只需要 1 个块。</p><p>因此，整个数据库表存储需要 250 个块，一级索引存储需要 32 个块，二级索引存储仅需 1 个块。所以现在查询任意一条记录总共需要访问 1 个二级索引块，加上 1 个一级索引块，加上一个数据库表存储块，总共只需要访问 3 个块。所以，通过增加一级索引，我们大大减少了块访问的次数，这也是 B 树和 B+ 树的基本思想起源。</p><p>假设我们的数据量很大，我们需要构建多级索引以减少磁盘上的块访问次数：</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-01-02-WX20210102-223736%402x.png" alt="img"></p><p>我们可以将上面的结构旋转一下会发现其类似树形结构：</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-01-02-WX20210102-224633%402x.png" alt="img"></p><p>那么，多级索引的要求是什么呢？我们应该手动添加索引吗？我们应该检查数据库大小的增长然后一级一级增加索引吗？</p><p>不是。我们希望这些高层级的索引可以被自动地添加或者删除：当有大量数据插入使得数据库表越来越大时，我们希望索引级数可以自动增加；而当我们删除大量记录使得数据库表减小后，我们希望索引级数可以自动减少。简而言之，我们希望自动管理那些高层级索引，即我们希望能够实现自我管理功能的多级索引，这也为 B 树和 B+ 树提供了思路。实际上，B 树和 B+ 树来源于多路查找树，所以接下来我们将讨论多路查找树，然后再讨论 B 树和 B+ 树。</p><h2 id="6-多路查找树"><a href="#6-多路查找树" class="headerlink" title="6. 多路查找树"></a>6. 多路查找树</h2><p>在讨论多路查找树之前，我们先回忆一下二叉查找树：</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-01-03-WX20210103-102927%402x.png" alt="img"></p><p><strong>二叉查找树 (Binary search tree, BST)</strong> 是指一棵空树或者具有下列性质的二叉树：</p><ol><li>若任意结点的左子树不空，则左子树上所有结点的值均小于它的根结点的值；</li><li>若任意结点的右子树不空，则右子树上所有结点的值均大于或等于它的根结点的值；</li><li>任意结点的左、右子树也分别为二叉查找树。</li></ol><p>在二叉查找树中，当我们想要在二叉查找树中查找任何元素时，我们将从根结点出发，如果查找目标比当前结点值小，则继续沿着其左子树进行查找；如果查找目标比当前结点值大，则继续沿着其右子树进行查找。</p><p>在二叉查找树中，每个结点只有 1 个 key，并且最多只有 2 个子结点。那么，我们可以构造一棵树使得每个结点有多个 key 和多个子结点吗？答案是可以，我们可以构造一棵下图所示的树：</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-01-03-WX20210103-111450%402x.png" alt="img"></p><p>显然，这样一棵树的查找效率要高于普通的二叉查找树，但是其搜索方式和普通的二叉查找树是相同的，我们称之为 **多路查找树 (multi-way search tree, �-way ST)**。在上图的例子中，每个结点有 2 个 key，并且最多可以有 3 个子结点，所以这是一棵 3 路查找树。对于一个 � 路查找树，每个结点最多可以有 � 个子结点，并且每个结点包含 �−1 个 key。所以，� 路查找树是二叉查找树的一种扩展，或者说二叉查找树是 � 路查找树的一个特例。</p><p>现在我们来看一下 � 路查找树的结点。对于一棵二叉查找树，其结点结构如下：</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-01-03-WX20210103-113742%402x.png" alt="img"></p><p>对于一棵 � 路查找树，这里以 4 路查找树为例，每个结点可以包含 3 个 key 和最多 4 个子结点，其结构如下：</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-01-03-WX20210103-114106%402x.png" alt="img"></p><p>下面我们尝试利用 � 路查找树的结点结构来构建索引，回忆一下，索引结构如下：</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-01-03-WX20210103-115704%402x.png" alt="img"></p><p>结合 � 路查找树的特点，我们可以构建以下结点结构：</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-01-03-WX20210103-115900%402x.png" alt="img"></p><p>对于多级索引结构，我们可以为其构建一棵 � 路查找树，其中结点结构如上图所示：每个 key 后面紧跟一个指向对应的数据库记录的指针。</p><p>现在思考一下，如果我们只是像这样简单地利用 � 路查找树构建索引，会存在什么问题？我们来看一下在 � 路查找树中，新数据的插入操作是如何完成的，以及它存在什么问题。</p><p>我们以 10 路查找树为例，即每个结点可以有最多 10 个子结点，并且每个结点中包含 9 个 key。假设现在我们需要对一棵空树插入一些新的 key。</p><p>Keys: 10,20,30</p><p>首先是 10，我们先创建一个根结点，并且将 10 插入其中：</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-01-05-045034.png" alt="img"></p><p>然后，对于根结点，我们还可以在其中插入最多 8 个 key。但是，对于 20，我们这里选择创建一个新的子结点，并将其插入其中：</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-01-05-045511.png" alt="img"></p><p>然后，对于 30，我们这里还是选择创建一个新的子结点，并将其插入其中：</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-01-05-045808.png" alt="img"></p><p>因此，我们可以按照上面的方式完成三个 key 的插入，但这显然是有问题的。我们应该先填满一个结点，然后再创建下一个结点。但是，� 路查找树并没有任何机制可以保证这点，我们可以以任意方式完成插入操作。这意味着，如果我们一共有 � 个待插入的 key，那么 � 路查找树的高度可能会达到 �，这会使得 � 路查找树的时间复杂度退化为线性时间复杂度 �(�)，这就是 � 路查找树存在的主要问题：<strong>树的创建过程不可控，可能会退化为效率低下的线性查找</strong>。</p><h2 id="7-B-树"><a href="#7-B-树" class="headerlink" title="7. B 树"></a>7. B 树</h2><p>所以，我们需要在 � 路查找树的创建过程中引入一些机制来防止上述情况的发生，这就是 B 树。实际上，B 树就是附加了一些规则的 � 路查找树。我们来看一下 B 树具体引入了哪些规则：</p><ol><li><strong>每个结点必须填满至少一半的 key。对于 M 路查找树，每个结点要至少包含 ⌈�/2⌉ 个 key</strong>。例如，对于 10 路查找树，每个结点至少需要包含 5 个 key，然后我们才会考虑创建新的结点。这条规则可以帮助我们控制树的高度。</li><li><strong>根结点可以有至少 2 个子结点</strong>。</li><li><strong>所有的叶子结点必须在同一层</strong>。</li><li><strong>树的创建过程是由底向上的</strong>。</li></ol><p>当我们按照以上规则构建 � 路查找树时，我们实际上构建的就是 B 树。</p><p>下面我们以一些 key 为例来由底向上地构建 B 树：</p><ul><li>�=4</li><li>Keys: 10,20,40,50</li></ul><p>首先，我们创建一个根结点，并将 10 插入其中：</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-01-05-064238.png" alt="img"></p><p>它可以有最少 2 个子结点。并且，在之前的多级索引中我们提到过，每个 key 后面可以有一个记录指针。然后，我们在根结点中插入 20：</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-01-05-064216.png" alt="img"></p><p>然后我们在根结点中继续插入 40：</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-01-05-064139.png" alt="img"></p><p>接下来是 50，但是根结点已经满了，没有剩余空间留给这个 key 了。所以，我们需要分裂该结点，在新结点中插入 50，并将 40 取出并上升一层，将其放入新的根结点中：</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-01-05-064651.png" alt="img"></p><p>所以，当某个结点已经装满 key 时，如果此时有新的 key 需要插入，我们将创建一个新的结点并将新的 key 插入其中。同时对满 key 结点进行分裂，取出其中的一个 key 并将其放进新的根结点中。这就是在 B 树创建过程中的结点分裂过程，可以看到，这种方式将由底向上地构建 B 树。</p><p>B 树非常适合用于实现多级索引结构，它可以根据 key 的数量自动构建高层级的索引。对于上面的例子，我们可以继续插入一些新的 key，来观察 B 树是如何自动构建多级索引的。</p><p>Keys: 10,20,40,50,60,70,80,30,35,5,15</p><p>继续插入 60：</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-01-05-065627.png" alt="img"></p><p>继续插入 70：</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-01-05-065745.png" alt="img"></p><p>继续插入 80，由于右子结点已满，我们需要分裂右子结点，在新结点中插入 80，并将 70 取出并放入根结点中：</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-01-05-070203.png" alt="img"></p><p>继续插入 30：</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-01-05-070413.png" alt="img"></p><p>继续插入 35，由于左子结点已满，我们需要分裂左子结点，在新结点中插入 35，并将 30 取出并放入根结点中：</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-01-05-070715.png" alt="img"></p><p>所以，每当有结点被填满时，我们将分裂该结点，并将其中一个 key 取出并放入其父结点中。</p><p>继续插入 5：</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-01-05-071039.png" alt="img"></p><p>继续插入 15，由于左子结点已满，我们需要分裂左子结点，将 20 放入新结点中，并将 15 放入根结点中。然而，由于根结点也已经满了，所以我们继续分裂根结点，将 70 放入新结点中，并将 40 取出并提升一层，将其放入新的根结点中：</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-01-05-072138.png" alt="img"></p><p>注意，这里我们在每次分裂结点时总是保证取出 key 的左侧结点拥有更多的 key，实际上，我们也可以让右侧的 key 更多，这取决于我们自己的习惯。可以看到，现在我们已经构建了一个具有三级索引的 B 树，这就是多级索引的构建过程。现在，树的生成过程是可控的，所有的叶子结点都在同一层级，这使得 B 树与多级索引在结构上更加相似。</p><p>现在，我们来看一下 B 树在数据库中是如何使用的。这里有一个常重要的细节，正如我们前面所讨论的，每个 key 后面都可以存放一个记录指针，它直接指向数据库中的某条特定记录：</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-01-05-080211.png" alt="img"></p><p>所以，B 树中的结点包含 key、子结点指针和记录指针。</p><h2 id="8-B-树"><a href="#8-B-树" class="headerlink" title="8. B+ 树"></a>8. B+ 树</h2><p>在 B+ 树中，除了叶子结点，其余非叶子结点中不再包含记录指针，非叶子结点中的所有 key 都将在叶子结点中保存一份拷贝，即 B+ 树中的所有 key 都可以在叶子结点中找到，并且叶子结点之间采用类似链表的方式连接：</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-01-05-082800.png" alt="img"></p><p>B+ 树的这种结构与我们之前讨论的多级索引结构完全一样。B+ 树与 B 树的主要区别在于：</p><ol><li>B+ 树中，叶子结点中保存了所有的 key，其中有些 key 在非叶子结点中也进行了冗余存储。</li><li>B+ 树中，非叶子结点不再包含记录指针，所有的记录指针都保存在叶子结点中。</li><li>B+ 树中，所有的叶子结点构成了一个链表。因此，叶子结点所在的层实际上构成了一层密集索引，而非叶子结点所在的层构成了稀疏索引。</li></ol><p>因此，与 B 树相比，B+ 树的结构更加符合多级索引。</p><h2 id="9-总结"><a href="#9-总结" class="headerlink" title="9. 总结"></a>9. 总结</h2><p>我们首先讨论了磁盘的物理结构以及数据是如何存储在磁盘上的，然后我们介绍了关于索引和多级索引的概念，之后我们介绍了 � 路查找树及其存在的问题，最后我们介绍了基于 � 路查找树的高级数据结构 B 树和 B+ 树，二者的差异以及它们与多级索引之间的联系。</p>]]></content>
      
      
      <categories>
          
          <category> 学习型索引 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习型索引 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LearnedIndex-02</title>
      <link href="posts/8043.html"/>
      <url>posts/8043.html</url>
      
        <content type="html"><![CDATA[<h1 id="数据库中的机器学习：哈希表、B-树与学习索引"><a href="#数据库中的机器学习：哈希表、B-树与学习索引" class="headerlink" title="数据库中的机器学习：哈希表、B 树与学习索引"></a>数据库中的机器学习：哈希表、B 树与学习索引</h1><p><strong>参考资料</strong>：</p><ul><li><a href="https://www.youtube.com/watch?v=o1bN3gryKaw&amp;t=3087s">https://www.youtube.com/watch?v=o1bN3gryKaw&amp;t=3087s</a></li><li><a href="https://arxiv.org/abs/1712.01208">https://arxiv.org/abs/1712.01208</a></li></ul><h2 id="1-主要内容"><a href="#1-主要内容" class="headerlink" title="1. 主要内容"></a>1. 主要内容</h2><h3 id="1-1-数据库索引"><a href="#1-1-数据库索引" class="headerlink" title="1.1 数据库索引"></a>1.1 数据库索引</h3><p>索引存在的意义：</p><ul><li>我们有很多数据需要存储</li><li>我们需要快速查询一些特定条目</li><li>我们使用 <strong>索引 (Index)</strong> 完成这一任务</li></ul><p>假设我们存储了许多数据，并且希望 <strong>快速</strong> 查找其中某条或者某个特定范围的数据时，<strong>索引</strong> 可以帮助我们实现这一点。这很重要，不论是对于分析软件还是机器学习使用者，只要我们构建了一些数据 Pipeline，当我们需要快速抓取所需数据时，我们就是索引结构的 “消费者”。而 <strong>学习索引</strong> 这个新研究方向的出现使得机器学习人员也可以成为索引结构的 “生产者”。</p><h3 id="1-2-传统索引策略"><a href="#1-2-传统索引策略" class="headerlink" title="1.2 传统索引策略"></a>1.2 传统索引策略</h3><p>几种常见的传统数据库索引结构及其应用场景：</p><ul><li><strong>B 树</strong>：范围索引</li><li><strong>哈希表</strong>：点索引</li><li><strong>Bloom 过滤器</strong>：存在索引</li></ul><h3 id="1-3-新的研究方向：学习索引"><a href="#1-3-新的研究方向：学习索引" class="headerlink" title="1.3 新的研究方向：学习索引"></a>1.3 新的研究方向：学习索引</h3><p>新型 <strong>学习索引 (Learned Index)</strong> 方向：</p><ul><li>Google 和 MIT 合作研究</li><li><a href="https://arxiv.org/abs/1712.01208">https://arxiv.org/abs/1712.01208</a></li></ul><p>我们将介绍 Kraska 等人的这篇论文<a href="https://www.cl.cam.ac.uk/~ey204/teaching/ACS/R244_2018_2019/papers/Kraska_SIGMOD_2018.pdf">《The Case for Learned Index Structures》</a>，他们提出了使用 RMI 等神经网络结构代替 B 树等传统索引结构的思路，开启了数据库领域的一个新的研究方向。</p><h3 id="1-4-学习索引的优势在哪里"><a href="#1-4-学习索引的优势在哪里" class="headerlink" title="1.4 学习索引的优势在哪里"></a>1.4 学习索引的优势在哪里</h3><p>我们将探讨他们已经完成的初步研究工作：利用机器学习方法去解决之前传统索引结构解决过的一些问题，以及他们的对标实验和权衡比较：</p><ul><li>什么情况下，传统算法具有优势？</li><li>什么情况下，学习索引具有优势？</li><li>实践中如何在两者之间进行权衡？</li></ul><p>从长远看，学习索引可以更加充分利用硬件优势。随着指导预测 CPU 性能增长的摩尔定律日渐走向衰亡，目前来看，未来高性能计算将主要集中在 GPU 和并行数据指令集，机器学习方法可以很好地发挥这些优势，而传统索引结构很难做到这点。未来，随着 AI 工作负载在硬件方面的进一步深度优化，学习索引这类算法在未来将具有很大的发展潜力。</p><h2 id="2-什么是索引"><a href="#2-什么是索引" class="headerlink" title="2. 什么是索引"></a>2. 什么是索引</h2><p>我们先回顾一下索引：</p><ul><li>索引可以帮助我们在海量数据中快速定位到某些特定条目。</li><li>尤其是当数据并没有按照我们要查询的字段排序时会非常有用。</li></ul><p>假设我们存储了大量数据，当我们需要查询其中某些特定数据时，我们并不希望从头到尾扫描整个磁盘记录来寻找该数据，例如，当我们查询 “名字叫 Tim 的人有哪些” 或者 “某个特定日期范围内出生的人” 等，我们可以使用索引。</p><p>在数据库中，我们有很多行记录，每一行都存储在硬盘上，即下图中底部的块状条，其中每个格子代表一行记录：</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-08-22-WX20200822-151353%402x.png" alt="img"></p><p>假设现在我们想要查询名字为 Tim 的人对应的行记录所在的位置：</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-08-22-WX20200822-152124%402x.png" alt="img"></p><p>如果没有索引，整个查询过程将非常缓慢，因为我们需要依次扫描磁盘上的每一条记录。对于大公司而言，通常需要在数百万甚至数十亿条记录中搜索我们想要的结果，因此，这是一个计算成本和时间开销都非常高的过程：</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-08-22-WX20200822-152936%402x.png" alt="img"></p><p>为了解决这一问题，我们构建了一种辅助数据存储结构，称为索引。例如哈希表、B 树等等，我们将在后面对这些传统索引以及新型的学习索引进行讨论。假设我们构建了一个以用户名字为 Key 的索引结构，相比之前通过扫描整个表来查看数据库中的每条记录，现在我们可以直接询问索引 “嘿，名字为 Tim 的记录在哪里？”，然后索引将告诉我们名字为 Tim 的记录的内存地址的偏移量为 7。我们可以基于任何我们想要的 Key 来构建各种索引，例如用户 ID、名字、年龄等等，相比基础数据，这些索引都属于额外的辅助数据结构。</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-08-22-WX20200822-154853%402x.png" alt="img"></p><p>这里涉及到计算机科学研究中的一个典型的权衡问题：</p><ul><li>索引是一种在内存用量和计算速度两者之间的权衡。</li><li>索引会占用一些内存，但是可以让查询更快。</li></ul><p>我们使用一些内存来创建这样一种辅助数据存储，然后通过利用一些合适的算法和数据结构来加速进程，但是代价就是会占用部分内存。而这也是学习索引中最令人感兴趣的地方：传统的索引结构通常都非常大，并且其大小还会随着输入数据量的增加而增加；然而，对于机器学习模型，尤其是神经网络模型，我们能够以一种更小的格式来获得对数据更强的表征能力，因此，对于训练好的学习索引，通常会比传统索引更小，并且同样具备快速查找记录的能力，甚至更快。</p><h2 id="3-索引的性能"><a href="#3-索引的性能" class="headerlink" title="3. 索引的性能"></a>3. 索引的性能</h2><p>对于索引结构，我们一般会从以下几个方面考察其性能优劣：</p><ul><li>查询类型 (单值查询 VS. 范围查询)</li><li>查找速度</li><li>插入一条新记录的开销</li><li>删除一条记录的开销</li><li>更新一条记录的开销</li><li>索引大小和利用率</li></ul><p>我们将逐个介绍这几个方面，但是请注意，在我们要讨论的 Kraska 等人的这篇论文中，并没有显式地考虑插入、删除、更新记录的开销，该论文主要集中在只读数据集，因此并没有涉及到这些操作。所以实际上，这是目前学习索引存在局限性的一个方面，未来还需要对此进行更多研究。</p><p>当然，有些应用程序是专门针对只读索引的。例如 BigTable 使用只读索引并且在需要更新时会选择重构索引，以及像数据仓库这种，当涉及大量数据转储时，我们会选择在晚上访问流量较低时重构其索引。所以，目前学习索引的局限并非一种致命缺陷，但它仍然是一个有待进一步研究的问题。</p><h3 id="3-1-查询类型"><a href="#3-1-查询类型" class="headerlink" title="3.1 查询类型"></a>3.1 查询类型</h3><p>查询类型主要可以分为两种：<strong>点到点查询</strong> 和 <strong>范围查询</strong>。</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-08-22-WX20200822-163102%402x.png" alt="img"></p><p>我们可能希望利用索引查询某条或者某个范围内的特定数据，其中，对于单点查询，最常用的索引结构是哈希表；而对于范围查询，最常用的是 B 树索引。</p><p>此外，Karask 等人的论文中还提到了另一种常见的用于存在索引的 Bloom 过滤器以及相应的 AI 替代结构，但是这里我们不会对此进行展开，具体细节可以参考论文。</p><h3 id="3-2-查找速度"><a href="#3-2-查找速度" class="headerlink" title="3.2 查找速度"></a>3.2 查找速度</h3><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-08-22-WX20200822-170313%402x.png" alt="img"></p><p>查找速度也是一个非常重要的指标：当我们询问索引时，需要等待多久才能得到结果。通常，我们用大 � 记号来表示其时间复杂度。此外，有时我们也会采用一些经验测量，因为目前已经有很多相关研究，例如缓存优化以及其他一些针对计算机硬件的优化等，并且实践表明它们在改进索引性能方面都取得了一定的效果。</p><h3 id="3-3-插入和更新开销"><a href="#3-3-插入和更新开销" class="headerlink" title="3.3 插入和更新开销"></a>3.3 插入和更新开销</h3><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-08-22-WX20200822-191710%402x.png" alt="img"></p><p>如果我们要将一条新的记录插入数据库中，我们需要更新索引，这个过程中会引入一些无法忽视的开销。同理，对某条已存在的记录进行更新时，我们同样需要更新索引结构。</p><h3 id="3-4-删除开销"><a href="#3-4-删除开销" class="headerlink" title="3.4 删除开销"></a>3.4 删除开销</h3><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-08-22-WX20200822-192442%402x.png" alt="img"></p><p>从原数据库中删除某条记录同样需要更新索引结构。例如，当我们询问索引 “名字叫 Tim 的人记录在哪里” 时，索引会告诉我们在红色方格所在的内存地址；而当我们将该记录删除后，索引应当告诉我们不存在相关记录。</p><h3 id="3-5-索引大小和利用率"><a href="#3-5-索引大小和利用率" class="headerlink" title="3.5 索引大小和利用率"></a>3.5 索引大小和利用率</h3><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-08-22-WX20200822-204444%402x.png" alt="img"></p><p>索引会占用一些内存空间，具体大小取决于数据量，因为我们需要一种数据结构来支持我们查询索引中的每条记录或者每个 Key。利用率则是指索引中的信息密度，某些索引结构会占用远远超出其需求的大量空间，这意味着这些索引中存在许多待使用的空槽，我们将这些空槽占比称为利用率。例如，如果有一个索引结构，它实际占用的空间是其本身包含记录的两倍，那么该索引的利用率为 50%。通常，对于索引结构，尤其是哈希表，随着其利用率越来越高，其性能会逐渐下降。因此，在索引的利用率和性能之间也存在权衡问题。</p><h3 id="3-6-注意事项"><a href="#3-6-注意事项" class="headerlink" title="3.6 注意事项"></a>3.6 注意事项</h3><p>然而，在我们考虑时间开销等问题时，还有一些注意事项：</p><ul><li>索引是一种工具，主要用于 <strong>数据量很大</strong> 的情况</li><li>优化访问硬盘次数比优化总操作数更为普遍</li></ul><p>索引对于数据量很大的场景非常重要，尤其是数据多到需要使用硬盘存储时。很多索引结构实际上是在对新数据所需的硬盘访问次数进行优化，而不是在对总操作数进行优化。而这对机器学习而言是个好消息，因为我们可以将机器学习模型加载进内存，并且以一种高度并行的方式进行推断，然后告知用户应当去硬盘上的哪个地方查看指定记录，所以我们仍然可以在实现很多操作的同时最小化硬盘访问次数，甚至可能要比使用传统索引结构实现的操作数更多。并且，由于推断过程采用的是高度并行化的计算方式，学习索引还可以获得来自硬件方面的提升，例如使用 GPU 和 TPU 等，而传统索引结构则无法利用这些优势。</p><p><strong>上面提到的访问 (Access) 和操作 (Operation) 的区别是什么？</strong></p><p>当我们询问索引 “Tim 在哪里” 时，我们实际是在询问这个辅助数据存储，我们可能会在已经加载进 RAM 内存的索引中通过 10 个 CPU 周期来回答这个问题，这其中包含了一系列操作，例如计算哈希编码、内存查找指定记录的偏移量等。而在这里，关于 Tim 的记录所在的辅助数据存储区实际上是位于硬盘中的，对于 RAM 芯片和硬盘这两个单独的物理设备而言，从硬盘中访问某些内容要比从 RAM 中访问某些内容的时间开销更大，因此 RAM 的全部目的就是充当一种数据缓存区的功能 (或许在这里使用 “缓存” 这个词并不恰当，因为 CPU 上也有专门被称为内存高速缓存的区域)。与硬盘相比，RAM 距离 CPU 更近，访问速度也更快。因此，当谈到 <strong>访问</strong> 时，我们指的是硬盘访问；而当我们谈到 <strong>操作</strong> 时，我们指的是可以在 CPU 中完成的事情，例如单精度浮点算术运算之类的东西。</p><h2 id="4-哈希表"><a href="#4-哈希表" class="headerlink" title="4. 哈希表"></a>4. 哈希表</h2><p>我们已经介绍了关于索引的一些基本概念，现在我们来看一下哈希表，它也是我们前面提到的两种主要的索引结构中的第一种。</p><h3 id="4-1-哈希函数"><a href="#4-1-哈希函数" class="headerlink" title="4.1 哈希函数"></a>4.1 哈希函数</h3><p>哈希表中的基本组件之一就是 <strong>哈希函数</strong>。我们接受任意数据，将其输入哈希函数，然后得到某个整数值。存在许多不同种类的哈希函数，包括密码安全哈希函数、非密码哈希函数、完美哈希函数等等。哈希表最重要的一点在于哈希函数产生了一个来自 <strong>均匀分布</strong> 的整数，因此，<strong>哈希表中的任何整数与其他数字出现的可能性一样</strong>。这一点至关重要，我们既可以将其与 Kraska 等人的论文中讨论的机器学习哈希进行比较，也可以用于一般性地考虑哈希函数。</p><p>哈希函数将任意数据确定性地映射为一个整数：</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-08-22-WX20200822-212001%402x.png" alt="img"></p><h3 id="4-2-哈希表"><a href="#4-2-哈希表" class="headerlink" title="4.2 哈希表"></a>4.2 哈希表</h3><p>哈希表索引将这些整数映射到相应的数据库记录的内存地址：</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-08-22-WX20200822-225626%402x.png" alt="img"></p><p>这些由哈希函数生成的整数被称为 <strong>哈希编码</strong>，它被作为哈希表中的查找 Key，所以哈希表本质上就是 Key-Value 对，其中，Key 就是这些整数，而 Value 就是实际存储在硬盘上的数据库中对应记录的内存地址。</p><h3 id="4-3-数据查找过程"><a href="#4-3-数据查找过程" class="headerlink" title="4.3 数据查找过程"></a>4.3 数据查找过程</h3><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-08-22-WX20200823-012448%402x.png" alt="img"></p><p>当我们询问哈希表 “Tim” 所在记录的位置时，我们将字符串数据 “Tim” 输入哈希函数，然后得到了一个整数 15，该整数将受我们已经为哈希表分配的空间大小的限制，因此，哈希表的空间是预先分配好的。然后，我们查看哈希表中 Key 为 15 的位置 (即图中最右侧的绿色格子)，而该位置的 Value (这里为 2) 即为数据库中 “Tim” 所在记录的偏移量。</p><p>因此，相比直接对整个数据库进行表扫描并检查每条记录直到找到匹配 “Tim” 的记录的做法，哈希表的查找过程实际上分为两步：我们只进行了一次常数时间复杂度 �(1) 的函数计算来得到 15 这个 Key，然后我们在 RAM 中直接对该位置进行单次查找，而 RAM 中该位置的 Value 则直接告诉我们去硬盘中相应的地址获取所需数据，这就是哈希表索引。</p><h3 id="4-4-哈希表的性能"><a href="#4-4-哈希表的性能" class="headerlink" title="4.4 哈希表的性能"></a>4.4 哈希表的性能</h3><p>哈希表作为索引，在性能方面具有以下特点：</p><ul><li>非常适用于单值查找，但对于范围查找表现很差。</li><li>查询时间复杂度为 �(1)，首先访问一次索引，然后访问一次内存。</li><li>插入和删除的时间复杂度为 �(1)，访问一次索引。</li><li>索引大小和利用率取决于所选择的哈希函数以及冲突处理策略，通常，哈希表的利用率大约在 50% 左右。</li></ul><p>首先，哈希表非常适合单值查找的场景，但不适用于范围查找，这是由于哈希函数本身特性造成的。我们知道，哈希函数只是生成一些服从均匀分布的随机整数，如果我们的输入不变，那么输出总是确定的 (例如对于输入 “Tim”，输出总是 15)，所以这里的随机并不是说它们像随机数生成器那样，而是说这些整数来自某个均匀分布，我们并不能预先知道 “Tim” 对应的哈希编码，我们需要通过计算哈希函数才能得到结果。所以，对于单值查询，这个过程非常快，我们只需要计算一次哈希函数即可。但是，这种方法并不适用于范围查询，因为对于哈希函数而言，两个不同输入 (例如 “2020-01-01” 和 “2020-01-31”) 之间并不存在任何关联，它只会返回两个完全不同的不相关的数字。因此，哈希表不适用于范围查询，例如日期范围或者名字以某个字母开头的所有用户的记录块等。</p><p>然后，哈希表的查询时间复杂度为 �(1)，所以这是一个常数时间操作：计算一次哈希编码、单次索引查找、单次记录查找。</p><p>另外，插入和删除记录的时间复杂度也是 �(1)，但是这里有一些注意事项，如果我们插入/删除/更改某条记录，我们只需要在计算哈希编码时将其加入哈希表，然后查找该位置，将其加入索引，我们就完成了对该索引的更新。</p><p>最后，索引大小和利用率会根据我们所选择的哈希函数和冲突处理策略的不同具有较大差异。例如，我们有线性探测、分离链接法、布谷鸟哈希等等各种冲突处理策略，但是这里我们不会对其进行展开，因为机器学习版的哈希表只是简单地将哈希函数本身用机器学习模型代替，所以，和常规哈希函数一样，我们仍然可以将这些经典的冲突处理策略与学习型哈希函数一起使用。通常，哈希表的利用率为 50% 左右，但是也有一些高性能的哈希函数或者哈希表可以达到 90% 甚至 95% 的利用率。</p><h3 id="4-5-注意事项"><a href="#4-5-注意事项" class="headerlink" title="4.5 注意事项"></a>4.5 注意事项</h3><p>对于哈希表索引，有些可能存在的陷阱需要注意：</p><ul><li>冲突处理策略的不同可能会带来相当大的性能差异。(线性探测 VS. 分离链接法 VS. 布谷鸟哈希)</li><li>对于插入操作，如果存在冲突过多或者表空间用尽的情况，可能会要求重建整个索引结构，而这同样也会对利用率造成影响。</li><li>哈希函数的选择可能会影响到冲突发生率和计算速度，而这会对查询速度和利用率造成影响。</li></ul><p>冲突处理策略的不同会导致很大差异，但是对于机器学习模型而言问题不大，因为它们可以使用和传统哈希表相同的冲突处理策略。</p><p>在某些情况下，插入操作可能需要我们重建整个索引。因此，大多数情况下插入记录是常数时间操作，但某些情况下可能会变为线性时间操作。</p><p><strong>为什么我们不能使哈希表的利用率尽可能高？</strong></p><p>这是一个非常复杂的问题，这里仅提供一些大致的考虑。当索引利用率越高时，哈希表会越趋近饱和，此时，每个新插入的项与表中其他已存在项发生冲突的概率会越大。例如，在之前的例子中，我们的哈希表的大小为固定的 16 个单元，每次我们加入新的项时，我们需要将对应的哈希编码加入 0-15 之间的某个单元中。如果我们向该哈希表中一共插入了 17 个项，那么将产生冲突，因为我们无法同时将两个对象存储在同一块内存地址中。所以，随着利用率上升，冲突发生的可能性就越大，而当两个项需要放置在哈希表的同一个槽中时，我们要么重建整个哈希表，要么必须采用某种冲突处理策略，例如前面提到的线性探测、分离链接法、布谷鸟哈希等，但是这些操作的开销都非常高，因为我们不能简单地把这些项加入表中，我们需要管理这些冲突，而这将降低索引的性能。</p><h2 id="5-B-树"><a href="#5-B-树" class="headerlink" title="5. B 树"></a>5. B 树</h2><h3 id="5-1-B-树的结构"><a href="#5-1-B-树的结构" class="headerlink" title="5.1 B 树的结构"></a>5.1 B 树的结构</h3><p>哈希表通常用于单点查询，而对于范围查询，最常见的索引结构是 B 树，其结构和二叉树非常相似，区别在于 B 树具有分支因子。</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-08-22-WX20200823-012137%402x.png" alt="img"></p><p>分支因子的一个常见取值是 100，即每个结点有 100 个子结点，因此，如果我们有 100 个项，我们可以使用单层的 B 树来表示数据；如果有 10,000 个项，我们可以使用两层的 B 树来表示数据；如果有 1,000,000 项，可以使用三层的 B 树来表示数据，以此类推。另外，我们也可以采取类似于二叉搜索树的表示方式对数据进行排序，即最左边的叶子结点总是排序项中的第一项，而最右的叶子结点总是排序项中的最后一项。</p><h3 id="5-2-B-树的查找过程"><a href="#5-2-B-树的查找过程" class="headerlink" title="5.2 B 树的查找过程"></a>5.2 B 树的查找过程</h3><p><strong>B 树的划分策略</strong>：每个结点都是一个由划分点组成的列表。假如我们有一些小写文本数据，这里每个结点只有 9 个项。我们从根结点开始查找相对粗粒度的分区 (所有以 “a” 开头的成员，所有以 “b-d” 开头的成员等等)，结点中的每个成员的值都是一个指向下一个粒度更细的结点的指针，例如：根结点中第一项 “a” 的值是一个指向下一个包含以 “a” 开头前两个字母 (例如 “aa-ab”、“ac-ad” 等) 的内部结点的指针；第二项 “b-d” 的值是指向另一个内部结点的指针等等。</p><p>根节点和每个内部节点都是指向整体数据的已排序部分的指针列表：</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-08-27-WX20200827-142052%402x.png" alt="img"></p><p>如果现在我们需要查找以 “ae-ag” 开头的数据，我们将从根结点开始，沿着对应项 “a” 中的指针到达另一个粒度更细的内部结点，然后再根据对应项 “ae-ag” 中的指针到达相应的叶子结点，其中包含 4 个项：“aeon”、“afar”、“afraid”、“agriculture”。</p><p>而最终叶子结点中的每一项的值都是一个指向硬盘上的数据库表中对应数据条目的地址偏移量，我们可以通过这种方式取回所有以 “ae-ag” 开头的数据。</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-08-27-WX20200827-150023%402x.png" alt="img"></p><p>由于索引是针对硬盘访问进行优化的，每个结点的大小通常取决于硬盘上的存储块的大小，通常为 512K。</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-08-27-WX20200827-150821%402x.png" alt="img"></p><p>对于数据的读取过程，由于它最初是针对硬盘访问进行优化的，因此，与使用 B 树进行硬盘访问相比，我们将在内存中执行更多的操作，并且与哈希表相比，操作数要更多。因此，此过程是我们将数据块读入RAM，就像根节点一样。然后，我们对该数据块进行迭代搜索直到找到匹配项，并且实际上，通常此迭代过程是对排序后的数据进行二分查找。因此我们将找到下一个要访问的数据块或结点，只是通过简单的快速扫描，然后我们将读取下一个数据块并将其作为 B 树中的下一个结点。重复此过程，直到到达叶子结点并找到我们正在寻找的数据。</p><p><strong>如果数据库中不存在 “j-p” 的记录，那么 B 树中会存在相应的结点吗？</strong></p><p>这取决于具体的实现细节。由于叶子结点的存储方式是完全排序的，有时，这些存储页会被提前分配，即会存在一个用于存储 “j-p” 之间的数据的空间，只不过该空间内是空的 (即 Root 结点中 “j-p” 的单元格是空的)，因此，当我们搜索 “j-p” 时，我们会知道不存在相应记录，所以会立即停止搜索过程。但是考虑到利用率，这些数据也有可能已经分配好了，所以存储页可能已经存在，只不过包含的都是空记录，当我们搜索到这些空记录时会发现所要查找的记录并不存在。</p><p><strong>在大部分 B 树实现中，不同内部结点中区间大小或者不同叶子结点中项的数量是否通常是相等的？</strong></p><p>是的，我们对于每个结点大小的分配是基于一次从硬盘中获取的数据量的大小。所以，它是针对硬盘访问高度优化的，硬盘中有一些相对标准的数据块大小，而 B 树是以此为基础构建的。虽然，这并非构建 B 树数据结构时的强制要求，但是大部分 B 树都是这样构建的。</p><p><strong>在上面的例子中，如果我们希望查找以 “ae-af” 开头的记录，应该如何实现过滤？</strong></p><p>首先，我们先进入 “ae-ag” 所指向的内部结点。然后，我们对其指向的所有叶子结点进行扫描直到碰到第一个不符合查询条件的叶子结点。所以，我们会发现 “aeon”、“afar” 和 “afraid” 都符合查询条件，而 “agriculture” 不符合条件，然后我们停止搜索，并返回之前的三个符合条件的记录即可。</p><p><strong>表扫描的范围不能太大，否则时间成本将很高，对吗？</strong></p><p>是的，这些结点的大小都相对较小。另外，虽然不是必须的，但是通常对于叶子结点，我们会采用二分查找，所以并不只是简单的表扫描。</p><p><strong>如果我们的数据库是可扩展的，那么 B 树可以设置为在给定一个利用率后可动态扩展吗？</strong></p><p>可以，B 树不一定具有固定的利用率，它可以扩展、增长和变化。</p><p><strong>为什么一个叶子结点中的索引数量和一个硬盘中的存储块大小相关？</strong></p><p>在上面的例子中，“aeon”、“afar”、“afraid” 和 “agriculture” 这 4 个项都在同一个叶子结点中，而该叶子结点的大小是固定的，具体取决于硬盘的存储块大小。例如，如果满足条件以 “ae-ag” 开头的记录不足一个块大小 (512K)，那么该叶子结点中可能会存在空闲空间。但是，单个叶子结点中允许的最大实体数量是固定的。也就是说，对于单个叶子结点，我们会分配给其一个块大小 (512K) 的内存空间 (虽然其中某些槽可能是空的)，除了该结点外，其他对象无法使用该内存空间。</p><p><strong>所有这些结点都在磁盘上吗？</strong></p><p>是的，对于主流数据库，所有这些结点通常都在磁盘上。所以，访问根结点就是一次磁盘访问的过程，然后通过扫描该结点中的实体， 我们找到接下来应该访问磁盘中的哪个位置 (例如 “a” 指向的下一个内部结点)，只有在这之后，我们才会将根结点中的 “a” 所指向的整个内部结点从磁盘上加载进 RAM 中。</p><h3 id="5-3-B-树的性能"><a href="#5-3-B-树的性能" class="headerlink" title="5.3 B 树的性能"></a>5.3 B 树的性能</h3><p>B 树作为索引，在性能方面具有以下特点：</p><ul><li>非常适用于范围查询，同时也很适用于单值查询。</li><li>查询时间复杂度为 �(log⁡�)，叶子结点包含了我们实际需要的数据，并且树的深度为 log⁡�。</li><li>插入和删除的时间复杂度为 �(log⁡�)，和查询一样。</li><li>索引大小和利用率取决于每个结点的大小、树的整体平衡度以及每个结点的饱和程度。</li></ul><p>首先，与哈希表相比，B 树非常适合范围查询。例如，假如我们要查找所有 “au-az” 开头的记录，我们只需要访问所有 “au-av”、“aw-ax”、“ay-az” 这三个内部结点所对应的最终叶子结点即可，并且 B 树的结构可以使得整个过程非常高效。</p><p>然后，B 树的查询时间复杂度为 �(log⁡�) 次的磁盘访问，但需要注意的是，每当我们进行一次磁盘访问时 (即访问 B 树中的一个结点)，我们都会扫描一个磁盘存储块的大小。当我们到达最终的叶子结点时，我们应该已经找到了所需的记录，整个过程包含了 log⁡� 次查询。</p><p>另外，B 树中插入和删除的的时间复杂度也是 �(log⁡�)，同样需要注意的是，和哈希表类似，某些情况下插入和删除记录可能会导致需要重建整个或者部分索引结构，或者重新平衡 B 树，这些可能会导致更高的计算时间开销。</p><p>最后，索引大小和利用率根据数据量的大小变化会非常大。例如，假设我们有 9K 条记录，我们可以用三层的 B 树表示这些数据。但是一旦我们有了 10K 加 1 条记录，并且每个结点包含 100 条记录，那么我们需要为最后多出来的一条记录在 B 树中构建整个下一层的结点并分配相应的空间。</p><h3 id="5-4-注意事项"><a href="#5-4-注意事项" class="headerlink" title="5.4 注意事项"></a>5.4 注意事项</h3><p>对于 B 树索引，有些可能存在的陷阱需要注意：</p><ul><li>B 树必须是相对平衡的，否则其性能无法达到 �(log⁡�)。</li><li>对一个已经饱和的结点进行插入操作将导致该结点 “分裂” 为两个新结点，这会导致额外的计算开销，并且两个新结点的利用率将都比较低 (大约 50%)。</li><li>删除一个结点可能会导致高计算开销的重新平衡操作。</li></ul><p>如果我们没有很好地构建 B 树 (当然，如今由于已经有很多良好的开源实现，很少会发生这种情况)，我们可能会得到一个近似链表结构的 B 树，这种极端不平衡的情况下，我们将无法达到预期的 �(log⁡�) 时间复杂度。</p><p>另外，插入、删除、更新等操作有可能导致重新平衡 B 树或者重新分裂结点，甚至重构整个索引结构，这些都会带来额外的计算开销。</p><h2 id="6-学习索引"><a href="#6-学习索引" class="headerlink" title="6. 学习索引"></a>6. 学习索引</h2><p>2018 年，一项由 Google 和 MIT 合作的研究探索了将机器学习应用于索引结构的想法。</p><p>论文地址：<a href="https://arxiv.org/abs/1712.01208">https://arxiv.org/abs/1712.01208</a></p><h3 id="6-1-哈希表和-B-树只是模型"><a href="#6-1-哈希表和-B-树只是模型" class="headerlink" title="6.1 哈希表和 B 树只是模型"></a>6.1 哈希表和 B 树只是模型</h3><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-08-27-WX20200827-233052%402x.png" alt="img"></p><p>这篇论文的关键见解是：哈希表和 B 树都只是模型。这些模型可以将我们所感兴趣的条目的 Key 映射到某个相应的内存地址，而这本质上只是一个数据的 CDF (累积分布函数) 上的回归问题。</p><h3 id="6-2-范围索引只是-CDF"><a href="#6-2-范围索引只是-CDF" class="headerlink" title="6.2 范围索引只是 CDF"></a>6.2 范围索引只是 CDF</h3><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-08-27-WX20200828-000726%402x.png" alt="img"></p><p>我们可以只是模拟一个函数，它接受 Key 作为输入，然后我们将该函数的输出 (0 到 1 之间的某个值，代表 CDF) 乘以索引中的元素总数，我们将得到估计的位置。</p><p>基于这种思路，作者首先构建了一种非常朴素的索引结构，但是效果较差，但是最终在经过一些实验后，作者提出了一些让这些机器学习模型更好地工作的方式。</p><h3 id="6-3-机器学习模型存在的问题"><a href="#6-3-机器学习模型存在的问题" class="headerlink" title="6.3 机器学习模型存在的问题"></a>6.3 机器学习模型存在的问题</h3><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-08-27-WX20200828-002642%402x.png" alt="img"></p><p>作者的第一个发现的问题是：机器学习模型不能很好地提供精确的答案。虽然，目前机器学习模型的性能正在变得越来越好，并且能够为我们的问题提供一个近似答案。但是，对于索引系统而言，近似答案还不够好，我们需要确切地知道所查找的记录在内存中的位置，否则我们将无法检索出正确记录。如果我们只是接受一个大概的位置，那么每次当我们请求 Tim 的记录时，我们可能会得到 Ted 或者 Tin 的记录 (或者其他一些不相关的记录)，而这样的结果实际上并没有什么用。因此，对于要充分用作索引结构的机器学习模型，我们必须解决这个近似问题。</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-08-27-WX20200828-004613%402x.png" alt="img"></p><p>作者是通过这种方式解决这一问题的：<strong>同时存储所使用的每个机器学习模型在训练数据上的最小误差和最大误差</strong>。</p><p>请注意，这种情况并非常见的机器学习研究场景：<strong>在这种特定情况下，训练数据同时也是测试数据</strong>。因为对于索引结构而言，它只需要处理我们已经加入索引中的数据，因此，当我们构建预测模型时，我们就已经拥有了它必须处理的所有数据。</p><p>对于机器学习人员而言，从某些方面来看这是一个好消息，因为现在我们不必担心过拟合问题，并且 <strong>实际上我们的目标正是过拟合</strong>：我们要使模型完全拟合所拥有的数据，而并非去考虑一些可能会出现的假设数据。</p><p>虽然，对于这类 <strong>只读内存数据库 (Read-only In-memory Databases)</strong> 而言，以上观点是正确的。但实际上，对于具有大量写工作负载或者只要涉及到写操作的数据库而言，这是一个巨大的问题。因为每当有新数据输入时，我们都有可能需要重新训练整个模型，而这显然是有问题的。因此，这是一个有待进一步研究的方向，作者在论文中并未尝试解决这方面的问题。</p><p>重点是，我们必须为每条记录，或者说训练机器学习模型所用到的整个数据集，存储最小误差和最大误差。然后，我们将得到一个 **有界问题 (Bounded Problem)**。因此，我们首先得到所要查找的记录在内存中的大概位置，然后只需要在基础数据库中使用二分查找或者其他搜索算法来查找我们感兴趣的记录即可，作者在论文中还提到了一些其他的搜索策略，但是通常而言，二分查找是最强大的。</p><p><strong>过拟合和仅建立一个完整索引有什么区别，这对于不在完整索引中的新数据有帮助吗？</strong></p><p>论文中对于第二个问题的答案尚不清楚，因为作者并没有尝试过考虑哪些数据可能会加入索引。因此，如果我们有一个完全过拟合的机器学习模型，即模型已经完全记住了训练数据，那么它将是一个完美且完整的索引。作者在论文中进行了一些相关尝试，目前来看，与经典索引结构相比，学习索引至少在时间框架上具有一定的竞争力。</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-08-27-WX20200828-022149%402x.png" alt="img"></p><p>作者有时甚至针对学习问题的某些部分使用非常简单的线性回归模型，但这并不是内存中位置不够精确导致的唯一问题。 因此，内存中位置不够精确导致的第二个问题是：机器学习模型并非超级精确，我们可能需要一个非常复杂的模型来训练一亿条记录并获得精确的答案，以便从这一亿条记录中找到该数据对应的 CDF 的位置。</p><h3 id="6-4-递归模型索引-Recursive-Model-Index-RMI"><a href="#6-4-递归模型索引-Recursive-Model-Index-RMI" class="headerlink" title="6.4 递归模型索引 (Recursive Model Index, RMI)"></a>6.4 递归模型索引 (Recursive Model Index, RMI)</h3><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-08-28-WX20200828-182355%402x.png" alt="img"></p><p>作者并没有选择构建一个大型的复杂模型，而是构建了一个层次框架，并采用了一种类似于网格搜索的方法来为框架中的每个模型构建单独的参数。图中每一个方框都代表一个不同的单独训练的机器学习模型，该模型通常很小。作者在框架顶层使用了一个包含 2 个隐藏层的神经网络，当需要查询一条记录时，位于根结点的机器学习模型将预测在下一层中应该使用哪个机器学习模型来回答 “该记录在内存中的什么位置” 这一问题。作者尝试了不同层数的框架和模型，发现仅仅采用 2 层系统就能够获得最佳效果 (至少该设置在论文中是最成功的)。因此，在该框架下，一个机器学习模型会选择下一个单独的机器学习模型来完成 “最后一英里” 的工作，并预测记录在内存中的实际位置。</p><p>该层次模型的重点在于，我们可以训练一个简单的非常快速的推理模型，从而使我们逐步到达目标记录的位置。 因此，顶层模型将空间一分为二，作者在第二层中使用了大于 10,000 个简单模型，并且实验效果相当不错。因此，作者提出的是一个框架，其中包含了网格搜索和混合模型方法，其中所有这些模型可以是不同类型的模型，其中某些可能是神经网络，某些可能是线性回归，在某些情况下，甚至可以使某些机器学习模型实际上只是标准的 B 树实现。因此，当某种数据分布对于简单机器学习模型而言具有挑战性时，我们可以使用 B 树来解决此问题。</p><p><strong>是否可以使用随机森林模型代替神经网络？</strong></p><p>在论文中，作者并没有使用随机森林模型，而是采用了小型的简单神经网络进行了实验，该网络具有两个隐藏层，总共多达 32 个节点，并且作者尝试了一些不同的组合，例如：1 个隐藏层 8 个节点、1 个隐藏层 16 个节点、1 个隐藏层 32 个节点、2 个隐藏层每个 8 个节点等等，并且使用网格搜索策略确定合适的参数尺寸。但是，作者并没有尝试使用随机森林，当然，这将是一个有趣的研究领域，我们可以进行这种尝试以找出还有哪些其他类型的机器学习模型可能适用，尤其是在此层次模型结构的每一层。</p><p><strong>这种多层简单模型是如何构建的？不同层上的模型应该如何排列？</strong></p><p>基本上，作者采用的是网格搜索策略。作者提出的训练过程几乎像是对各种可能性进行网格搜索一样，因此他们构建了许多不同样式的模型，并且尝试了许多组合，直到找到一个在数据集上表现良好的组合，然后再将最优组合和基准进行对比测试。</p><h3 id="6-5-学习哈希函数-Learned-Hash-Function"><a href="#6-5-学习哈希函数-Learned-Hash-Function" class="headerlink" title="6.5 学习哈希函数 (Learned Hash Function)"></a>6.5 学习哈希函数 (Learned Hash Function)</h3><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-08-28-WX20200828-203726%402x.png" alt="img"></p><p>对于非范围查询，作者尝试替换点对点查询中的哈希函数，而不是尝试替换哈希表的全部相关内容 (例如冲突处理策略等等)。如上图所示，使用下方的机器学习模型 (由多个机器学习模型组成的层次系统) 替换哈希函数本身，然后我们得到的这个 <strong>学习哈希函数 (Learned Hash Function)</strong> 将能够考虑数据分布，并且 (但愿) 最终减少基础哈希表中的冲突，而标准的经典哈希函数就其本质而言不可能做到这点。</p><h3 id="6-6-实验结果"><a href="#6-6-实验结果" class="headerlink" title="6.6 实验结果"></a>6.6 实验结果</h3><p>论文作者主要关注简单模型，这篇论文是一个概念上的证明：</p><ul><li>作者主要关注小型全连接神经网络</li><li>主要关注一次创建、只读、内存数据结构</li><li>对于具有大量写工作负载的场景，这些模型存在很大问题，而论文作者并没有尝试解决这些问题。</li></ul><p>这里，我们将总结论文中的一些结果，如果希望知道更多相关细节 (例如：RMI 模型的损失函数如何选择、训练过程如何实现等)，请参阅论文原文。</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-08-28-WX20200828-220833%402x.png" alt="img"></p><p>作者首先比较了 B 树和学习索引在三个不同类型的数据集上的性能差异。对于这两种索引结构，作者对具有不同页面大小的 B 树和具有不同阶段二模型大小的学习索引 (两层模型，根结点用于预测第二层的 10K 到 200K 个简单模型) 进行了对比。</p><p>第一个数据集是地图数据，其中的 Key 是经度值，该数据可能来自 Google 地图 (因为该论文是与 Google 合作的，所以作者应该能够得到相关的地图数据)。论文中给出了更多相关细节，但总的来说，它们只是以经度作为 Key 的来自真实世界的数据。</p><p>第二个数据集是 Web 数据，它是由时间戳作为 Key 的 Web 日志记录 (来自 MIT 网站)。因此，对于经典索引结构而言，这确实是非常棘手的数据，因为这类数据的分布通常非常复杂：像班级时间表、假期等等所有这些奇怪的难以预测的东西都会被添加到这些时间戳中，非节假日或者期末考试期间，网站访问记录会比平时更多，所以它们对应的时间戳会包含更多的记录。</p><p>最后，作者还创建了一个人工数据集，通过对数正态分布生成了一个包含数百万条记录的数据集。</p><p>如上图结果所示，值得注意的是，和 B 树相比，学习索引的大小 (以 MB 为单位）减小了多少。可以看到，最小的 B 树与最大的学习索引模型一样大。因此，这是一个非常有趣的发现：我们可以用更少的内存来换取相对可观的性能。注意到 Jeff Dean 也是这篇论文的署名作者之一，上述结果项的比较应该是比较合理的，未来可以在此基础上进行进一步研究。可以看到，和 B 树相比，学习索引的内存占用量明显减少，并且在查询速度方面，在地图数据集上的速度提高了 2 到 3 倍左右；Web 数据集上的提升略小 (因为该数据分布很复杂)，只有 1.1 到 2 倍；类似地，在对数正态分布的人工数据上提升了大约 1.5 倍。该层次模型的所有不同参数组合上的性能改进都相对一致，值得进一步研究，并且我们对于这些模型能够达到多小很感兴趣。</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-08-28-WX20200829-003608%402x.png" alt="img"></p><p>对于论文中哈希表的相关工作，作者并没有对哈希表进行实质上的比较。因为，正如我们之前提到过的，我们可以使用相同的哈希表基础结构，因此作者在这里仅比较了冲突发生率。而且我们注意到，对于第一个地图数据集，冲突发生率仅为 7.9%，这意味着机器学习模型能够很好地学习到该基础经度数据的分布，从而显着减少了发生冲突的次数。对此，作者在论文中并没有提供相关图表，但的确指出了这点。</p><p>需要注意的是，与标准哈希函数相比，学习模型需要更长的时间用于计算，因此在此基础上还有更多工作要做。与非常高效的标准哈希函数相比 (类似 2 次乘法和 1 次移位运算)，学习模型在推理上的计算开销较高。因此，如果没有采用专门的加速硬件 (例如 GPU)，冲突减小了 77.5％ 这个数据看上去还是不错的。因此，在某些情况下，尤其是对于冲突非常昂贵的场景，这种学习哈希索引策略可能会很有用。</p><h3 id="6-7-总结"><a href="#6-7-总结" class="headerlink" title="6.7 总结"></a>6.7 总结</h3><p>最后，我们再总结几点：</p><ul><li><p>事实证明，将数据分布考虑进去可以使模型变得更好：</p><ul><li>B 树和哈希表没有考虑数据的分布，这会对索引利用率产生负面影响。</li><li>机器学习模型从本质上考虑了数据的分布，这可以帮助提高索引利用率。</li></ul></li><li><p>从某种程度上看，在学习索引中，过拟合是我们的目标：</p><ul><li>索引结构永远不需要预测那些尚未建立索引的数据的位置。</li><li>对索引中的数据进行准确的预测要比泛化到保留数据集上更重要。</li></ul></li><li><p>误差边界要比平均误差更重要：</p><p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-08-28-WX20200829-010343%402x.png" alt="img"></p><p>表扫描将在最小误差边界和最大误差边界之间进行，因此，从对查询性能方面的影响考虑，相比如何使得猜测的位置更加准确，实际上我们更加关心的是如何缩小这个误差范围。所以这是另一种有趣的权衡，在一般的机器学习研究中我们通常不必考虑这点，至少并不总是需要将其作为目标，有时我们的目标只是为了使猜测尽可能准确，尤其是对于一般的分类问题而言。</p></li><li><p>关于扩展的有趣特性：</p><ul><li>标准索引结构的大小会随着输入数据的增长而增长。</li><li>机器学习模型的大小不必随数据增长而增长，它只需要足够的复杂度即可生成 N 个值作为输出。</li></ul></li><li><p>利用新硬件的优势：</p><ul><li>摩尔定律正在日渐衰亡，传统索引结构并不能很好地适应并行化。</li><li>机器学习工作负载是当前硬件发展的主要驱动力，这意味着学习索引可以更好地利用新硬件的优势。</li></ul></li><li><p>多维索引：</p><ul><li>传统索引结构仅考虑某些数据的单个特征。</li><li>学习型索引结构可以跨多个维度建立索引，允许立即应用各种查询和过滤器。</li><li>这得益于机器学习在识别高维关系方面的能力。</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 学习型索引 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习型索引 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LearnedIndex-Lab</title>
      <link href="posts/30bd.html"/>
      <url>posts/30bd.html</url>
      
        <content type="html"><![CDATA[<h1 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h1><p>传统的索引，如 哈希的点查询、B 树的范围查询还是位图的存在性查询，其本质都是给定一个 key，返回该 key 对应的数据位置position，这本质就是一个 model。所以，我们可以尝试使用 ML 中的经典回归模型，如：线性回归、支持向量机、决策树、神经网络等来拟合单变量 key 与 position 之间的函数关系。同样的，我们也可以将模型推广到多维（多变量）甚至高维以构建多维数据甚至高维数据索引。</p><h2 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h2><p>B树啥的都有多层，所以ML也训练多层，即分层训练（目前就两层），先训练最顶层，然后进行数据分发，再训练底层。具体地，上层将 key 预测到具体的下层，该下层就拥有这条训练数据。随着层数的加深，以及每一层模型数量的提升，每个越底层模型拥有的训练数据就越少。这样的优点是底层模型可以非常容易的拟合这一部分数据的分布；缺点是较少的数据量带来了模型的选择限制，复杂模型没法收敛。实践中，越简单的模型越好。</p><p>上层预测的 key 可由：预测值 * 下一层模型总数 / 最大的position 下取整获得其该分发给下层哪个模型。</p><h1 id="生成数据集训练模型"><a href="#生成数据集训练模型" class="headerlink" title="生成数据集训练模型"></a>生成数据集训练模型</h1><h2 id="生成数据集"><a href="#生成数据集" class="headerlink" title="生成数据集"></a>生成数据集</h2><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> random<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pdnum_data <span class="token operator">=</span> <span class="token number">10000</span>    <span class="token comment" spellcheck="true"># 数据量</span>paras <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">30000.0</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># 均匀分布上下限</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 定义随机种子</span>datasets <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># 定义数据集</span>low_range<span class="token punctuation">,</span> max_range <span class="token operator">=</span> paras<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> paras<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># 均匀分布上下限</span>length <span class="token operator">=</span> max_range <span class="token operator">-</span> low_range<span class="token comment" spellcheck="true"># 生成满足均匀分布的 num_data 个数的数据集</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>num_data<span class="token punctuation">)</span><span class="token punctuation">:</span>    datasets<span class="token punctuation">.</span>append<span class="token punctuation">(</span>int<span class="token punctuation">(</span>low_range <span class="token operator">+</span> random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> length<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 从小到大进行排序: learned index 要求 key 有序</span>datasets <span class="token operator">=</span> sorted<span class="token punctuation">(</span>datasets<span class="token punctuation">)</span>position <span class="token operator">=</span> <span class="token punctuation">[</span>i <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>datasets<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># 模拟生成 key 对应的 local_local_local_local_local_local_local_local_position</span>datasets <span class="token operator">=</span> np<span class="token punctuation">.</span>column_stack<span class="token punctuation">(</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>datasets<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>position<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>df <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>data<span class="token operator">=</span>datasets<span class="token punctuation">,</span> columns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"key"</span><span class="token punctuation">,</span> <span class="token string">"position"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>df<span class="token punctuation">.</span>to_csv<span class="token punctuation">(</span><span class="token string">"output.csv"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 值得注意的是:均匀分布随机生成的key可能有重复，实际中，数据可能跨页存储，所以key对应的position也可能有多个（个人理解）</span></code></pre><h2 id="索引实现"><a href="#索引实现" class="headerlink" title="索引实现"></a>索引实现</h2><pre><code>首先，通过一个两层的循环遍历所有的训练阶段和模型。在每个阶段中，为每个模型添加一个空列表。如果不是最后一个阶段，则需要为下一个阶段初始化数据集。创建一个空的next_key列表和next_pos列表，长度为下一个阶段的模型数量。在当前阶段的每个模型中，进行模型的初始化操作。根据给定的隐藏层数量和每层神经元数量，创建一个神经网络模型。定义代价函数和优化器，分别使用均方误差作为代价函数，并使用Adam算法进行优化。通过torch_data.DataLoader从数据集ds中按照批次大小为64加载当前阶段当前模型对应的样本数量。在最多迭代1000次的循环内，进行模型的训练。从数据加载器中依次提取keys和pos。进行模型的前向传播，得到输出结果。计算损失值（均方误差）并进行反向传播和参数更新。在每次迭代后，计算当前获取的数据集的总损失值。通过设置torch.set_grad_enabled(False)将梯度计算禁用，以提高计算效率。判断是否满足早停条件，即当前迭代的损失值大于上一次迭代的损失值，或者两次迭代的损失值之差与上一次迭代的损失值之比小于0.001。绘制各阶段各模型的损失函数图，展示训练过程中损失值的变化情况。将训练好的模型添加到模型树中。如果不是最后一个阶段，则开始上层模型向下层模型分发数据集。遍历数据加载器中的每个样本，计算下一阶段应该分配给哪个模型。使用当前模型对当前样本的输出值进行处理，根据输出值、下一阶段模型的数量和最大位置来计算模型的选择。将当前样本添加到对应的数据集中。为下一个阶段创建对应的数据集，将next_key和next_pos加载进去。重复以上步骤，直到完成所有的训练阶段和模型的训练。</code></pre><h3 id="导入相关依赖"><a href="#导入相关依赖" class="headerlink" title="导入相关依赖"></a>导入相关依赖</h3><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn  <span class="token comment" spellcheck="true"># 方便使用 全连接层</span><span class="token keyword">import</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">as</span> torch_data  <span class="token comment" spellcheck="true"># 方便数据处理</span><span class="token keyword">import</span> warnings<span class="token keyword">from</span> matplotlib <span class="token keyword">import</span> pyplot <span class="token keyword">as</span> pltwarnings<span class="token punctuation">.</span>filterwarnings<span class="token punctuation">(</span><span class="token string">"ignore"</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 忽略警告信息</span>plt<span class="token punctuation">.</span>rcParams<span class="token punctuation">[</span><span class="token string">"font.sans-serif"</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"SimHei"</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># 显示中文</span>plt<span class="token punctuation">.</span>rcParams<span class="token punctuation">[</span><span class="token string">"axes.unicode_minus"</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">False</span>  <span class="token comment" spellcheck="true"># 显示负数</span></code></pre><h3 id="定义相关类"><a href="#定义相关类" class="headerlink" title="定义相关类"></a>定义相关类</h3><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 定义数据集类方便处理数据</span><span class="token keyword">class</span> <span class="token class-name">Dataset</span><span class="token punctuation">(</span>torch_data<span class="token punctuation">.</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> keys_list<span class="token punctuation">,</span> pos_list<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>keys <span class="token operator">=</span> keys_list   <span class="token comment" spellcheck="true"># 定义键集合</span>        self<span class="token punctuation">.</span>pos <span class="token operator">=</span> pos_list     <span class="token comment" spellcheck="true"># 定义 position集合</span>    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>keys<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 获取keys长度</span>    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> index<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>keys<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>pos<span class="token punctuation">[</span>index<span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># 获取指定 index 的 item(key, value)</span><span class="token comment" spellcheck="true"># 继承 nn.Module 方便自定义 BP 模型</span><span class="token keyword">class</span> <span class="token class-name">Network</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hidden_num<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 隐藏层数量, 每个隐藏层神经元数量</span>        super<span class="token punctuation">(</span>Network<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>hidden_num <span class="token operator">=</span> hidden_num        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># 存储全连接层</span>        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># ReLU 激活函数</span>        input_size <span class="token operator">=</span> <span class="token number">1</span>  <span class="token comment" spellcheck="true"># 前一层大小(亦为当前层大小)</span>        <span class="token keyword">for</span> fc_idx <span class="token keyword">in</span> range<span class="token punctuation">(</span>hidden_num<span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>fc<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">[</span>fc_idx<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 添加全连接层</span>            self<span class="token punctuation">.</span>relu<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 添加 ReLU 激活函数</span>            input_size <span class="token operator">=</span> hidden_size<span class="token punctuation">[</span>fc_idx<span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># 当前层的输出是下一层的输入</span>        <span class="token comment" spellcheck="true"># 创建输出层，其输入维度为最后一个隐藏层的输出维度，输出维度为1</span>        self<span class="token punctuation">.</span>last <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 定义前向传播函数</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        out <span class="token operator">=</span> x        <span class="token keyword">for</span> fc_idx <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_num<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># 通过循环将输入x依次传递给全连接层和ReLU激活函数</span>            out <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">[</span>fc_idx<span class="token punctuation">]</span><span class="token punctuation">(</span>out<span class="token punctuation">)</span>            out <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">[</span>fc_idx<span class="token punctuation">]</span><span class="token punctuation">(</span>out<span class="token punctuation">)</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>last<span class="token punctuation">(</span>out<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 将输出传递给输出层</span>        <span class="token keyword">return</span> out</code></pre><h3 id="定义相关变量"><a href="#定义相关变量" class="headerlink" title="定义相关变量"></a>定义相关变量</h3><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 加载数据集</span>datasets <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">"output.csv"</span><span class="token punctuation">,</span> header<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"key"</span><span class="token punctuation">,</span> <span class="token string">"position"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 将 Serious 数据转换成 PyTorch tensor 张量</span>keys <span class="token operator">=</span> <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>x<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> datasets<span class="token punctuation">[</span><span class="token string">"key"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>values<span class="token punctuation">]</span>pos <span class="token operator">=</span> <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>x<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> datasets<span class="token punctuation">[</span><span class="token string">"position"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>values<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># 第一个阶段加载所有数据</span>ds <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span>Dataset<span class="token punctuation">(</span>keys<span class="token punctuation">,</span> pos<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># ds 2 维: 行表示阶段,行内表示每阶段模型所需数据集---动态扩容</span>torch<span class="token punctuation">.</span>set_num_threads<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 设置运行计算 线程数 16</span>device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cpu"</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 运行在 CPU 上(有服务器的可运行在 GPU 上)</span>models <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># 2-dim: 定义不同阶段采用的模型</span>model_nums <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 定义不同阶段的模型数量(此处只有两个阶段: 第一个阶段1个模型,第二个阶段8个模型)</span>model_params <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 定义不同阶段模型参数: hidden_num, hidden_size</span>max_pos <span class="token operator">=</span> len<span class="token punctuation">(</span>keys<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 索引(位置) 的最大值, 方便确定下一阶段采用哪个模型</span>stage_nums <span class="token operator">=</span> len<span class="token punctuation">(</span>model_nums<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 阶段数量，论文图示为3个阶段，修改 model_nums、model_params 即可</span></code></pre><h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 创建一个2x9的子图布局</span><span class="token comment" spellcheck="true"># fig, axs = plt.subplots(2, 8)</span><span class="token keyword">for</span> stage_index <span class="token keyword">in</span> range<span class="token punctuation">(</span>stage_nums<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 遍历所有训练阶段---目前就两个阶段</span>    models<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 为每个阶段添加训练模型</span>    <span class="token keyword">if</span> stage_index <span class="token operator">!=</span> stage_nums <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 判断是否是最后一个阶段</span>        <span class="token comment" spellcheck="true"># 不是最后一个阶段时,需要为下一个阶段初始化数据集</span>        next_key <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>model_nums<span class="token punctuation">[</span>stage_index <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># 此处下一个阶段有8个模型，每个模型都需要key、pos,故需要初始化8个空的key、pos:2-dim,每个模型对应的数据集</span>        next_pos <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>model_nums<span class="token punctuation">[</span>stage_index <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> model_index <span class="token keyword">in</span> range<span class="token punctuation">(</span>model_nums<span class="token punctuation">[</span>stage_index<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 当前阶段模型训练数量</span>        error_mean <span class="token operator">=</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;&amp;#125;    # 记录每阶段每个模型的loss信息以便绘图</span>        <span class="token comment" spellcheck="true"># 初始化模型, 两个参数: 对应隐藏层数量, 每个隐藏层神经元数量</span>        model <span class="token operator">=</span> Network<span class="token punctuation">(</span>model_params<span class="token punctuation">[</span>stage_index<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> model_params<span class="token punctuation">[</span>stage_index<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>        criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 代价函数使用均方误差MSE</span>        optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 优化器使用 adam 算法, 学习率0.001</span>        <span class="token comment" spellcheck="true"># 从ds中按照批次大小为64加载当前阶段当前模型对应的样本数量，有序故不能打乱</span>        data_gen <span class="token operator">=</span> torch_data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>ds<span class="token punctuation">[</span>stage_index<span class="token punctuation">]</span><span class="token punctuation">[</span>model_index<span class="token punctuation">]</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        last_loss <span class="token operator">=</span> float<span class="token punctuation">(</span><span class="token string">"inf"</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 早停: loss 停止下降, 模型就停止训练</span>        <span class="token comment" spellcheck="true"># 打印i阶段j模型训练的数据量信息</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Stage=&amp;#123;&amp;#125;, Model=&amp;#123;&amp;#125;, 训练集: &amp;#123;&amp;#125; 个样本"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>stage_index<span class="token punctuation">,</span> model_index<span class="token punctuation">,</span> len<span class="token punctuation">(</span>ds<span class="token punctuation">[</span>stage_index<span class="token punctuation">]</span><span class="token punctuation">[</span>model_index<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 迭代最多 1000 次</span>        <span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Epoch"</span><span class="token punctuation">,</span> epoch<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 打印当前迭代次数（epoch）</span>            <span class="token comment" spellcheck="true"># 模型训练</span>            <span class="token keyword">for</span> local_keys<span class="token punctuation">,</span> local_pos <span class="token keyword">in</span> data_gen<span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 从数据加载器中依次提取keys、pos</span>                <span class="token comment" spellcheck="true"># 将 key 和 pos 移动到指定的设备上</span>                local_keys<span class="token punctuation">,</span> local_pos <span class="token operator">=</span> local_keys<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> local_pos<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>                <span class="token comment" spellcheck="true"># 使用模型对输入数据进行前向传播，得到输出结果（outputs）</span>                outputs <span class="token operator">=</span> model<span class="token punctuation">(</span>local_keys<span class="token punctuation">)</span>                <span class="token comment" spellcheck="true"># print(outputs)</span>                <span class="token comment" spellcheck="true"># 计算损失值---MSE</span>                loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> local_pos<span class="token punctuation">)</span>                <span class="token comment" spellcheck="true"># print("损失值为: ", "===", loss)</span>                optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 清空优化器的梯度缓存,</span>                loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 然后进行反向传播</span>                optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 计算梯度并更新模型参数</span>            <span class="token comment" spellcheck="true"># 计算当前迭代下当前获取的数据集的总损失值</span>            <span class="token keyword">with</span> torch<span class="token punctuation">.</span>set_grad_enabled<span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 设置梯度计算为禁用状态</span>                loss_sum <span class="token operator">=</span> <span class="token number">0.0</span>  <span class="token comment" spellcheck="true"># 总损失值</span>                <span class="token keyword">for</span> local_keys<span class="token punctuation">,</span> local_pos <span class="token keyword">in</span> data_gen<span class="token punctuation">:</span>                    local_keys<span class="token punctuation">,</span> local_pos <span class="token operator">=</span> local_keys<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> local_pos<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>                    outputs <span class="token operator">=</span> model<span class="token punctuation">(</span>local_keys<span class="token punctuation">)</span>                    loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> local_pos<span class="token punctuation">)</span>                    loss_sum <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>                <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Loss:"</span><span class="token punctuation">,</span> loss_sum <span class="token operator">/</span> len<span class="token punctuation">(</span>ds<span class="token punctuation">[</span>stage_index<span class="token punctuation">]</span><span class="token punctuation">[</span>model_index<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 打印总平均损失值</span>                error_mean<span class="token punctuation">[</span>epoch<span class="token punctuation">]</span> <span class="token operator">=</span> loss_sum <span class="token operator">/</span> len<span class="token punctuation">(</span>ds<span class="token punctuation">[</span>stage_index<span class="token punctuation">]</span><span class="token punctuation">[</span>model_index<span class="token punctuation">]</span><span class="token punctuation">)</span>                <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>loss_sum <span class="token operator">-</span> last_loss<span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">or</span> <span class="token punctuation">(</span>abs<span class="token punctuation">(</span>last_loss <span class="token operator">-</span> loss_sum<span class="token punctuation">)</span> <span class="token operator">/</span> last_loss <span class="token operator">&lt;</span> <span class="token number">0.001</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># 如果后一次比前一次大或者两次迭代比率小于lr均停止迭代</span>                    <span class="token keyword">break</span>                <span class="token keyword">else</span><span class="token punctuation">:</span>                    last_loss <span class="token operator">=</span> loss_sum   <span class="token comment" spellcheck="true"># 更新loss</span>        <span class="token comment" spellcheck="true"># 绘制各阶段各模型的损失函数图</span>        <span class="token comment" spellcheck="true"># axs[stage_index, model_index].plot(error_mean.keys(), error_mean.values())</span>        <span class="token comment" spellcheck="true"># axs[stage_index, model_index].set_title("Loss of S&amp;#123;&amp;#125;,M&amp;#123;&amp;#125;".format(stage_index, model_index))</span>        plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>error_mean<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> error_mean<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">"Loss of Stage_&amp;#123;&amp;#125;,Model_&amp;#123;&amp;#125;"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>stage_index<span class="token punctuation">,</span> model_index<span class="token punctuation">)</span><span class="token punctuation">)</span>        plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>        models<span class="token punctuation">[</span>stage_index<span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>model<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 将训练好的模型添加到模型树中</span>        <span class="token comment" spellcheck="true"># 开始上层 Model 向下层 Model 分发数据集</span>        <span class="token keyword">if</span> stage_index <span class="token operator">!=</span> stage_nums <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 若还不是最后一个阶段</span>            <span class="token comment" spellcheck="true"># 根据当前阶段和模型的索引，从数据集中加载样本</span>            data_gen <span class="token operator">=</span> torch_data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>ds<span class="token punctuation">[</span>stage_index<span class="token punctuation">]</span><span class="token punctuation">[</span>model_index<span class="token punctuation">]</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>            <span class="token keyword">for</span> local_keys<span class="token punctuation">,</span> local_pos <span class="token keyword">in</span> data_gen<span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 遍历数据加载器中的每个样本</span>                local_keys<span class="token punctuation">,</span> local_pos <span class="token operator">=</span> local_keys<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> local_pos<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>                <span class="token comment" spellcheck="true"># 计算下一阶段应该分配给哪个模型: 使用当前模型对当前样本的输出值进行处理，</span>                <span class="token comment" spellcheck="true"># 根据输出值、下一阶段模型的数量和最大位置来计算模型的选择</span>                output <span class="token operator">=</span> model<span class="token punctuation">(</span>local_keys<span class="token punctuation">)</span>                <span class="token comment" spellcheck="true"># 预测的pos*下一个阶段模型总数/最大的pos--->下取整</span>                model_select <span class="token operator">=</span> int<span class="token punctuation">(</span>output<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> model_nums<span class="token punctuation">[</span>stage_index <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">/</span> max_pos<span class="token punctuation">)</span>                <span class="token keyword">if</span> model_select <span class="token operator">>=</span> model_nums<span class="token punctuation">[</span>stage_index <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 超过上限取上限</span>                    model_select <span class="token operator">=</span> model_nums<span class="token punctuation">[</span>stage_index <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token number">1</span>                <span class="token keyword">elif</span> model_select <span class="token operator">&lt;=</span> <span class="token number">0</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 小于下限取下限</span>                    model_select <span class="token operator">=</span> <span class="token number">0</span>                                <span class="token comment" spellcheck="true"># 根据模型的选择，将当前样本添加到对应的数据集中</span>                next_key<span class="token punctuation">[</span>model_select<span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>local_keys<span class="token punctuation">)</span>                next_pos<span class="token punctuation">[</span>model_select<span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>local_pos<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 为下一个阶段创建对应数据集</span>    <span class="token keyword">if</span> stage_index <span class="token operator">!=</span> stage_nums <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">:</span>        ds<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 扩容---增加一行</span>        <span class="token keyword">for</span> next_model_index <span class="token keyword">in</span> range<span class="token punctuation">(</span>model_nums<span class="token punctuation">[</span>stage_index <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            ds<span class="token punctuation">[</span>stage_index <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>Dataset<span class="token punctuation">(</span>next_key<span class="token punctuation">[</span>next_model_index<span class="token punctuation">]</span><span class="token punctuation">,</span> next_pos<span class="token punctuation">[</span>next_model_index<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 将对应数据集加载进去</span></code></pre><h3 id="训练结果"><a href="#训练结果" class="headerlink" title="训练结果"></a>训练结果</h3><table><thead><tr><th>Stage=1, Model=0, 训练集: 1256 个样本<br>Epoch 0<br>Loss: 27719.089567731142<br>……<br>Epoch 96<br>Loss: 1.5727592304254034<br>Epoch 97<br>Loss: 1.5714545656161703</th><th>Stage=1, Model=1, 训练集: 1263 个样本<br>Epoch 0<br>Loss: 42152.201207442595<br>……<br>Epoch 134<br>Loss: 4.268974411421509<br>Epoch 135<br>Loss: 4.265463400906452</th></tr></thead><tbody><tr><td><img src="/posts/image-20240412225034325.png" alt="img"></td><td><img src="/posts/image-20240412225045243.png" alt="img"></td></tr><tr><td>Stage=1, Model=2, 训练集: 1243 个样本<br>Epoch 0<br>Loss: 236817.26387771522<br>……<br>Epoch 125<br>Loss: 0.8422801908905815<br>Epoch 126<br>Loss: 0.8416047759911559</td><td>Stage=1, Model=3, 训练集: 1310 个样本<br>Epoch 0<br>Loss: 775904.1709923664<br>……<br>Epoch 57<br>Loss: 3.1448414471313244<br>Epoch 58<br>Loss: 3.1428348242781543</td></tr><tr><td><img src="/posts/image-20240412225053260.png" alt="img"></td><td><img src="/posts/image-20240412225058837.png" alt="img"></td></tr><tr><td>Stage=1, Model=4, 训练集: 1203 个样本<br>Epoch 0<br>Loss: 782197.6059850374<br>……<br>Epoch 104<br>Loss: 12.20371767013943<br>Epoch 105<br>Loss: 12.193250914563365</td><td>Stage=1, Model=5, 训练集: 1237 个样本<br>Epoch 0<br>Loss: 912518.8100242523<br>……<br>Epoch 252<br>Loss: 2.42460509896182<br>Epoch 253<br>Loss: 2.422264087171254</td></tr><tr><td><img src="/posts/image-20240412225106470.png" alt="img"></td><td><img src="/posts/image-20240412225111059.png" alt="img"></td></tr><tr><td>Stage=1, Model=6, 训练集: 1277 个样本<br>Epoch 0<br>Loss: 1886042.2803445575<br>……<br>Epoch 81<br>Loss: 1.573664175315863<br>Epoch 82<br>Loss: 1.572465995185153</td><td>Stage=1, Model=7, 训练集: 1211 个样本<br>Epoch 0<br>Loss: 1195227.937241949<br>……<br>Epoch 297<br>Loss: 2.0064403804051394<br>Epoch 298<br>Loss: 2.0046200957010836</td></tr><tr><td><img src="/posts/image-20240412225118146.png" alt="img"></td><td><img src="/posts/image-20240412225123070.png" alt="img"></td></tr></tbody></table><hr><h3 id="模型测试"><a href="#模型测试" class="headerlink" title="模型测试"></a>模型测试</h3><pre class=" language-python"><code class="language-python">test_ds <span class="token operator">=</span> Dataset<span class="token punctuation">(</span>keys<span class="token punctuation">,</span> pos<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 创建数据集 test_ds</span><span class="token comment" spellcheck="true"># 创建数据加载器 test_gen，每次只加载一个样本</span>test_gen <span class="token operator">=</span> torch_data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>test_ds<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>err_sum <span class="token operator">=</span> <span class="token number">0</span>  <span class="token comment" spellcheck="true"># 模型树总的 error</span><span class="token keyword">for</span> local_keys<span class="token punctuation">,</span> local_pos <span class="token keyword">in</span> test_gen<span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 遍历 数据加载器</span>    local_keys<span class="token punctuation">,</span> local_pos <span class="token operator">=</span> local_keys<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> local_pos<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>    model_select <span class="token operator">=</span> <span class="token number">0</span>  <span class="token comment" spellcheck="true"># 默认选择模型0</span>    <span class="token keyword">for</span> stage_index <span class="token keyword">in</span> range<span class="token punctuation">(</span>stage_nums<span class="token punctuation">)</span><span class="token punctuation">:</span>        model <span class="token operator">=</span> models<span class="token punctuation">[</span>stage_index<span class="token punctuation">]</span><span class="token punctuation">[</span>model_select<span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># 当前阶段下的模型</span>        output <span class="token operator">=</span> model<span class="token punctuation">(</span>local_keys<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 计算当前模型的输出</span>                <span class="token keyword">if</span> stage_index <span class="token operator">!=</span> stage_nums <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 如果不是最后一个阶段</span>            <span class="token comment" spellcheck="true"># 根据输出值、下一阶段模型的数量和最大位置来计算模型的选择</span>            model_select <span class="token operator">=</span> int<span class="token punctuation">(</span>output<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> model_nums<span class="token punctuation">[</span>stage_index <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">/</span> max_pos<span class="token punctuation">)</span>            <span class="token keyword">if</span> model_select <span class="token operator">>=</span> model_nums<span class="token punctuation">[</span>stage_index <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 超过上限取上限</span>                model_select <span class="token operator">=</span> model_nums<span class="token punctuation">[</span>stage_index <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token number">1</span>            <span class="token keyword">elif</span> model_select <span class="token operator">&lt;=</span> <span class="token number">0</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 小于下限取下限</span>                model_select <span class="token operator">=</span> <span class="token number">0</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># 分别获取 output 和 pos 张量中的标量值,然后转换为整数类型计算它们的差值</span>            err_sum <span class="token operator">+=</span> abs<span class="token punctuation">(</span>int<span class="token punctuation">(</span>output<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">-</span> int<span class="token punctuation">(</span>local_pos<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 最后一个阶段,累加预测值-真实值</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Final Loss:"</span><span class="token punctuation">,</span> float<span class="token punctuation">(</span>err_sum<span class="token punctuation">)</span> <span class="token operator">/</span> len<span class="token punctuation">(</span>test_ds<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 输出总的误差均值</span></code></pre><pre><code>Final Loss: 11.5413</code></pre><h2 id="改进策略"><a href="#改进策略" class="headerlink" title="改进策略"></a>改进策略</h2><p>这里运用了全连接层的BP进行简单地回归预测，亦可以根据具体实例采用适当的回归算法进行拟合，以提高精度；或者再增加一层，适当扩大每层的模型数，但这样会增加训练时间，需要折中感觉。这里仅仅是索引的训练，即构建索引，至于增，删等操作对索引带来的影响导致索引如何变更需要进一步研读论文掌握其思想加以认真思考得出。对于多维索引的构建可以结合Z-order，即目前我所研究的那篇Blinkhash就是在分布式环境下整合突破了hash索引与B+树索引的深度融合，解决时序数据最右节点热争抢问题，但其构建的索引也是递增有序的，多维数据可以用z-order降为一维，此处应该亦可以和ML相关算法结合来构建索引已解决多维索引问题。</p><hr>]]></content>
      
      
      <categories>
          
          <category> 实验 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 实验 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LearnedIndex</title>
      <link href="posts/36c1.html"/>
      <url>posts/36c1.html</url>
      
        <content type="html"><![CDATA[<h2 id="The-Case-for-Learned-Index-Structures"><a href="#The-Case-for-Learned-Index-Structures" class="headerlink" title="The Case for Learned Index Structures"></a>The Case for Learned Index Structures</h2><p>注：作者认为索引即模型，故研究了如何通过 ML 模型 替换 传统数据库索引结构——B树、哈希、布隆过滤器。</p><h3 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h3><p>思想的来源：传统数据库</p><p>然而，所有这些索引仍然是通用数据结构；他们对数据分布没有任何假设，也没有利用现实世界数据中更常见的模式。例如，如果目标是构建一个高度调整的系统来存储和查询一组连续整数键（例如键 1 到 100M）上的固定长度记录范围，则不会使用传统的 B 树对键进行索引，因为键本身可以用作偏移量，这使其成为查找任何键或一系列键的开头的 O(1) 而不是 O(logn) 操作。同样，索引内存大小将从 O(n) 减少到 O(1)。也许令人惊讶的是，其他数据模式也可以进行类似的优化。换句话说，了解确切的数据分布可以高度优化几乎任何索引结构。</p><p>当然，在大多数现实用例中，数据并不完全遵循已知模式，并且为每个用例构建专门解决方案的工程工作量通常过高。然而，我们认为机器学习（ML）提供了学习反映数据模式的模型的机会，从而能够以较低的工程成本自动合成专门的索引结构（称为学习索引）。</p><p>在本文中，我们探讨了学习模型（包括神经网络）可在多大程度上用于增强甚至取代从 B 树到布隆过滤器的传统索引结构。这似乎有悖常理，因为机器学习无法提供我们传统上与这些索引相关联的语义保证，而且传统上认为最强大的机器学习模型、神经网络的计算成本非常高。然而，我们认为这些明显的障碍并不像它们看起来那样有问题。相反，我们使用学习模型的建议有可能带来显着的好处，特别是在下一代硬件上。</p><p>就语义保证而言，索引在很大程度上已经是学习模型，因此用其他类型的 ML 模型替换它们变得非常简单。例如，B 树可以被视为一种模型，它以键作为输入并预测数据记录在排序集中的位置（必须对数据进行排序以实现有效的范围请求）。布隆过滤器是一种二元分类器，它基于键来预测某个键是否存在于集合中。显然，存在微妙但重要的差异。例如，布隆过滤器可以有误报，但不能有误报。【For example, a Bloom filter can have false positives but not false negatives.】然而，正如我们将在本文中展示的那样，可以通过新颖的学习技术和/或简单的辅助数据结构来解决这些差异。</p><p>在性能方面，我们观察到每个CPU都已经具备强大的SIMD功能，并且我们推测许多笔记本电脑和手机很快就会拥有图形处理单元（GPU）或张量处理单元（TPU）。推测 CPU-SIMD/GPU/TPU 将变得越来越强大也是合理的，因为与通用指令集相比，扩展神经网络使用的有限（并行）数学运算集要容易得多。因此，执行神经网络或其他机器学习模型的高成本在未来实际上可能可以忽略不计。例如，Nvidia 和 Google 的 TPU 已经能够在单个周期内执行数千甚至数万个神经网络操作 [3]。</p><p>此外，据称到 2025 年 GPU 的性能将提高 1000 倍，而 CPU 的摩尔定律基本上已经失效 [5]。通过用神经网络取代分支密集的索引结构，数据库和其他系统可以从这些硬件趋势中受益。虽然我们看到了 TPU 等专用硬件上学习索引结构的未来，但本文完全关注 CPU，并且令人惊讶地表明，即使在这种情况下，我们也可以实现显着的优势。</p><p>值得注意的是，我们并不主张用学习索引完全取代传统索引结构。相反，本文的主要贡献是概述和评估一种建立索引的新方法的潜力，该方法补充了现有的工作，并且可以说为这个已有数十年历史的领域开辟了一个全新的研究方向。这是基于以下关键观察：许多数据结构可以分解为学习模型和辅助结构，以提供相同的语义保证。这种方法的潜在力量来自于这样一个事实：描述数据分布的连续函数可用于构建更高效的数据结构或算法。在针对只读分析工作负载的合成数据集和真实世界数据集评估我们的方法时，我们根据经验得到了非常有希望的结果。然而，许多开放的挑战仍然存在，例如如何处理写入繁重的工作负载，我们概述了未来工作的许多可能的方向。此外，我们相信我们可以使用相同的原理来替换（数据库）系统中常用的其他组件和操作。如果成功，将学习模型深度嵌入算法和数据结构的核心思想可能会导致系统开发方式的彻底背离。</p><p>本文的其余部分概述如下：在接下来的两节中，我们以 B 树为例介绍学习索引的一般思想。在第 4 节中，我们将这个想法扩展到哈希映射，并在第 5 节中扩展到布隆过滤器。所有部分都包含单独的评估。最后，在第 6 节中，我们讨论相关工作并在第 7 节中得出结论。</p><hr><h3 id="2-B树—范围查询"><a href="#2-B树—范围查询" class="headerlink" title="2. B树—范围查询"></a>2. B树—范围查询</h3><p>范围索引结构（如 B 树）已经是模型：给定一个键，它们“预测”键排序集中的值的位置。要看到这一点，请考虑分析内存数据库中已排序的主键列上的 B 树索引（即只读），如图 1(a) 所示。在这种情况下，B 树提供从查找键到已排序记录数组内的位置的映射，并保证该位置处的记录的键是等于或高于查找的第一个键钥匙。必须对数据进行排序以允许有效的范围请求。这个相同的一般概念也适用于二级索引，其中数据将是 &lt;key,record_pointer&gt; 对的列表，其中键是索引值，指针是对记录的引用。</p><p><img src="/posts/image-20240412215411356.png" alt="img"></p><p>出于效率原因，通常不会对已排序记录的每个键进行索引，而是仅对每个第 n 条记录的键（即页面的第一个键）进行索引。这里我们只假设固定长度记录和连续内存区域（即单个数组）上的逻辑分页，而不是位于不同内存区域的物理页（物理页和可变长度记录在附录 D.2 中讨论）。仅对每个页面的第一个键建立索引有助于显着减少索引必须存储的键数，而不会造成任何显着的性能损失。因此，B 树是一个模型，或者用 ML 术语来说，是一棵回归树：它将键映射到具有最小和最大误差的位置（最小误差为 0，最大误差为页面误差 page-size），并保证可以在该区域中找到该密钥（如果存在）。因此，我们可以用其他类型的 ML 模型（包括神经网络）替换该索引，只要它们也能够对最小和最大误差提供类似的强有力保证。</p><p>乍一看，似乎很难为其他类型的ML模型提供相同的保证，但实际上它非常简单。首先，B-Tree只对存储的键提供最小和最大错误保证，而不是对所有可能的键提供最大和最小错误保证。对于新数据，b树需要重新平衡，或者在机器学习术语中重新训练，以仍然能够提供相同的错误保证。也就是说，对于单调模型，我们唯一需要做的就是对每个键执行模型，并记住位置的最坏预测，以计算最小和最大误差；其次，更重要的是，甚至不需要强错误界限。</p><p>无论如何，数据必须排序以支持范围请求，因此任何错误都可以通过围绕预测的局部搜索(例如，使用指数搜索)轻松纠正，因此，甚至允许非单调模型。因此，我们可以用任何其他类型的回归模型代替b树，包括线性回归或神经网络(见图1(b))。</p><p>现在，在用学习索引取代 B 树之前，我们还需要解决其他技术挑战。例如，B 树的插入和查找成本有限，并且特别擅长利用缓存。此外，B 树可以将键映射到未连续映射到内存或磁盘的页面。所有这些都是有趣的挑战/研究问题，并在本节和附录中更详细地解释，以及潜在的解决方案。</p><p>同时，使用其他类型的模型作为索引可以提供巨大的好处。最重要的是，它有可能将 B 树查找的 logn 成本转换为常量操作。例如，假设一个数据集具有 1M 个唯一键，其值介于 1M 和 2M 之间（因此值 1,000,009 存储在位置 10）。在这种情况下，由单个乘法和加法组成的简单线性模型可以完美预测点查找或范围扫描的任何键的位置，而 B 树则需要 logn 操作。机器学习（尤其是神经网络）的美妙之处在于它们能够学习各种数据分布、混合以及其他数据特性和模式。挑战在于平衡模型的复杂性和准确性。</p><p>对于本文的大部分讨论，我们保留本节的简化假设：我们仅索引按键排序的内存中密集数组。这似乎是有限制的，但是许多现代硬件优化的 B 树，例如 FAST [44]，做出了完全相同的假设，并且这些索引对于内存数据库系统来说非常常见，因为它们在扫描方面具有卓越的性能 [44, 48]或二分查找。</p><p>然而，虽然我们的一些技术可以很好地适应某些场景（例如，具有非常大块的磁盘驻留数据，例如 Bigtable [23] 中使用的），但对于其他场景（细粒度分页、插入繁重的工作负载等） .）需要更多的研究。在附录 D.2 中，我们更详细地讨论了其中一些挑战和潜在的解决方案。</p><hr><h4 id="2-1-什么样的模型复杂度是可以接受的？"><a href="#2-1-什么样的模型复杂度是可以接受的？" class="headerlink" title="2.1 什么样的模型复杂度是可以接受的？"></a>2.1 什么样的模型复杂度是可以接受的？</h4><p>为了更好地理解模型的复杂性，重要的是要知道在遍历 B 树所需的相同时间内可以执行多少个操作，以及模型需要达到什么精度才能比 B 树更高效。</p><p>考虑一个索引 100M 记录、页大小为 100 的 B 树。我们可以将每个 B 树节点视为一种划分空间的方式，减少“错误”并缩小查找数据的区域。因此，我们说页面大小为 100 的 B 树每个节点的精度增益为 1/100，并且我们总共需要遍历 log100N 个节点。因此，第一个节点将空间从 100M 划分为 100M/100 = 1M，第二个节点从 1M 划分为 1M/100 = 10k，依此类推，直到找到记录。</p><p>现在，使用二分搜索遍历单个 B 树页面大约需要 50 个周期，并且众所周知很难并行化3。相比之下，现代 CPU 每个周期可以执行 8-16 个 SIMD 操作。</p><p>因此，只要每 50 * 8 = 400 次算术运算的精度增益高于 1/100，模型就会更快。请注意，此计算仍然假设所有 B-Tree 页面都在缓存中。一次缓存未命中会花费 50-100 个额外周期，因此可以支持更复杂的模型。</p><p>此外，机器学习加速器正在彻底改变游戏规则。它们允许在相同的时间内运行更复杂的模型，并减轻 CPU 的计算负担。例如，NVIDIA最新的Tesla V100 GPU能够实现120 TeraFlops的低精度深度学习算术运算（约每周期60, 000次运算）。</p><p>假设整个学习索引适合 GPU 的内存（我们在第 3.7 节中表明这是一个非常合理的假设），在短短 30 个周期内，我们可以执行 100 万个神经网络操作。当然，传输输入和从 GPU 检索结果的延迟仍然明显较高，但考虑到批处理和/或最近更紧密地集成 CPU/GPU/TPU 的趋势，这个问题并非无法克服 [4]。最后，可以预期的是，GPU/TPU 的能力和每秒浮点/整数运算的数量将继续增加，而提高 CPU 执行 if 语句性能的进展基本上已经停滞 [5]。尽管我们认为 GPU/TPU 是在实践中采用学习索引的主要原因之一，但在本文中，我们将重点放在更有限的 CPU 上，以更好地研究通过机器学习替换和增强索引的影响，而不会影响硬件变化。</p><hr><h4 id="2-2-范围索引模型是CDF模型"><a href="#2-2-范围索引模型是CDF模型" class="headerlink" title="2.2 范围索引模型是CDF模型"></a>2.2 范围索引模型是CDF模型</h4><p>正如本节开头所述，索引是一种以键作为输入并预测记录位置的模型。对于点查询，记录的顺序并不重要，而对于范围查询，必须根据查找键对数据进行排序，以便可以有效地检索范围内（例如，时间范围内）的所有数据项。这导致了一个有趣的观察：预测排序数组中给定键的位置的模型有效地近似了累积分布函数（CDF）。我们可以对数据的 CDF 进行建模来预测位置：        p = F (Key) ∗ N        (1)。</p><p>其中 p 是位置估计，F (Key) 是数据的估计累积分布函数，用于估计观察到小于或等于查找键 P(X ≤ Key) 的键的可能性，N 是键总数（另请参见图 2）。</p><p><img src="/posts/image-20240412215445045.png" alt="img"></p><p>这一观察开辟了一组全新的有趣方向：首先，它意味着索引实际上需要学习数据分布。 B 树通过构建回归树来“学习”数据分布。线性回归模型将通过最小化线性函数的（平方）误差来学习数据分布。其次，估计数据集的分布是一个众所周知的问题，学习的索引可以从数十年的研究中受益。第三，学习 CDF 在优化其他类型的索引结构和潜在算法方面也发挥着关键作用，我们将在本文后面概述研究 。第四，关于理论 CDF 与经验 CDF 的接近程度的研究由来已久，这为从理论上理解这种方法的好处提供了立足点 [28]。我们在附录 A 中对我们的方法的扩展程度进行了高级理论分析。</p><hr><h4 id="2-3-第一个学习型索引"><a href="#2-3-第一个学习型索引" class="headerlink" title="2.3 第一个学习型索引"></a>2.3 第一个学习型索引</h4><p>为了更好地理解通过学习模型替换 B 树的要求，我们使用了 200M 的 Web 服务器日志记录，目的是使用 Tensorflow [9] 在时间戳上构建二级索引。我们使用 ReLU 激活函数训练了一个每层 32 个神经元的两层全连接神经网络；时间戳是输入特征，排序数组中的位置是标签。之后，我们使用 Tensorflow 和 Python 作为前端，测量了随机选择的键的查找时间（在几次运行中平均，不考虑第一个数字）。</p><p>在此设置中，我们每秒实现了约 1250 次预测，即使用 Tensorflow 执行模型需要约 80, 000 纳秒 (ns)，而无需搜索时间（从预测位置找到实际记录的时间）。作为比较点，对相同数据进行 B 树遍历大约需要 300 纳秒，而对整个数据进行二分搜索大约需要 900 纳秒。仔细观察，我们发现我们的简单方法在几个关键方面受到限制：（1）Tensorflow 被设计为有效地运行更大的模型，而不是小模型，因此，具有显着的调用开销，特别是使用 Python 作为前端 -结尾。 (2) B 树，或者一般的决策树，非常擅长通过一些操作来过度拟合数据，因为它们使用简单的 if 语句递归地划分空间。相比之下，其他模型可以更有效地近似 CDF 的一般形状，但在单个数据实例级别上存在准确性问题。要了解这一点，请再次考虑图 2。该图表明，从顶层视图来看，CDF 函数显得非常平滑和规则。然而，如果放大到单条记录，就会发现越来越多的违规行为；众所周知的统计效应。因此，神经网络、多项式回归等模型可能会更高效地使用 CPU 和空间，将整个数据集中某个项目的位置缩小到数千个区域，但单个神经网络为了“最后一英里”，通常需要更多的空间和 CPU 时间将错误从数千进一步减少到数百。 (3) B 树的缓存和操作效率极高，因为它们始终将顶部节点保留在缓存中，并在需要时访问其他页面。相比之下，标准神经网络需要所有权重来计算预测，这在乘法次数方面具有很高的成本。</p><h3 id="3-RM-INDEX"><a href="#3-RM-INDEX" class="headerlink" title="3. RM-INDEX"></a>3. RM-INDEX</h3><p>为了克服挑战并探索模型作为索引替代或优化的潜力，我们开发了<strong>学习索引框架 (LIF)、递归模型索引 (RMI) 和基于标准误差的搜索策略</strong>。我们主要关注简单、完全连接的神经网络，因为它们简单且灵活，但我们相信其他类型的模型可能会提供额外的好处。</p><h4 id="3-1-学习索引框架（LIF）"><a href="#3-1-学习索引框架（LIF）" class="headerlink" title="3.1 学习索引框架（LIF）"></a>3.1 学习索引框架（LIF）</h4><p>LIF 可以看作是一个指标综合系统；给定索引规范，LIF 会生成不同的索引配置、优化它们并自动测试它们。虽然 LIF 可以即时学习简单模型（例如线性回归模型），但它依赖于 Tensorflow 来学习更复杂的模型（例如神经网络）。然而，它在推理时从不使用 Tensorflow。</p><p>相反，给定一个经过训练的 Tensorflow 模型，LIF 会自动从模型中提取所有权重，并根据模型规范在 C++ 中生成有效的索引结构。我们的代码生成是专门为小型模型设计的，消除了 Tensorflow 管理大型模型所需的所有不必要的开销和仪器。在这里，我们利用[25]中的想法，它已经展示了如何避免 Spark 运行时不必要的开销。因此，我们能够在 30 纳秒的时间内执行简单的模型。</p><p>然而，应该指出的是，LIF 仍然是一个实验性框架，用于快速评估不同的索引配置（例如，ML 模型、页面大小、搜索策略等），这会以额外计数器的形式引入额外的开销，虚拟函数调用等。除了编译器完成的矢量化之外，我们不使用特殊的 SIMD 内在函数。虽然这些低效率在我们的评估中并不重要，因为我们通过始终使用我们的框架来确保公平比较，但在生产环境中或在将报告的性能数据与其他实现进行比较时，应考虑/避免这些低效率</p><h4 id="3-2-递归模型索引"><a href="#3-2-递归模型索引" class="headerlink" title="3.2 递归模型索引"></a>3.2 递归模型索引</h4><p>正如第 2.3 节所述，构建替代学习模型来替代 B 树的主要挑战之一是最后一英里搜索的准确性。例如，使用单个模型将 100M 记录的预测误差降低到数百个量级通常很困难。同时，即使使用简单的模型，将误差从 100M 降低到 10k，例如通过模型替换 B-Tree 的前 2 层，精度增益为 100 * 100 = 10000，也更容易实现。同样，将误差从 10k 减少到 100 是一个更简单的问题，因为模型可以只关注数据的子集。</p><p>基于这一观察并受到专家工作的启发 [62]，我们提出了递归回归模型（见图 3）。也就是说，我们构建模型的层次结构，在每个阶段模型将密钥作为输入，并基于它选择另一个模型，直到最后阶段预测位置。</p><p><img src="/posts/image-20240412215509381.png" alt="img"></p><p>更正式地说，对于我们的模型 f (x)，其中 x 是键，y ∈ [0, N) 位置，我们假设在阶段 l 有 M<del>l</del> 个模型。我们在阶段 0 训练模型，f0(x) ≈ y。因此，阶段 中的模型 k（表示为 f (k) ）经过损失训练：</p><p><img src="/posts/image-20240412215519648.png" alt="img"></p><p>考虑不同模型的一种方法是，每个模型都会对密钥的位置做出具有一定误差的预测，并且该预测用于选择下一个模型，该模型负责密钥空间的特定区域以做出更好的预测和更低的误差。但是，递归模型索引不一定是树。如图3所示，一个阶段的不同模型有可能在下一个阶段选择相同的模型。此外，每个模型不一定像 B 树那样覆盖相同数量的记录（即页面大小为 100 的 B 树覆盖 100 条或更少的记录）。最后，根据所使用的模型，不同的阶段不一定被解释为位置估计，而应被视为选择对某些键有更好了解的专家（另见[62]）。</p><p>这种模型架构有几个好处：（1）它将模型大小和复杂性与执行成本分开。 (2)它利用了易于学习数据分布的整体形状的事实。 (3)它有效地将空间划分为更小的子范围，就像B树一样，可以更轻松地以更少的操作实现所需的“最后一英里”精度。 (4) 阶段之间不需要搜索过程。例如，模型1.1的输出直接用于下一阶段的模型选取。这不仅减少了管理结构的指令数量，还允许将整个索引表示为 TPU/GPU 的稀疏矩阵乘法。</p><h4 id="3-3-混合索引"><a href="#3-3-混合索引" class="headerlink" title="3.3 混合索引"></a>3.3 混合索引</h4><p>递归模型索引的另一个优点是，我们能够构建模型的混合。例如，虽然在顶层，小型 ReLU 神经网络可能是最佳选择，因为它们通常能够学习各种复杂的数据分布，但模型层次结构底部的模型可能是数千个简单的线性回归模型，因为它们在空间和执行时间上都很便宜。此外，如果数据特别难学习，我们甚至可以在底层使用传统的 B 树。</p><p>在本文中，我们重点关注两种类型的模型：具有零到两个全连接隐藏层和 ReLU 激活函数的简单神经网络以及最多 32 个神经元的层宽度和 B 树（又名决策树）。请注意，零隐藏层神经网络相当于线性回归。给定索引配置（将阶段数和每个阶段的模型数指定为大小数组），混合索引的端到端训练如算法 1 所示完成：</p><p><img src="/posts/image-20240412215529772.png" alt="img"></p><p>从整个数据集（第 3 行）开始，它首先训练顶部节点模型。根据此顶部节点模型的预测，它会从下一阶段（第 9 行和第 10 行）中选择模型，并添加属于该模型的所有键（第 10 行）。最后，在混合索引的情况下，如果绝对最小/最大误差高于预定义阈值（第 11-14 行），则通过用 B 树替换 NN 模型来优化索引。</p><p>请注意，我们在最后阶段存储每个模型的标准误差和最小误差和最大误差。这样做的好处是，我们可以根据每个键所使用的模型单独限制搜索空间。目前，我们通过简单的网格搜索来调整模型的各种参数（即阶段数、每个模型的隐藏层等）。然而，存在许多潜在的优化来加速从 ML 自动调整到采样的训练过程。</p><p>请注意，混合索引允许我们将学习索引的最坏情况性能与 B 树的性能结合起来。也就是说，在极难学习数据分布的情况下，所有模型都会自动替换为 B-Tree，使其几乎成为一整棵 B-Tree。</p><h4 id="3-4-搜索策略和单调性"><a href="#3-4-搜索策略和单调性" class="headerlink" title="3.4 搜索策略和单调性"></a>3.4 搜索策略和单调性</h4><p>范围索引通常实现 upper_bound(key) [lower_bound(key)] 接口来查找排序数组中第一个等于或高于[低于]查找键的键的位置，以有效地支持范围请求。因此，对于学习的范围索引，我们必须根据预测从查找键中找到第一个较高[较低]的键。尽管付出了很多努力，但人们一再报道[8]，二分搜索或扫描具有小有效负载的记录通常是在排序数组中查找键的最快策略，因为替代技术的额外复杂性很少得到回报。然而，学习索引在这里可能有一个优势：模型实际上预测键的位置，而不仅仅是键的区域（即页面）。在这里，我们讨论两种利用此信息的简单搜索策略：</p><p>模型偏向搜索：我们的默认搜索策略，与传统的二分搜索的唯一不同之处在于第一个中间点设置为模型预测的值。</p><p>偏向四元搜索：四元搜索采用三个点而不是一个分割点，希望硬件一次预取所有三个数据点，以便在数据不在缓存中时获得更好的性能。在我们的实现中，我们将四元搜索的初始三个中间点定义为 pos − σ,pos,pos + σ。也就是说，我们猜测我们的大多数预测都是准确的，并将我们的注意力首先集中在位置估计上，然后我们继续传统的四元搜索。</p><p>对于我们所有的实验，我们使用最小和最大误差作为所有技术的搜索区域。也就是说，我们对每个键执行 RMI 模型，并存储每个最后阶段模型最差的过度预测和不足预测。虽然此技术保证找到所有现有键，但对于不存在的键，如果 RMI 模型不是单调的，则可能会返回错误的上限或下限。为了克服这个问题，一种选择是强制我们的 RMI 模型是单调的，正如机器学习中所研究的那样 [41, 71]。</p><p>或者，对于非单调模型，我们可以自动调整搜索区域。也就是说，如果找到的上（下）界键位于由最小和最大误差定义的搜索区域的边界上，我们将逐步调整搜索区域。</p><p>然而，另一种可能性是使用指数搜索技术。假设误差呈正态分布，这些技术平均应该与替代搜索策略一样好，同时不需要存储任何最小和最大误差。</p><h4 id="3-5-索引字符串"><a href="#3-5-索引字符串" class="headerlink" title="3.5 索引字符串"></a>3.5 索引字符串</h4><p>我们主要关注于索引真实值键，但许多数据库依赖于索引字符串，幸运的是，重要的机器学习研究集中在建模字符串上。和以前一样，我们需要设计一个高效且富有表现力的字符串模型。对于字符串来说，做好这件事会带来许多独特的挑战。</p><p>第一个设计考虑因素是如何将字符串转换为模型的特征，通常称为标记化。为了简单和高效，我们将 n 长度的字符串视为特征向量 x ∈ Rn，其中 xi 是 ASCII 十进制值（或 Unicode 十进制值，具体取决于字符串）。此外，如果所有输入的大小相同，大多数机器学习模型的运行效率会更高。因此，我们将设置最大输入长度 N。因为数据是按字典顺序排序的，所以我们在标记化之前将键截断为长度 N。对于长度为 n &lt; N 的字符串，当 i &gt; n 时，我们设置 xi = 0。</p><p>为了提高效率，我们通常遵循与真实值输入类似的建模方法。我们学习相对较小的前馈神经网络的层次结构。唯一的区别是输入不是单个实值 x 而是向量 x。线性模型 w · x + b 随输入长度 N 线性缩放乘法和加法次数。即使具有宽度为 h 的单个隐藏层的前馈神经网络也将缩放 O(hN) 乘法和加法。</p><p>最终，我们相信未来有大量研究可以优化字符串键的学习索引。例如，我们可以轻松想象其他标记化算法。在自然语言处理方面，有大量关于字符串标记化的研究，旨在将字符串分解为对 ML 模型更有用的片段，例如翻译中的单词片段 [70]。此外，将后缀树的想法与学习索引相结合以及探索更复杂的模型架构（例如循环神经网络和卷积神经网络）可能会很有趣。</p><h4 id="3-6-训练"><a href="#3-6-训练" class="headerlink" title="3.6 训练"></a>3.6 训练</h4><p>虽然训练（即加载）时间不是本文的重点，但应该指出的是，我们所有的模型，浅层神经网络甚至简单的线性/多元回归模型，训练速度都相对较快。虽然简单的神经网络可以使用随机梯度下降进行有效训练，并且可以在随机数据上通过不到一到几次的时间收敛，但线性多变量模型（例如，0 层神经网络）存在封闭形式的解决方案，并且它们可以对排序后的数据进行一次训练。因此，对于200M的记录训练一个简单的RMI索引不会比几秒钟长多少时间（当然，这取决于执行多少自动调整）；神经网络可以对每个模型进行几分钟的训练，具体取决于复杂性。另请注意，通常不需要在整个数据上训练顶层模型，因为即使在对整个随机数据进行单次扫描之前，这些模型也经常会收敛。这部分是因为我们使用简单的模型，不太关心精度的最后几个数字点，因为它对索引性能影响很小。</p><p>最后，机器学习社区 [27, 72] 关于改善学习时间的研究适用于我们的背景，我们预计未来会有很多这方面的研究。</p><hr><h3 id="4-哈希—点查询"><a href="#4-哈希—点查询" class="headerlink" title="4. 哈希—点查询"></a>4. 哈希—点查询</h3><p>除了范围索引之外，用于点查找的哈希映射在 DBMS 中也发挥着同样重要的作用。从概念上讲，哈希映射使用哈希函数确定性地将键映射到数组内的位置（参见图 7(a)）。任何有效的哈希映射实现的关键挑战是防止太多不同的键被映射到哈希映射内的相同位置，下文称为冲突。例如，假设有 100M 条记录，哈希映射大小为 100M。对于统一随机化密钥的哈希函数，可以与生日悖论类似地导出预期冲突的数量，并且预期约为 33% 或 33M 个槽。对于每一个冲突，Hash-map架构都需要处理这个冲突。例如，单独的链接哈希映射将创建一个链接列表来处理冲突（参见图 7(a)）。然而，存在许多替代方案，包括二次探测、使用具有多个槽的桶，直至同时使用多个哈希函数（例如，如 Cuckoo Hashing [57] 所做的那样）。</p><p><img src="/posts/image-20240412215542041.png" alt="img"></p><p>然而，无论哈希映射架构如何，冲突都会对性能和/或存储要求产生重大影响，并且机器学习模型可能提供减少冲突数量的替代方案。虽然将学习模型作为哈希函数的想法并不新鲜，但现有技术并未利用底层数据分布。例如，各种完美哈希技术[26]也试图避免冲突，但用作哈希函数一部分的数据结构随着数据大小而增长；学习模型可能没有的属性（回想一下，对 1 到 100M 之间的所有键进行索引的示例）。据我们所知，尚未探索是否可以学习产生更有效点索引的模型。</p><h4 id="4-1-哈希模型索引"><a href="#4-1-哈希模型索引" class="headerlink" title="4.1 哈希模型索引"></a>4.1 哈希模型索引</h4><p>令人惊讶的是，学习密钥分布的 CDF 是学习更好的哈希函数的一种潜在方法。然而，与范围索引相比，我们的目的并不是紧凑地或严格排序地存储记录。相反，我们可以通过哈希映射的目标大小 M 来缩放 CDF，并使用 h(K) = F (K)*M，其中密钥 K 作为我们的哈希函数。如果模型 F 完美地学习了按键的经验 CDF，则不会存在冲突。此外，散列函数与实际的散列映射架构正交，并且可以与单独的链接或任何其他散列映射类型组合。</p><p>对于模型，我们可以再次利用上一节中的递归模型架构。显然，和以前一样，索引的大小和性能之间存在权衡，这受到模型和数据集的影响。</p><p>请注意，如何处理插入、查找和冲突取决于哈希映射架构。因此，学习的哈希函数相对于传统哈希函数（将键映射到均匀分布的空间）的优势取决于两个关键因素：（1）模型表示观察到的 CDF 的准确程度。例如，如果数据是由均匀分布生成的，简单的线性模型将能够学习一般的数据分布，但所得的哈希函数不会比任何充分随机的哈希函数更好。 (2) 哈希映射架构：取决于架构、实现细节、有效负载（即值）、冲突解决策略以及将或可以分配多少内存（即槽），都会显着影响性能。例如，对于小键和小值或没有值，采用 Cuckoo 散列的传统散列函数可能会很好地工作，而较大的有效负载或分布式散列映射可能会从避免冲突中获益更多，从而从学习的散列函数中获益更多。</p><hr><h3 id="5-布隆过滤器—存在性查询"><a href="#5-布隆过滤器—存在性查询" class="headerlink" title="5. 布隆过滤器—存在性查询"></a>5. 布隆过滤器—存在性查询</h3><p>DBMS 最后一种常见的索引类型是存在索引，最重要的是布隆过滤器，这是一种节省空间的概率数据结构，用于测试元素是否是集合的成员。</p><p>它们通常用于确定冷存储中是否存在密钥。例如，Bigtable 使用它们来确定 SSTable 中是否包含某个键 [23]。</p><p>在内部，布隆过滤器使用大小为 m 的位数组和 k 个哈希函数，每个函数将一个键映射到 m 个数组位置之一（参见图 9(a)）。为了将一个元素添加到集合中，将一个键输入到 k 个哈希函数中，并将返回位置的位设置为 1。为了测试某个键是否是该集合的成员，将该键再次输入到 k 个哈希函数中。用于接收 k 个数组位置的哈希函数。如果这 k 个位置上的任何一位为 0，则该密钥不是集合的成员。</p><p>换句话说，布隆过滤器确实保证不存在误报，但存在潜在的误报。 </p><p>In other words, a Bloom filter does guarantee that there exists no false negatives, but has potential<br>false positives.</p><p>虽然布隆过滤器具有很高的空间效率，但它们仍然会占用大量内存。例如，对于 10 亿条记录，大约需要 ≈ 1.76 GB。对于 0.01% 的 FPR，我们需要 ≈ 2.23 GB。人们曾多次尝试提高布隆过滤器的效率[52]，但一般观察结果仍然存在。</p><p><img src="/posts/image-20240412215554614.png" alt="img"></p><p>然而，如果存在某种可以学习的结构来确定集合内部和外部的内容，那么就有可能构建更有效的表示。有趣的是，对于数据库系统的存在索引，延迟和空间要求通常与我们之前看到的有很大不同。考虑到访问冷存储（例如磁盘甚至带）的高延迟，我们可以负担更复杂的模型，而主要目标是最小化索引空间和误报数量。我们概述了使用学习模型构建存在索引的两种潜在方法。</p>]]></content>
      
      
      <categories>
          
          <category> 学习型索引 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习型索引 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Alex</title>
      <link href="posts/53ff.html"/>
      <url>posts/53ff.html</url>
      
        <content type="html"><![CDATA[<h1 id="ALEX-An-Updatable-Adaptive-Learned-Index"><a href="#ALEX-An-Updatable-Adaptive-Learned-Index" class="headerlink" title="ALEX: An Updatable Adaptive Learned Index"></a>ALEX: An Updatable Adaptive Learned Index</h1><hr><h2 id="整体数据结构设计"><a href="#整体数据结构设计" class="headerlink" title="整体数据结构设计"></a>整体数据结构设计</h2><p>ALEX旨在实现以下目标：</p><ol><li>插入时间应与 B+Tree 相当;</li><li>查找时间应比 B+Tree 和 Learned Index 快;</li><li>索引存储空间应小于 B+Tree 和 Learned Index;</li><li>数据存储空间(叶级)应与 动态B+Tree 相当。</li></ol><p><img src="/posts/image-20240412211819896.png" alt="img"></p><hr><h3 id="设计概览"><a href="#设计概览" class="headerlink" title="设计概览"></a>设计概览</h3><p>ALEX：内存中的可更新的学习型索引。但与学习索引亦有许多不同之处：</p><p>区别一：叶节点的数据结构。像B+Tree一样，ALEX在每个叶子上使用一个节点。这允许更灵活地扩展和分割单个节点，并且还限制了插入期间所需的移位次数。在典型的B+树中，每个叶节点存储一个键和有效负载数组，并且在数组末尾有“空闲空间”来吸收插入。ALEX使用了类似的设计，但更仔细地选择了如何使用空闲空间。通过在数组元素之间策略性地引入间隙，可以实现更快的插入和查找时间。如图2所示，ALEX对每个数据节点使用间隙数组(GA)布局。</p><p>区别二：ALEX使用指数搜索来搜索叶级键，以纠正RMI的错误预测，如图2所示。我们通过实验验证了无边界的指数搜索比有边界的二叉搜索更快。这是因为如果模型是好的，它们的预测就足够接近正确的位置。指数搜索还消除了在RMI模型中存储错误边界的需要。</p><p>区别三：ALEX在模型预测键应该在的位置将键插入数据节点。我们称之为基于模型的插入。相反，学习索引在不改变数组中记录位置的情况下对记录数组生成RMI。基于模型的插入具有更好的搜索性能，因为它减少了模型误预测误差。</p><p>区别四：ALEX根据工作量动态调整RMI的形状和高度。</p><p>区别五：ALEX没有需要为每个数据集或工作负载重新调整的参数，不像学习索引，其中必须调整模型的数量。ALEX通过使用 cost模型 自动批量加载和调整 RMI 结构以实现高性能。</p><hr><h3 id="节点布局"><a href="#节点布局" class="headerlink" title="节点布局"></a>节点布局</h3><h4 id="数据节点"><a href="#数据节点" class="headerlink" title="数据节点"></a>数据节点</h4><p>与B+树一样，ALEX的叶节点存储数据记录，因此被称为数据节点，如图2中的圆圈所示。</p><p>数据节点<strong>存储一个线性回归模型</strong>(斜率和截距的两个双值)，它将一个键映射到一个位置，<strong>以及两个gap数组</strong>，<strong>一个用于键，一个用于有效载荷</strong>。我们在图2中只显示了keys数组。默认情况下，键和有效负载都是固定大小的。(注意，有效负载可以是记录或指向可变大小记录的指针，存储在内存中单独分配的空间中)。出于实际原因，我们还强制设置了最大节点大小。</p><p>ALEX使用间隙数组布局，它使用基于模型的插入来分配数组元素之间的额外空间，从而实现更快的插入和查找。相反，B+Tree将所有的空格放在数组的末尾。</p><p><strong>间隙数组用间隙右边最近的键来填充间隙，这有助于保持指数搜索性能</strong>。</p><p><strong>为了在扫描时有效地跳过间隙，每个数据节点维护一个位图，该位图跟踪节点中的每个位置是否被键占用或是否为间隙。与间隙数组相比，位图查询速度快，空间开销低</strong>。</p><hr><h4 id="内部节点"><a href="#内部节点" class="headerlink" title="内部节点"></a>内部节点</h4><p>我们将作为RMI结构部分的所有节点称为内部节点，如图2中矩形所示。内部节点存储一个线性回归模型和一个包含指向子节点指针的数组。像B+树一样，内部节点直接遍历树，<strong>但与B+树不同的是，ALEX中的内部节点使用模型来“计算”指针数组中下一个子指针的位置</strong>。与数据节点类似，我们施加了一个最大节点大小。</p><p>ALEX 的内部节点在概念上与学习索引的目的不同。 LearnedIndex的内部节点有适合数据的模型；<strong>具有完美模型的内部节点将键平均分配给其子节点，并且具有完美内部节点的 RMI 会导致每个数据节点中的键数量相同</strong>。然而，<strong>RMI结构的目标不是产生相同大小的数据节点，而是产生键分布大致线性的数据节点，以便线性模型可以精确地拟合其键</strong>。</p><p>因此，ALEX中内部节点的作用就是提供一种灵活的方式来划分键空间。假设图3中的内部节点A覆盖了键空间[0,1)并且有四个子指针。 LearnedIndex 将为每个指针分配一个节点，可以是所有内部节点，也可以是所有数据节点。然而，ALEX 更灵活地划分空间。内部节点A将键空间[0,1/4)和[1/2,1)分配给数据节点（因为这些空间中的CDF是线性的），并将[1/4,1/2)分配给另一个内部节点（因为 CDF 是非线性的，并且 RMI 需要对该key空间进行更精细的划分）。<strong>如图所示，多个指针可以指向同一个子节点</strong>。</p><p>将每个内部节点中的指针数量始终限制为 2 的幂。这允许节点在不重新训练其子树的情况下进行分裂。</p><p><img src="/posts/image-20240412211959414.png" alt="img"></p><hr><h2 id="算法设计"><a href="#算法设计" class="headerlink" title="算法设计"></a>算法设计</h2><h3 id="查找和范围查找"><a href="#查找和范围查找" class="headerlink" title="查找和范围查找"></a>查找和范围查找</h3><p>为了查找键，从 RMI 的根节点开始，迭代地使用模型来“计算”指针数组中的位置，然后沿着指针指向下一级的子节点，直到到达数据节点。通过构建，内部节点模型具有完美的精度，因此内部节点不涉及搜索。我们使用数据节点中的模型来预测键中搜索键的位置阵列，如果需要，进行指数搜索找到 key 的实际位置。如果找到键，我们从有效负载数组中读取同一位置的相应值并返回记录。否则，返回一个空记录。图2中直观地显示了（使用红色箭头）查找。范围查询首先执行查找以查找值不小于范围起始值的第一个键的位置和数据节点，然后向后扫描直到到达范围的结束值，<strong>使用节点的位图跳过间隙</strong>，并在必要时使用存储在节点中的指针跳转到下一个数据节点。【基于模型的查找O(1)+指数搜索】</p><p><img src="/posts/image-20240412212047079.png" alt="img"></p><hr><h3 id="插入未满的数据节点"><a href="#插入未满的数据节点" class="headerlink" title="插入未满的数据节点"></a>插入未满的数据节点</h3><p>对于插入算法，到达正确数据节点（即TraverseToLeaf）的逻辑与上述查找算法相同。在未满数据节点中，为了找到新元素的插入位置，可使用数据节点中的模型来预测插入位置。如果预测位置不正确（插入到那里将无法保持排序顺序），则会进行指数搜索来找到正确的插入位置。</p><p>如果插入位置是一个间隙，则直接将元素插入到间隙中。<strong>否则，便通过将元素在最近间隙的方向上移动一个位置以在插入位置处形成一个间隙</strong>。然后将元素插入到新创建的间隙中。间隙数组以高概率实现 O(logn) 插入时间。</p><hr><h3 id="插入已满的数据节点"><a href="#插入已满的数据节点" class="headerlink" title="插入已满的数据节点"></a>插入已满的数据节点</h3><p>当数据节点已满时，ALEX 使用两种机制来创建更多空间：扩展和拆分。 ALEX 依靠简单的 cost 模型在不同的机制之间进行选择。</p><h4 id="节点饱满度的标准"><a href="#节点饱满度的标准" class="headerlink" title="节点饱满度的标准"></a>节点饱满度的标准</h4><p>ALEX 不会等待数据节点达到 100% 满，因为间隙数组上的插入性能会随着间隙数量的减少而恶化。我们引入间隙数组的密度下限和上限：d<del>l</del>,d<del>u</del> ∈ (0,1]，约束条件是 d<del>l</del>&lt;d<del>u</del>。</p><p>密度定义为被元素填充的位置的分数。如果下一次插入导致超过 d<del>u</del> ，则节点已满。默认情况下，我们设置 d<del>l</del>=0.6 和 d<del>u</del>=0.8，以实现平均数据存储利用率 0.7，类似于 B+Tree [14]，根据我们的实验，这总是会产生良好的结果，并且不需要调整。相反，B+Tree 节点通常具有 d<del>l</del>=0.5 和 d<del>u</del>=1。</p><h4 id="节点扩展机制"><a href="#节点扩展机制" class="headerlink" title="节点扩展机制"></a>节点扩展机制</h4><p>为了扩展包含 n 个键的数据节点，我们分配一个具有 n/d<del>l</del> 个槽的新的更大的间隙数组。然后，我们缩放或重新训练线性回归模型，然后使用缩放或重新训练的模型对这个新的较大节点中的所有元素进行基于模型的插入。创建后，新数据节点处于密度下限 d<del>l</del> 。</p><h4 id="节点分裂机制"><a href="#节点分裂机制" class="headerlink" title="节点分裂机制"></a>节点分裂机制</h4><p>为了将数据节点一分为二，即将 key 分配给两个新的数据节点，使得每个新节点负责原始节点的 key 空间的一半。 ALEX支持两种分割节点的方式：</p><ol><li><p><strong>横向拆分</strong>在概念上类似于 B+Tree 使用拆分的方式。有两种情况：</p><p>a）如果分裂数据节点的父内部节点尚未达到最大节点大小，则将父节点指向分裂数据节点的指针替换为指向两个新数据节点的指针。父内部节点的指针数组可能有指向拆分数据节点的冗余指针。如果是这样，我们将一半的冗余指针分配给两个新节点中的每一个。否则，我们通过将父节点指针数组的大小加倍并为每个指针制作冗余副本来创建指向拆分数据节点的第二个指针，然后将冗余指针之一赋予两个新节点中的每一个。图 5a 显示了不需要扩展父内部节点的横向拆分的示例。 </p><p>b) 如果父内部节点已达到最大节点大小，则可以选择拆分父内部节点，如图 5b 所示。请注意，通过将所有内部节点大小限制为 2 的幂，便始终以“边界保留”方式分割节点，因此不需要重新训练分割内部节点下方的任何模型。请注意，分裂可以一直传播到根节点，就像在 B+ 树中一样。</p></li></ol><p><img src="/posts/image-20240412212101573.png" alt="img"></p><ol start="2"><li>纵向拆分将数据节点转换为具有两个子数据节点的内部节点，如图 5c 所示。<strong>两个子数据节点中的模型根据各自的键进行训练</strong>。 B+Tree 没有类似的分裂机制。</li></ol><p><img src="/posts/image-20240412212111535.png" alt="image-20240412212111535"></p><hr><h4 id="cost-模型"><a href="#cost-模型" class="headerlink" title="cost 模型"></a>cost 模型</h4><p>为了决定应用哪种机制，ALEX 依赖于简单的线性 cost 模型，该模型根据在每个数据节点跟踪出来的两个简单统计数据 来预测平均查找时间和插入时间：（a）指数搜索迭代的平均数; （b) 插入的平均移位次数。查找性能与（a）直接相关，而插入性能与（a）和（b）直接相关（因为插入首先需要进行查找以找到正确的插入位置）。</p><p><strong>这两个统计数据在创建数据节点时是未知的。为了找到新数据节点的预期 cost ，我们在假设对现有键统一进行查找并根据现有键分布进行插入的情况下计算这些统计数据的预期值</strong>。</p><p>具体来说：</p><p>(a) 被计算为所有键的模型预测误差的平均以 2 为底的对数； </p><p>(b) 计算所有现有键到间隙数组中最近间隙的平均距离。</p><p>这些期望值可以在不创建数据节点的情况下计算。如果数据节点是使用现有数据节点的键子集创建的，我们可以使用查找与插入的经验比率来衡量两个统计数据的相对重要性以进行计算预期 cost 。</p><p>除了节点内 cost 模型之外，ALEX 还使用 TraverseToLeaf cost 模型来预测从根节点遍历到数据节点的时间。 TraverseToLeaf cost 模型使用两个统计数据：</p><ol><li>遍历到的数据节点的深度；</li><li>所有内部节点和数据节点元数据（除了key和有效负载之外的所有内容）的总大小（以B为单位）。</li></ol><p>这些统计数据捕获了遍历的 cost ：较深的数据节点需要更多的指针追踪来查找，较大的大小会降低 CPU 缓存局部性，从而减慢对数据节点的遍历。我们提供有关 cost 模型的更多详细信息并在 [10] 的附录 D 中显示了它们的低使用开销。【<strong>计算叶节点深度，以及遍历到叶节点所需公共元数据大小</strong>】</p><hr><h4 id="插入算法"><a href="#插入算法" class="headerlink" title="插入算法"></a>插入算法</h4><p>当查找和插入在数据节点上完成时，就计算指数搜索迭代和每次插入的移位 次数。<strong>根据这些统计数据，我们使用节点内 cost 模型计算数据节点的经验 cost **。</strong>一旦数据节点已满，就将预期 cost （在节点创建时计算）与经验 cost 进行比较。如果它们没有显著偏离，那么就断定模型仍然准确，就执行节点扩展（如果扩展后的大小小于最大节点大小），缩放模型而不是重新训练**。 RMI 内部节点中的模型不会重新训练或重新缩放。</p><p><strong>显著 cost 偏差定义为：经验 cost 比预期 cost 高出 50% 以上</strong>。根据我们的实验，50% 的 cost 偏差阈值始终会产生良好的结果，无需进行调整。</p><p>否则，<strong>如果经验 cost 偏离了预期 cost ，就必须（i）扩展数据节点并重新训练模型，（ii）横向分割数据节点，或（iii）纵向分割数据节点。根据节点内 cost 模型，选择导致最低预期 cost 的操作</strong>。</p><p><strong>为简单起见，ALEX 始终将数据节点一分为二。数据节点在概念上可以拆分为 2 的任意幂，但确定最佳扇出可能非常耗时，我们通过实验验证，在大多数情况下，根据 cost 模型，2 的扇出是最佳的</strong>。</p><hr><h4 id="为什么经验-cost-会偏离预期-cost-？"><a href="#为什么经验-cost-会偏离预期-cost-？" class="headerlink" title="为什么经验 cost 会偏离预期 cost ？"></a>为什么经验 cost 会偏离预期 cost ？</h4><p>当 插入键 的分布不遵循现有键的分布时，通常会发生这种情况，从而导致模型变得不准确。不准确的模型可能会导致没有任何间隙的长连续区域。插入这些完全填充的区域需要移动其中最多一半的元素以创建间隙，在最坏的情况下这需要 O(n) 时间。随着节点变大，或由于查找的访问模式发生变化，性能也可能仅仅由于随机噪声而降低。</p><hr><h3 id="删除、更新"><a href="#删除、更新" class="headerlink" title="删除、更新"></a>删除、更新</h3><p>要删除key，则进行查找以找到key的位置，然后删除它及其有效负载。删除不会移动任何现有键，因此删除是比插入更简单的操作，并且不会导致模型准确性降低。如果数据节点由于删除而达到密度下限 d<del>l</del>，则 <strong>收缩</strong> 该数据节点以避免空间利用率低。此外，使用节点内 cost 模型来确定两个数据节点应该合并在一起并可能向上增长，从而局部将 RMI 深度减少 1。但是，为了简单起见，我们不实现这些合并操作。</p><p>【<strong>如何收缩？简单起见，不合并，要合并怎么办？效果会好吗</strong>？】</p><p>更新：修改键是通过组合插入和删除来实现的。仅修改有效负载的更新将查找 key 并将新值写入有效负载。</p><hr><h3 id="处理越界插入"><a href="#处理越界插入" class="headerlink" title="处理越界插入"></a>处理越界插入</h3><p>低于或高于现有键空间的键将分别插入到最左边或最右边的数据节点中。一系列越界插入（如仅追加插入工作负载）将导致性能不佳，因为该数据节点没有分裂越界键空间的机制。因此，ALEX有两种方式可以顺利处理越界插入。</p><p>首先，当检测到现有键空间之外的插入时，ALEX 将扩展根节点，从而扩展键空间，如图 6 所示。可将子指针数组的大小向右扩展。指向现有子项的现有指针不会被修改。为扩展的指针数组中的每个新槽创建一个新的数据节点。若此扩展导致根节点超过最大节点大小，ALEX 将创建一个新的根节点。新根节点的第一个子指针将指向旧根节点，并为新根节点的其他每个指针槽创建一个新数据节点。最后，越界键将落入新创建的数据节点之一。</p><p><img src="/posts/image-20240412212122272.png" alt="img"></p><p>其次，ALEX 最右边的数据节点通过 维护节点中最大键的值 并 保留插入超过最大值的计数器 来检测追加插入行为。如果大多数【<strong>是根据计数器大小判断</strong>？】插入超过最大值，则意味着仅追加行为，因此数据节点向右扩展，而不进行基于模型的重新插入；扩展的空间最初保持为空，以期待更多类似附加的插入。</p><hr><h3 id="批量加载"><a href="#批量加载" class="headerlink" title="批量加载"></a>批量加载</h3><p>ALEX 支持批量加载操作，该操作在实践中用于在初始化时索引大量数据或重建索引。我们的目标是找到一个 cost 最小的 RMI 结构，cost 定义为在此 RMI 上执行操作（即查找或插入）的预期平均时间。任何 ALEX 操作都是由到数据节点的 TraverseToLeaf 和节点内操作组成，因此 RMI cost 是通过结合 TraverseToLeaf 和节点内 cost 模型来建模的。</p><h4 id="批量加载算法"><a href="#批量加载算法" class="headerlink" title="批量加载算法"></a>批量加载算法</h4><p>使用 cost 模型，从根节点开始贪婪地向下增长 RMI。在每个节点，独立地决定该节点应该是数据节点还是内部节点，以及在后一种情况下，扇出应该是什么。<strong>扇出必须是2的幂，子节点将平分当前节点的key空间</strong>。</p><p>在每个节点本地就可做出此决策，因为使用线性 cost 模型，决策将对 RMI 的总体 cost 产生纯粹的累加效应。如果该节点是内部节点，则将在其每个子节点上递归。这将持续下去，直到所有数据都加载到 ALEX 中。</p><p><img src="/posts/image-20240412212129080.png" alt="img"></p><h4 id="扇出树"><a href="#扇出树" class="headerlink" title="扇出树"></a>扇出树</h4><p>随着 RMI 的发展，主要挑战是确定每个节点的最佳扇出。</p><p>引入扇出树（FT）概念，它是一棵完全二叉树。 FT 将帮助决定单个 RMI 节点的扇出；在批量加载算法中，每次决定 RMI 节点的最佳扇出时，都构造一个 FT 树。<strong>扇出为 1 意味着 RMI 节点应该是数据节点</strong>。</p><p>图 7 显示了 FT 示例。每个 FT 节点代表 RMI 节点的一个可能的子节点。如果 RMI 节点的 key 空间为[0,1)，则在某个具有 n 个节点的层级上，第 i 个 FT 节点表示 key 空间为 [i/n,(i+1)/n) 的 子RMI节点。每个 FT 节点都与在其 key 空间上构建数据节点的预期 cost 相关联，如节点内 cost 模型所预测的那样。目标是以最小的总体 cost 找到一组覆盖 RMI 节点整个 key 空间的 FT 节点。</p><p>覆盖集的总体 cost 是其 FT 节点 cost 以及由于模型大小而导致的 TraverseToLeaf cost 之和（如在 FT 中深入一层意味着 RMI 节点必须具有两倍的指针）。该覆盖集决定了 RMI 节点的最佳扇出（即子指针的数量）以及分配子指针的最佳方式。</p><p>使用以下方法来找到低 cost 的覆盖集：</p><ol><li>从FT根开始，一次增长FT的整个级别，并计算使用每个级别作为覆盖集的 cost 。继续这样做，直到每个连续级别的 cost 开始增加。在图 7 中，级别 2 的综合 cost 最低，则不再继续增长级别 3 之后的。从概念上讲，更深的级别可能具有较低的 cost ，但计算每个 FT 节点的 cost 是昂贵的。 </li></ol><ol start="2"><li>从组合 cost 最低的 FT 级别开始，开始本地合并或分裂 FT 节点。如果两个相邻 FT 节点的 cost 高于其父节点的 cost ，则合并（例如， cost 为 20 和 25 的节点合并为 cost 为 40 的节点）；当两个节点的键很少或它们的分布相似时，可能会发生这种情况。另一方面，如果 FT 节点的 cost 高于其两个子节点的 cost ，则分割 FT 节点（例如， cost 为 10 的节点被分割为两个 cost 为 1 的节点）；当 key 空间的两半具有不同的分布时，可能会发生这种情况。继续在本地合并和分裂相邻节点的过程，直到不再可能为止。</li></ol><p>最后，返回生成的 FT 节点覆盖集。</p><p>【<strong>大大的问题：经验 cost 和预期 cost 是怎么计算的，FT树的每个层级中的 cost 是怎么计算的？怎么和RMI对应的？该问题中，层级2 cost 最小，完事怎么又合并分裂了</strong>？】</p><hr><h2 id="ALEX分析"><a href="#ALEX分析" class="headerlink" title="ALEX分析"></a>ALEX分析</h2><h3 id="RMI-深度的限制"><a href="#RMI-深度的限制" class="headerlink" title="RMI 深度的限制"></a>RMI 深度的限制</h3><p>让 m 为最大节点大小，以槽数定义（在内部节点的指针数组中，在数据节点的间隙数组中）。将节点大小限制为 2 的幂： m = 2^k^ 。内部节点最多可以有 m 多个子指针，数据节点包含的键不得超过 md<del>u</del> 个。让所有要索引的键都落在键空间 s 内。令 p 为分区的最小数量，使得当 key 空间 s 被划分为 p 个宽度相等的分区时，每个分区包含的 key 不超过 md<del>u</del> 个。定义根节点深度为0。</p><p>定理5.1：可以构造一个满足最大节点大小和密度上限约束的 RMI，其深度不大于 ⌈log<del>m</del>p⌉——称之为最大深度。此外，可以保持插入下的最大深度。（请注意，p 在插入下可能会发生变化）</p><p>换句话说，RMI 的深度受到 <strong>s 最密集子区域的密度的限制</strong>。相比之下，B+树将深度限制为键数量的函数。定理 5.1 也可以应用于 s 内的子空间，它对应于 RMI 内的某个子树。</p><p>证明。构建具有最大深度的 RMI 非常简单。跨越大小为 |s|/p 的键空间的<strong>最密集子区域</strong>【<strong>如何甄别</strong>？】被分配给数据节点。从根到这个最密集区域的遍历路径由内部节点组成，每个内部节点有 m 个子指针。它需要 ⌈log<del>m</del>p⌉ 内部节点来缩小 |s| 的 key 空间大小至 |s|/p。为了最小化 RMI 的其他子树的深度，我们将这种构造机制递归地应用于空间 s 的其余部分。</p><p>从满足最大深度的 RMI 开始，我们使用 4.3 节中的机制根据以下策略维持最大深度： (1) 数据节点扩展直到达到最大节点大小。 (2) 当数据节点由于最大节点大小而必须分裂时，它会横向分裂以维持当前深度（可能将分裂传播到某个祖先内部节点）。(3) 当不再可能横向分裂时（所有祖先节点都处于最大节点大小），向下分裂。</p><p><strong>通过遵循此策略，RMI 仅当 p 增长 m 倍时才向下分裂，从而保持最大深度</strong>。</p><h3 id="复杂性分析"><a href="#复杂性分析" class="headerlink" title="复杂性分析"></a>复杂性分析</h3><p>查找和插入都在 ⌈log<del>m</del> p⌉ 时间内完成 TraverseToLeaf。在数据节点内，查找的指数搜索在最坏情况下受 O(logm) 限制。在最好的情况下，数据节点模型完美地预测了 key 的位置，并且查找需要 O(1) 时间。</p><p>未满节点的插入由查找组成，后面可能会进行移位以引入新键的间隙。在最坏的情况下，这受到 O(m) 的限制，但由于间隙数组以高概率实现每次插入 O(logm) 次移位，因此预计大多数情况下的复杂性为 O(logm)。在最好的情况下，预测的插入位置是正确的并且是一个间隙，则将 key 准确地放置在模型预测的位置，插入复杂度为 O(1) ；此外，稍后基于模型的查找将导致直接命中 O(1)。</p><p>cost 由必须复制的元素数量来定义： (1)数据节点的扩展，其 cost 以O(m)为界。 (2)向下分裂成两个节点，其 cost 为O(m)。 (3) 横向分裂成两个节点，并在路径中向上传播到某个祖先节点，其 cost 受 O(m⌈log<del>m</del>p⌉) 限制，因为该路径上的每个内部节点也必须分裂。因此，插入全节点的最坏情况性能为 O(m⌈log<del>m</del>p⌉)。</p><hr><h2 id="实验评估"><a href="#实验评估" class="headerlink" title="实验评估"></a>实验评估</h2><p>使用各种数据集和工作负载将 ALEX 与学习索引、B+树、模型增强的 B+树 和 自适应基数树 (ART) 进行比较。此次评估表明：</p><ul><li><p>在只读工作负载上，ALEX 的吞吐量比 B+Tree、学习索引、模型B+树和ART 高出 4.1 倍、2.2 倍、2.9 倍、3.0 倍；索引大小小 800 倍、15 倍、160 倍、8000 倍。</p></li><li><p>在读写工作负载上，ALEX 的吞吐量分别比 B+Tree、模型 B+Tree 和 ART 高 4.0×、2.7×、2.7×，索引大小分别小 2000×、475×、36000×。</p></li><li><p>ALEX 具有竞争力的批量加载时间，并且在扩展到更大的数据集以及由于数据倾斜而导致分布变化时，保持优于其他索引的优势。</p></li><li><p>Gapped Array 和自适应 RMI 结构使 ALEX 能够适应不同的数据集和工作负载。</p></li></ul><hr><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><p>使用来自某些数据集的 8 字节密钥和随机生成的固定大小的有效负载来运行所有实验。我们在 4 个数据集上评估 ALEX，其特征和 CDF 如表 1 和图 8 所示。经度数据集由 Open Street Maps [2] 中世界各地位置的经度组成。 longlat 数据集由复合键组成，这些复合键通过对每对经度和纬度应用变换 k =180·floor(经度)+纬度来组合来自开放街道地图的经度和纬度。由此产生的 keys k 分布是高度非线性的。对数正态数据集的值根据对数正态分布生成，其中 µ =0 且 σ =2，乘以 10^9^ 并向下舍入到最接近的整数。 YCSB 数据集具有表示根据 YCSB 基准 [8] 生成的用户 ID 的值，这些值均匀分布在整个 64 位域中，并使用 80 字节的有效负载。这些数据集不包含重复值。除非另有说明，否则这些数据集会随机打乱以模拟随时间变化的均匀数据集分布.</p><h4 id="工作负载"><a href="#工作负载" class="headerlink" title="工作负载"></a>工作负载</h4><p>我们评估 ALEX 的主要指标是平均吞吐量。我们评估五种工作负载的吞吐量：(1) 只读工作负载，(2) 包含 95% 读取和 5% 插入的读密集型工作负载，(3) 包含 50% 读取和 50% 插入的写密集型工作负载， (4) 具有 95% 读取和 5% 插入的短程查询工作负载，以及 (5) 只写工作负载，以完成读写范围。对于前三个工作负载，读取由单个键的查找组成。对于短程工作负载，读取由键查找和后续键扫描组成。要扫描的键的数量是从最大扫描长度为 100 的均匀分布中随机选择的。对于所有工作负载，要查找的键是根据 Zipfian 分布从索引中现有键的集合中随机选择的。前四个工作负载大致分别对应于 YCSB 基准 [8] 中的工作负载 C、B、A 和 E。对于给定的数据集，我们用 1 亿个键初始化一个索引。然后，我们运行工作负载 60 秒，插入剩余的密钥。我们报告当时完成的操作的吞吐量，其中操作要么是插入，要么是读取。对于读写工作负载，我们交错操作：对于读密集型工作负载和短范围工作负载，我们执行 19 次读取/扫描，然后执行 1 次插入，然后重复该循环；<strong>对于写入密集型工作负载，我们执行 1 次读取，然后执行 1 次插入，然后重复该循环</strong>。</p><hr><h3 id="Drilldown-into-ALEX-Design-Trade-offs"><a href="#Drilldown-into-ALEX-Design-Trade-offs" class="headerlink" title="Drilldown into ALEX Design Trade-offs"></a>Drilldown into ALEX Design Trade-offs</h3><p>在本节中，我们将深入探讨节点布局和自适应 RMI 如何帮助 ALEX 实现其设计目标。</p><p>ALEX 相对于学习索引的部分优势来自于在数据节点中使用基于模型的插入和间隙数组，但 ALEX 对于动态工作负载的大部分优势来自于<strong>自适应 RMI</strong>。</p><p>为了演示每个贡献的效果，图 13 显示，采用 2 层学习索引并用每个叶的间隙数组（LI w/Gapped Array）替换单个密集值数组，对于只读工作负载，已经比学习索引实现了显着的加速。然而，带有间隙数组的学习索引在读写工作负载上的性能较差，因为存在完全填充的区域，需要为每次插入移动许多键。 ALEX 使 RMI 结构适应数据的能力对于良好的插入性能是必要的。</p><p>在查找过程中，大部分时间都花在围绕预测位置进行本地搜索。较小的预测误差直接有助于减少查找时间。为了分析 Learned Index 和 ALEX 的预测误差，我们用经度数据集中的 1 亿个键初始化一个索引，使用该索引来预测这 1 亿个键中每个键的位置，并跟踪预测位置与实际位置之间的距离。图 14a 显示学习索引在模式大约 8-32 个位置处存在预测误差，并且右侧有长尾。另一方面，ALEX 通过使用基于模型的插入实现了低得多的预测误差。</p><p>图14b显示，初始化后，ALEX通常没有预测误差，发生的误差通常很小，并且误差的长尾已经消失。图 14c 显示，即使在 2000 万次插入之后，ALEX 仍保持较低的预测误差。</p><p>一旦数据节点变满，就会发生以下四种操作之一：如果不存在 cost 偏差，则 (1) 扩展节点并缩放模型。否则，节点要么 (2) 扩展并重新训练其模型，(3) 横向分割，或 (4) 向下分割。表 3 显示，在绝大多数情况下，数据节点只是扩展，模型也进行了缩放，这意味着即使在插入之后模型通常仍然保持准确（假设没有发生根本的分布变化）。数据节点变满的次数与数据节点的数量相关（表2）。在YCSB上，通过模型再训练进行扩展更为常见，因为数据节点很大，因此 cost 偏差通常只是由随机性引起的。</p><p>如果需要，用户可以调整最大节点大小以实现目标尾部延迟。在图 15 中，我们在经度数据集上运行写入密集型工作负载，测量每个操作的延迟。随着我们增加最大节点大小，ALEX 的中值甚至 p99 延迟都会减少，因为 ALEX 具有更大的灵活性来构建性能更好的 RMI（例如，具有更高的内部节点扇出的能力）。然而，最大延迟会增加，因为触发大节点扩展或分裂的插入速度很慢。如果用户对延迟有严格要求，则可以相应减小最大节点大小。将最大节点大小增加到超过 64MB 后，延迟不会改变，因为 ALEX 从未决定使用大于 64MB 的节点。</p><hr><h4 id="搜索方法比较"><a href="#搜索方法比较" class="headerlink" title="搜索方法比较"></a>搜索方法比较</h4><p>为了展示指数搜索和其他搜索方法之间的权衡，我们对合成数据进行了微基准测试。我们创建一个包含 1 亿个完全均匀分布的双精度数的数据集。然后，我们从该数据集中搜索 1000 万个随机选择的值。我们使用三种搜索方法：二分搜索和有偏四元搜索（在[20]中提出，以利用准确的预测），每种方法都使用两种不同的误差范围大小进行评估，以及指数搜索。对于每次查找，搜索方法都会得到一个预测位置，该位置与实际位置值的距离存在一定的综合误差量。图 16 显示，指数搜索的搜索时间与错误大小的对数成比例增加，而二分搜索方法花费恒定的时间，而与错误大小无关。这是因为二分查找必须始终在其误差范围内开始搜索，并且无法利用误差较小的情况。因此，如果 ALEX 中 RMI 模型的预测误差较小，指数搜索应该优于二分搜索。正如我们在第 6.3 节中所示，ALEX 通过基于模型的插入保持较低的预测误差。</p><p>因此，ALEX 非常适合利用指数搜索。当误差低于 σ 时（我们在本实验中设置 σ = 8；详细信息请参阅[20]），有向四元搜索与指数搜索具有竞争力，因为搜索可以限制在较小的范围内，但当误差超过 σ 时，其性能与二分搜索类似因为必须搜索完整的错误界限。与有偏差的四元搜索相比，我们更喜欢指数搜索，因为它的性能下降更平滑且实现简单（例如，无需调整 σ）。</p><p>参考博文：<a href="https://zhuanlan.zhihu.com/p/435878936">指数搜索 - 知乎 (zhihu.com)</a> </p><hr><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>我们在学习索引令人兴奋的基础上提出了 ALEX，这是一种新的可更新学习索引，它有效地将学习索引的核心见解与经过验证的存储和索引技术结合起来。具体来说，我们提出了一种间隙数组节点布局，它使用基于模型的插入和指数搜索，结合简单 cost 模型驱动的自适应 RMI 结构，以在动态工作负载上实现高性能和低内存占用。我们深入的实验结果表明，ALEX 不仅在读写工作负载范围内始终优于 B+Tree，甚至在所有数据集上，对于只读工作负载，其性能比现有的学习索引高出 2.2 倍。</p><p>我们相信这篇论文为我们的社区提供了重要的经验教训，并为该领域的未来研究开辟了道路。</p><p>我们打算研究有关 ALEX 性能的开放理论问题，<strong>支持大于内存数据集的二级存储，以及针对 ALEX 设计量身定制的新并发控制技术</strong>。</p><p>【<strong>方向：二级学习型索引，并发控制的学习型索引</strong>】</p><hr><h2 id="代码结构"><a href="#代码结构" class="headerlink" title="代码结构"></a>代码结构</h2><h3 id="类及属性"><a href="#类及属性" class="headerlink" title="类及属性"></a>类及属性</h3><h4 id="alex-base-h"><a href="#alex-base-h" class="headerlink" title="alex_base.h"></a>alex_base.h</h4><h5 id="线性回归模型"><a href="#线性回归模型" class="headerlink" title="线性回归模型"></a>线性回归模型</h5><pre class=" language-c++"><code class="language-c++">template<class T>class LinearModel &#123;public:    double a_ = 0;  // 斜率    double b_ = 0;  // 截距    LinearModel() = default;    LinearModel(double a, double b) : a_(a), b_(b) &#123;&#125;    // 拷贝构造函数    explicit LinearModel(const LinearModel &other) : a_(other.a_), b_(other.b_) &#123;&#125;    // 根据给定的扩展因子（expansion_factor）来扩展线性模型的斜率和截距    void expand(double expansion_factor) &#123;        a_ *= expansion_factor;        b_ *= expansion_factor;    &#125;    // 预测 key 的 position_int    inline int predict(T key) const &#123;        return static_cast<int>(a_ * static_cast<double>(key) + b_);    &#125;    // 预测 key 的 position_double    inline double predict_double(T key) const &#123;        return a_ * static_cast<double>(key) + b_;    &#125;&#125;;</code></pre><h5 id="构造线性回归"><a href="#构造线性回归" class="headerlink" title="构造线性回归"></a>构造线性回归</h5><pre class=" language-c++"><code class="language-c++">template<class T>class LinearModelBuilder &#123;public:    // 指向LinearModel<T>对象的指针，用于存储构建的线性模型    LinearModel <T> *model_;    explicit LinearModelBuilder<T>(LinearModel <T> *model) : model_(model) &#123;&#125;    // 该函数会更新相关的统计信息，包括计数、x和y的总和、x的平方总和、x和y的乘积总和以及x和y的最小值和最大值    inline void add(T x, int y) &#123;        count_++;        x_sum_ += static_cast<long double>(x);y_sum_ += static_cast<long double>(y);        xx_sum_ += static_cast<long double>(x) * x;xy_sum_ += static_cast<long double>(x) * y;        x_min_ = std::min<T>(x, x_min_);x_max_ = std::max<T>(x, x_max_);        y_min_ = std::min<double>(y, y_min_);y_max_ = std::max<double>(y, y_max_);    &#125;private:    // 整型变量，表示已添加的数据点的数量    int count_ = 0;    // 长双精度浮点型变量，表示所有数据点的x值的总和    long double x_sum_ = 0;    // 长双精度浮点型变量，表示所有数据点的y值的总和    long double y_sum_ = 0;    // 长双精度浮点型变量，表示所有数据点的x值的平方总和    long double xx_sum_ = 0;    // 长双精度浮点型变量，表示所有数据点的x值和y值的乘积总和    long double xy_sum_ = 0;        T x_min_ = std::numeric_limits<T>::max();    T x_max_ = std::numeric_limits<T>::lowest();    double y_min_ = std::numeric_limits<double>::max();    double y_max_ = std::numeric_limits<double>::lowest();&#125;;</code></pre><p><strong>build 方法构建模型</strong>：</p><pre class=" language-c++"><code class="language-c++">void build() &#123;    if (count_ <= 1) &#123;        model_->a_ = 0;        model_->b_ = static_cast<double>(y_sum_);        return;    &#125;    if (static_cast<long double>(count_) * xx_sum_ - x_sum_ * x_sum_ == 0) &#123;        // all values in a bucket have the same key.        model_->a_ = 0;        model_->b_ = static_cast<double>(y_sum_) / count_;        return;    &#125;    auto slope = static_cast<double>(        (static_cast<long double>(count_) * xy_sum_ - x_sum_ * y_sum_) /        (static_cast<long double>(count_) * xx_sum_ - x_sum_ * x_sum_));        auto intercept = static_cast<double>(        (y_sum_ - static_cast<long double>(slope) * x_sum_) / count_);    model_->a_ = slope;    model_->b_ = intercept;    // If floating point precision errors, fit spline    if (model_->a_ <= 0) &#123;        if (x_max_ - x_min_ == 0) &#123;            model_->a_ = 0;            model_->b_ = static_cast<double>(y_sum_) / count_;        &#125; else &#123;            model_->a_ = (y_max_ - y_min_) / (x_max_ - x_min_);            model_->b_ = -static_cast<double>(x_min_) * model_->a_;        &#125;    &#125;&#125;</code></pre><hr><h5 id="工具方法"><a href="#工具方法" class="headerlink" title="工具方法"></a>工具方法</h5><pre class=" language-c++"><code class="language-c++">/*** Helper methods for bitmap ***/// Extract the rightmost 1 in the binary representation.// e.g. extract_rightmost_one(010100100) = 000000100inline uint64_t extract_rightmost_one(uint64_t value) &#123;    return value & -static_cast<int64_t>(value);&#125;// Remove the rightmost 1 in the binary representation.// e.g. remove_rightmost_one(010100100) = 010100000inline uint64_t remove_rightmost_one(uint64_t value) &#123;    return value & (value - 1);&#125;// Count the number of 1s in the binary representation.// e.g. count_ones(010100100) = 3inline int count_ones(uint64_t value) &#123;    return static_cast<int>(_mm_popcnt_u64(value));&#125;// Get the offset of a bit in a bitmap.// word_id is the word id of the bit in a bitmap// bit is the word that contains the bitinline int get_offset(int word_id, uint64_t bit) &#123;    // 返回该位在位图中的偏移量    return (word_id << 6) + count_ones(bit - 1);&#125;// https://stackoverflow.com/questions/364985/algorithm-for-finding-the-smallest-power-of-two-thats-greater-or-equal-to-a-giv// 返回大于或等于x的最小2的幂次方数inline int pow_2_round_up(int x) &#123;    --x;    x |= x >> 1;    x |= x >> 2;    x |= x >> 4;    x |= x >> 8;    x |= x >> 16;    return x + 1;&#125;// https://stackoverflow.com/questions/994593/how-to-do-an-integer-log2-in-c// 返回小于或等于x的最大2的幂次方数的指数inline int log_2_round_down(int x) &#123;    int res = 0;    while (x >>= 1) ++res;    return res;&#125;</code></pre><h5 id="统计方法"><a href="#统计方法" class="headerlink" title="统计方法"></a>统计方法</h5><pre class=" language-c++"><code class="language-c++">/*** Cost model weights ***/// Intra-node cost weightsconstexpr double kExpSearchIterationsWeight = 20;constexpr double kShiftsWeight = 0.5;// TraverseToLeaf cost weightsconstexpr double kNodeLookupsWeight = 20;constexpr double kModelSizeWeight = 5e-7;/*** Stat Accumulators ***/// 统计数据节点的搜索迭代次数和位移数量struct DataNodeStats &#123;    double num_search_iterations = 0;    double num_shifts = 0;&#125;;// Used when stats are computed using a sample// 用于在计算统计数据时使用样本struct SampleDataNodeStats &#123;    // 样本大小的对数    double log2_sample_size = 0;    // 搜索迭代次数    double num_search_iterations = 0;    // 位移数量的对数    double log2_num_shifts = 0;&#125;;// Accumulates stats that are used in the cost model, based on the actual vs// predicted position of a keyclass StatAccumulator &#123;    public:    virtual ~StatAccumulator() = default;    // 用于根据实际位置和预测位置来累积统计信息    virtual void accumulate(int actual_position, int predicted_position) = 0;    // 用于获取当前的统计信息    virtual double get_stat() = 0;    // 重置统计信息    virtual void reset() = 0;&#125;;</code></pre><p><strong>计算预期的指数搜索迭代次数</strong>：</p><pre class=" language-c++"><code class="language-c++">// Mean log error represents the expected number of exponential search iterations when doing a lookupclass ExpectedSearchIterationsAccumulator : public StatAccumulator &#123;    public:    // 计算预期的指数搜索迭代次数    void accumulate(int actual_position, int predicted_position) override &#123;        // 计算实际位置和预测位置之间的差的绝对值加1的对数        cumulative_log_error_ +=            std::log2(std::abs(predicted_position - actual_position) + 1);        count_++;    &#125;    double get_stat() override &#123;        if (count_ == 0) return 0;        // 返回当前的平均对数误差        return cumulative_log_error_ / count_;    &#125;    void reset() override &#123;        cumulative_log_error_ = 0;        count_ = 0;    &#125;    public:    double cumulative_log_error_ = 0;    int count_ = 0;&#125;;</code></pre><p><strong>计算预期的插入操作中的移位次数</strong>：</p><pre class=" language-c++"><code class="language-c++">// Mean shifts represents the expected number of shifts when doing an insertclass ExpectedShiftsAccumulator : public StatAccumulator &#123;public:    explicit ExpectedShiftsAccumulator(int data_capacity)        : data_capacity_(data_capacity) &#123;&#125;    // A dense region of n keys will contribute a total number of expected shifts    // of approximately    // ((n-1)/2)((n-1)/2 + 1) = n^2/4 - 1/4    // This is exact for odd n and off by 0.25 for even n.    // Therefore, we track n^2/4.    void accumulate(int actual_position, int) override &#123;        if (actual_position > last_position_ + 1) &#123;            long long dense_region_length = last_position_ - dense_region_start_idx_ + 1;            num_expected_shifts_ += (dense_region_length * dense_region_length) / 4;            dense_region_start_idx_ = actual_position;        &#125;        last_position_ = actual_position;        count_++;    &#125;    double get_stat() override &#123;        if (count_ == 0) return 0;        // first need to accumulate statistics for current packed region        long long dense_region_length = last_position_ - dense_region_start_idx_ + 1;        long long cur_num_expected_shifts =            num_expected_shifts_ + (dense_region_length * dense_region_length) / 4;        return cur_num_expected_shifts / static_cast<double>(count_);    &#125;    void reset() override &#123;        last_position_ = -1;        dense_region_start_idx_ = 0;        num_expected_shifts_ = 0;        count_ = 0;    &#125;public:    int last_position_ = -1;            // 最后一个位置    int dense_region_start_idx_ = 0;    // 密集区域的起始索引    long long num_expected_shifts_ = 0; // 预期的移位次数    int count_ = 0;    int data_capacity_ = -1;            // 节点的容量&#125;;</code></pre><hr><h4 id="alex-node-h"><a href="#alex-node-h" class="headerlink" title="alex_node.h"></a>alex_node.h</h4><p><strong>宏定义，控制编译条件</strong>：</p><pre class=" language-c++"><code class="language-c++">// Whether we store key and payload arrays separately in data nodes// By default, we store them separately#define ALEX_DATA_NODE_SEP_ARRAYS 1#if ALEX_DATA_NODE_SEP_ARRAYS#define ALEX_DATA_NODE_KEY_AT(i) key_slots_[i]#define ALEX_DATA_NODE_PAYLOAD_AT(i) payload_slots_[i]#else#define ALEX_DATA_NODE_KEY_AT(i) data_slots_[i].first#define ALEX_DATA_NODE_PAYLOAD_AT(i) data_slots_[i].second#endif// Whether we use lzcnt and tzcnt when manipulating a bitmap (e.g., when finding the closest gap).// If your hardware does not support lzcnt/tzcnt (e.g., your Intel CPU is pre-Haswell), set this to 0.#define ALEX_USE_LZCNT 1</code></pre><h5 id="AlexNode"><a href="#AlexNode" class="headerlink" title="AlexNode"></a>AlexNode</h5><pre class=" language-c++"><code class="language-c++">// A parent class for both types of ALEX nodestemplate<class T, class P>class AlexNode &#123;public:    // Whether this node is a leaf (data) node    bool is_leaf_ = false;    // Power of 2 to which the pointer to this node is duplicated in its parent model node    // For example, if duplication_factor_ is 3, then there are 8 redundant    // pointers to this node in its parent    // 指向这个节点的指针在其父模型节点中被复制的次数。    // 例如，如果duplication_factor_为3，那么在其父节点中有8个指向这个节点的冗余指针    uint8_t duplication_factor_ = 0;    // Node's level in the RMI. Root node is level 0    short level_ = 0;    // Both model nodes and data nodes nodes use models    LinearModel<T> model_;    // Could be either the expected or empirical cost, depending on how this field is used    double cost_ = 0.0;    AlexNode() = default;    explicit AlexNode(short level) : level_(level) &#123;&#125;    AlexNode(short level, bool is_leaf) : is_leaf_(is_leaf), level_(level) &#123;&#125;    virtual ~AlexNode() = default;    // The size in bytes of all member variables in this class    virtual long long node_size() const = 0;&#125;;</code></pre><h5 id="AlexModelNode"><a href="#AlexModelNode" class="headerlink" title="AlexModelNode"></a>AlexModelNode</h5><pre class=" language-c++"><code class="language-c++">template<class T, class P, class Alloc = std::allocator<std::pair<T, P>>>class AlexModelNode : public AlexNode<T, P> &#123;public:    // 类型别名: 当前类的类型    typedef AlexModelNode<T, P, Alloc> self_type;    // 定义一个类型别名 alloc_type,它是 Alloc 分配器类针对 self_type 类型的特化版本的类型    typedef typename Alloc::template rebind<self_type>::other alloc_type;    typedef typename Alloc::template rebind<AlexNode<T, P> *>::other pointer_alloc_type;    const Alloc &allocator_;    // Number of logical children. Must be a power of 2    int num_children_ = 0;    // Array of pointers to children    AlexNode<T, P> **children_ = nullptr;    // 指向子节点的指针数组    // 构造函数1：使用默认分配器初始化对象    explicit AlexModelNode(const Alloc &alloc = Alloc())            : AlexNode<T, P>(0, false), allocator_(alloc) &#123;&#125;    // 构造函数2：使用指定的层级和默认分配器初始化对象    explicit AlexModelNode(short level, const Alloc &alloc = Alloc())            : AlexNode<T, P>(level, false), allocator_(alloc) &#123;&#125;    ~AlexModelNode() &#123;        if (children_ == nullptr) &#123;            return;        &#125;        // 调用 pointer_allocator() 函数获取分配器对象,        // 并使用其 deallocate 方法释放 children_ 指向的内存空间. num_children_ 参数指定要释放的多少        pointer_allocator().deallocate(children_, num_children_);    &#125;    AlexModelNode(const self_type &other) : AlexNode<T, P>(other), allocator_(other.allocator_),                                            num_children_(other.num_children_) &#123;        // 使用分配器对象pointer_allocator()分配一块内存空间,大小为other.num_children_个指针的大小,        // 并将返回的指针赋值给children_        children_ = new(pointer_allocator().allocate(other.num_children_))                AlexNode<T, P> *[other.num_children_];        // 将other对象的子节点指针数组复制到当前对象的子节点指针数组中        std::copy(other.children_, other.children_ + other.num_children_, children_);    &#125;    // Given a key, traverses to the child node responsible for that key    // 根据给定的键（key）获取对应的子节点指针    inline AlexNode<T, P> *get_child_node(const T &key) &#123;        int bucketID = this->model_.predict(key);        bucketID = std::min<int>(std::max<int>(bucketID, 0), num_children_ - 1);        return children_[bucketID];    &#125;    pointer_alloc_type pointer_allocator() &#123;        return pointer_alloc_type(allocator_);    &#125;    // 计算节点的大小,包括当前对象的大小以及子节点指针数组的大小    long long node_size() const override &#123;        long long size = sizeof(self_type);        size += num_children_ * sizeof(AlexNode<T, P> *);  // pointers to children        return size;    &#125;&#125;;</code></pre><p><strong>内部节点扩展</strong>：</p><pre class=" language-c++"><code class="language-c++">// Expand by a power of 2 by creating duplicates of all existing child pointers.// Input is the base 2 log of the expansion factor, in order to guarantee expanding by a power of 2.// Returns the expansion factor.// 通过创建现有子指针的副本来将数据结构扩展到2的幂次方大小int expand(int log2_expansion_factor) &#123;    assert(log2_expansion_factor >= 0);    // 实际的扩展因子    int expansion_factor = 1 << log2_expansion_factor;    // 新的子节点数量    int num_new_children = num_children_ * expansion_factor;    auto new_children = new(pointer_allocator().allocate(num_new_children))        AlexNode<T, P> *[num_new_children];    int cur = 0;    // 遍历现有的子节点，并将每个子节点复制到新数组中相应的位置    while (cur < num_children_) &#123;        AlexNode<T, P> *cur_child = children_[cur];        // 复制的数量由当前子节点的duplication_factor_属性决定。        int cur_child_repeats = 1 << cur_child->duplication_factor_;        for (int i = expansion_factor * cur; i < expansion_factor * (cur + cur_child_repeats); i++) &#123;            new_children[i] = cur_child;        &#125;        // 更新当前子节点的duplication_factor_属性，以便记录其被复制的次数。        cur_child->duplication_factor_ += log2_expansion_factor;        cur += cur_child_repeats;    &#125;    pointer_allocator().deallocate(children_, num_children_);    children_ = new_children;    num_children_ = num_new_children;    this->model_.expand(expansion_factor);    return expansion_factor;&#125;</code></pre><h5 id="AlexDataNode"><a href="#AlexDataNode" class="headerlink" title="AlexDataNode"></a>AlexDataNode</h5><p><strong>封装类型别名</strong>：</p><pre class=" language-c++"><code class="language-c++">template<class T, class P, class Compare = AlexCompare, class Alloc = std::allocator<std::pair<T, P>>, bool allow_duplicates = true>class AlexDataNode : public AlexNode<T, P> &#123;    public:        // 类型别名V, 表示键值对的类型        typedef std::pair<T, P> V;        // 类型别名self_type, 表示当前类的类型        typedef AlexDataNode<T, P, Compare, Alloc, allow_duplicates> self_type;        // 类型别名alloc_type, 表示用于分配self_type对象的分配器类型        typedef typename Alloc::template rebind<self_type>::other alloc_type;        // 类型别名key_alloc_type, 表示用于分配键类型的分配器类型        typedef typename Alloc::template rebind<T>::other key_alloc_type;        // 类型别名payload_alloc_type, 表示用于分配值类型的分配器类型        typedef typename Alloc::template rebind<P>::other payload_alloc_type;        // 类型别名value_alloc_type, 表示用于分配键值对类型的分配器类型        typedef typename Alloc::template rebind<V>::other value_alloc_type;        // 类型别名bitmap_alloc_type, 表示用于分配位图类型的分配器类型        typedef typename Alloc::template rebind<uint64_t>::other bitmap_alloc_type;        // 常量引用key_less_, 表示用于比较键大小的比较函数对象        const Compare &key_less_;        // 常量引用allocator_, 表示用于分配内存的分配器对象        const Alloc &allocator_;        // Forward declaration        template<typename node_type = self_type, typename payload_return_type = P, typename value_return_type = V>        class Iterator;    // 迭代容器中的元素        // 类型别名iterator_type, 表示非const迭代器类型        typedef Iterator<> iterator_type;        // 类型别名const_iterator_type, 表示const迭代器类型        typedef Iterator<const self_type, const P, const V> const_iterator_type;        // 一个指向下一个叶子节点的指针next_leaf_, 初始值为nullptr        self_type *next_leaf_ = nullptr;        // 一个指向上一个叶子节点的指针prev_leaf_, 初始值为nullptr        self_type *prev_leaf_ = nullptr;&#125;;</code></pre><p>数据节点属性：</p><pre class=" language-c++"><code class="language-c++">#if ALEX_DATA_NODE_SEP_ARRAYS    T *key_slots_ = nullptr;        // holds keys    P *payload_slots_ = nullptr;    // holds payloads, must be same size as key_slots#else    V* data_slots_ = nullptr;  // holds key-payload pairs#endif// 键/数据槽数组的大小int data_capacity_ = 0;  // size of key/data_slots array// 已填充的键/数据槽的数量（与间隙相对）int num_keys_ = 0;  // number of filled key/data slots (as opposed to gaps)// Bitmap: each uint64_t represents 64 positions in reverse order// (i.e., each uint64_t is "read" from the right-most bit to the left-most bit)// bitmap_是一个位图，每个uint64_t代表64个位置，从右到左读取uint64_t *bitmap_ = nullptr;// 位图中的int64_t数量int bitmap_size_ = 0;  // number of int64_t in bitmap// Variables related to resizing (expansions and contractions)static constexpr double kMaxDensity_ = 0.8;  // density after contracting,// also determines the expansion // thresholdstatic constexpr double kInitDensity_ = 0.7;  // density of data nodes after bulk loadingstatic constexpr double kMinDensity_ = 0.6;  // density after expanding, also// determines the contraction // thresholddouble expansion_threshold_ = 1;  // expand after m_num_keys is >= this numberdouble contraction_threshold_ = 0;  // contract after m_num_keys is < this numberstatic constexpr int kDefaultMaxDataNodeBytes_ = 1 << 24;  // by default, maximum data node size is 16MB// 最大键/数据槽的数量，不能超过这个数量进行扩展int max_slots_ = kDefaultMaxDataNodeBytes_ / sizeof(V);  // cannot expand beyond this number of key/data slots// Counters used in cost modelslong long num_shifts_ = 0;                 // does not reset after resizinglong long num_exp_search_iterations_ = 0;  // does not reset after resizingint num_lookups_ = 0;                      // does not reset after resizingint num_inserts_ = 0;                      // does not reset after resizingint num_resizes_ = 0;  // technically not required, but nice to have// Variables for determining append-mostly behaviorT max_key_ = std::numeric_limits<T>::lowest();  // max key in node, updates after inserts but not erasesT min_key_ = std::numeric_limits<T>::max();  // min key in node, updates after// inserts but not erasesint num_right_out_of_bounds_inserts_ = 0;  // number of inserts that are larger than the max keyint num_left_out_of_bounds_inserts_ = 0;  // number of inserts that are smaller than the min key// Node is considered append-mostly if the fraction of inserts that are out of// bounds is above this threshold// Append-mostly nodes will expand in a manner that anticipates further// appendsstatic constexpr double kAppendMostlyThreshold = 0.9;// Purely for benchmark debugging purposesdouble expected_avg_exp_search_iterations_ = 0;double expected_avg_shifts_ = 0;// Placed at the end of the key/data slots if there are gaps after the max key// 一个哨兵值，放置在键/数据槽的末尾，用于标记最大键之后的任何间隙static constexpr T kEndSentinel_ = std::numeric_limits<T>::max();</code></pre><p><strong>构造函数</strong>：</p><pre class=" language-c++"><code class="language-c++"></code></pre><p><strong>迭代器</strong>：</p><pre class=" language-c++"><code class="language-c++"></code></pre><p><strong>插入</strong>：</p><pre class=" language-c++"><code class="language-c++"></code></pre><p><strong>查询</strong>：</p><pre class=" language-c++"><code class="language-c++"></code></pre>]]></content>
      
      
      <categories>
          
          <category> 学习型索引 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习型索引 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FINEdex</title>
      <link href="posts/323e.html"/>
      <url>posts/323e.html</url>
      
        <content type="html"><![CDATA[<h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><p>随着现有数据量的快速增长，人们考虑使用学习索引来替换传统的索引结构。但是传统的学习索引无法解决高并发需求，作者首先总结了现有的学习索引存在的短板：</p><ul><li>有限的可扩展性。针对插入更新操作，现有的学习索引很难拥有很好的性能。目前提出的一些学习索引结构只能单独的解决高并发的读，写，重训练。FITing-tree,ALEX,PGM-index都没有考虑数据一致性问题。XIndex通过把数据保存在不同的数据结构中来解决并发的数据一致性问题，这样会导致范围查询操作的性能低下。</li><li>高开销问题。XIndex和FITing-tree都是通过建立一个delta buffer来解决插入操作，该缓冲区是B+树或者Mass树。并且XIndex显示，当缓冲区过大时，会产生很大程度的性能下降。ALEX和PGM-index是通过在索引结构中保留了空槽来解决插入时产生的高开销问题。一旦发生多线程访问时，会发生多个线程的冲突访问情况，这会导致并发操作的性能下降。</li></ul><p>作者提出了FINEdex做出了如下贡献：</p><ol><li>高可伸缩性。提出了一种用于并发内存系统的非粒度学习索引方案，即FINEdex，它有效地满足了可伸缩性的要求。主要的贡献是通过充实的数据结构来减少数据依赖性，并同时在两个粒度中对模型进行重新训练。</li><li>低开销的索引操作和无阻塞的重训练操作。</li><li>系统应用以及评测。</li></ol><p><img src="https://yux20000304.github.io/2022/10/12/FINEdex%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/1.png" alt="img"></p><p>上图给出了目前现有的学习索引的功能。</p><h2 id="设计部分介绍"><a href="#设计部分介绍" class="headerlink" title="设计部分介绍"></a>设计部分介绍</h2><p>首先给出了数据结构设计部分。</p><p><img src="https://yux20000304.github.io/2022/10/12/FINEdex%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/2.png" alt="img"></p><p>作者通过对XIndex的设计结构进行修改，实现了一个高效的并发性数据索引结构。实现了一个两级的有序数组结构。该结构能够很好的避免数据依赖性并且能够保持数据的有序性。并且在插入的过程中，旧数据的访问不会受到影响。只有当缓冲区满了的时候，才会发生数据合并以及模型重训练操作。作者把整个设计分成了两个部分：</p><ol><li>模型部分：这些独立的模型可以通过并发的再训练来适应新的数据分布。</li><li>数据部分：通过层级结构，建立了两层有序数组，提供能快速重训练的方案。</li></ol><ul><li>模型部分，首先作者设计了一个方案来提高模型的准确率。学习探针算法LPA。通过在f(x)函数周围建立一个平行四边形囊括所有的训练数据，之后再使用线性回归模型进行拟合。传统的RMI模型，由于无法针对不同分布的数据来决定模型的个数，RMI模型的准确率一直是一个潜在的问题。LPA算法通过设置一个错误率阈值，来判断某一组数据是否为线性分布的。如果大于这个阈值，那么移除该数据集靠后的一部分数据，如果小于阈值，那么添加新的数据进入原来的数据集中再重新训练。这个过程一共需要提供如下几个参数：</li></ul><ol><li>threshold：错误率阈值。</li><li>learning_step：学习步长，用来决定学习速度的参数之一。</li><li>learning_rate：学习率，用来决定学习速度的参数之一。<br>整个过程是一个贪心算法。最终得到的输出是一串错误率为threshold的线性模型链。</li></ol><p>除了以上的几个参数设置，作者还提出了优化模型输出的方案。上面提到了LPA算法得到的是一串线性模型链，在执行查找等操作时，需要顺序遍历整个模型链。最终作者选择使用优化后的b树来存储这些模型&lt;key，model&gt;，key代表了一个模型的最大元素。</p><ul><li><p>数据部分，主要解决了并发性问题。每一个模型都有一个小型的缓冲区来解决修改和插入等操作。整个小型缓冲区被设计成了两层的b树结构，在训练好的模型中，每个数组中的元素都有一个bin指针，用来指向level bin缓冲区，该缓冲区分为root层和child层，首先插入的数据会被放到root层，随着root层满了，会创建child节点来存储新插入的节点，之后插入的节点会优先插入到靠前的child节点来节约空间使用。<strong>缺陷：在插入的过程中需要不断地调整树形结构，这可能会造成很大的性能开销</strong></p></li><li><p>紧接着，作者提出了在并发情况下的模型重训练设计思路。作者把重训练分成了两个部分：level-bin重训练和模型重训练。</p><ul><li>level-bin重训练<br>作者提出每当一个level-bin层满了之后，只需要对一个该层进行重训练，不需要影响其余的level-bin节点，产生的时间开销仅有27us</li><li>模型重训练<br>作者通过使用RCU技术保证了新模型的训练过程中，旧模型的访问操作不被影响。由于level-bins是通过指针指向的，我们只需要在复制的过程中使用新的指针指向level-bin即可。模型重训练会在模型需要重新训练一个更小的模型时被触发。</li></ul></li><li><p>并发：作者通过提出一系列方案来解决并发冲突</p><ul><li>写写冲突：当不同线程需要对同样的数据或者是bin进行修改会产生写写冲突。作者提出使用细粒度的锁分别对记录和bin进行上锁。对记录上锁的操作比较简单，对于bin上锁，需要根据情况进行上锁，有的时候可能只需要对child bin进行上锁，有的情况需要对root bin也进行上锁。</li><li>读写冲突：通过使用版本控制实现数据一致性。每次完成训练后，都会给模型和数据分配一个版本号。在一次访问过程中，首先会获取版本号，如果在读取完成后版本号没有发生改变，那么说明读取操作的数据是有效的，如果无效那么重新发起读请求。</li></ul></li></ul><h2 id="测试部分介绍"><a href="#测试部分介绍" class="headerlink" title="测试部分介绍"></a>测试部分介绍</h2><p>在redis中进行测试。只修改了基础的排序数据结构。一共提供了六个常见的api（train，get，put，update，remove，scan）。一共分配了4个线程来执行模型的重训练，访问过程一共分为三步：在模型中进行查找，计算区间，操作level-bin。使用24线程进行测试。使用了Masstree，Xindex，LI+Δ三个索引设计方案进行对比。没有采用redis原生的跳表结构进行对比，因为其与树形结构的性能比较类似。使用LI+Δ方案时，为了让其支持并发操作，使用了XIndex的delta buffer方案，使用masstree管理该缓冲区。FIT树由于不支持并发没有进行比较。</p><ul><li>配置部分：学习索引结构都使用了两层RMI模型，第二层最多容纳250k的模型数量（XIndex中得出的结论）。FINEdex的threshold我们设置成32，level-bin层的root设置成8，child bin大小设置为16。</li><li>使用的测试工具：<ol><li>YCSB测试工具：一共提供了六种不同的测试负载。</li><li>网络博客负载</li><li>文档id负载</li></ol></li></ul>]]></content>
      
      
      <categories>
          
          <category> 学习型索引 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习型索引 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LearnedIndex-Papers</title>
      <link href="posts/5da4.html"/>
      <url>posts/5da4.html</url>
      
        <content type="html"><![CDATA[<h2 id="学习型索引"><a href="#学习型索引" class="headerlink" title="学习型索引"></a>学习型索引</h2><h3 id="1-论文"><a href="#1-论文" class="headerlink" title="1. 论文"></a>1. 论文</h3><h4 id="1-1-年份"><a href="#1-1-年份" class="headerlink" title="1.1 年份"></a>1.1 年份</h4><h5 id="2018"><a href="#2018" class="headerlink" title="2018"></a>2018</h5><table><thead><tr><th>发表年份</th><th>原文地址</th><th>Code</th></tr></thead><tbody><tr><td><a href="papers/2018-SIGMOD-The_Case_for_Learned_Index_Structures.pdf">2018-SIGMOD-The Case for Learned Index Structures</a> : 开山之作</td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3183713.3196909">Sigmod</a></td><td><a href="https://github.com/learnedsystems/RMI">learnedsystems/RMI</a></td></tr></tbody></table><hr><h5 id="2019"><a href="#2019" class="headerlink" title="2019"></a>2019</h5><table><thead><tr><th>发表年份</th><th>原文地址</th><th>Code</th></tr></thead><tbody><tr><td><a href="papers/2019-arXiv-A_Benchmark_for_Learned_Indexes.pdf">2019-arXiv-SOSD: A Benchmark for Learned Indexes</a>  : 测试基准</td><td><a href="https://arxiv.org/pdf/1911.13014.pdf">arXiv</a></td><td><a href="https://github.com/learnedsystems/SOSD">learnedsystems/SOSD</a></td></tr><tr><td><a href="papers/2019-arXiv-A_Scalable_Learned_Index_Scheme_in_Storage_Systems.pdf">2019-arXiv-A Scalable Learned Index Scheme in Storage Systems</a>:  FINEdex 版之始</td><td><a href="https://arxiv.org/pdf/1905.06256.pdf">arXiv</a></td><td></td></tr><tr><td><a href="papers/2019-SIGMOD-FITing-Tree_A_Data-aware_Index_Structure.pdf">2019-SIGMOD-FITing-Tree_A Data-aware Index Structure</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3299869.3319860">Sigmod</a></td><td></td></tr><tr><td><a href="papers/2019-%E8%BD%AF%E4%BB%B6%E5%AD%A6%E6%8A%A5-%E5%9F%BA%E4%BA%8E%E4%B8%AD%E9%97%B4%E5%B1%82%E7%9A%84%E5%8F%AF%E6%89%A9%E5%B1%95%E5%AD%A6%E4%B9%A0%E7%B4%A2%E5%BC%95%E6%8A%80%E6%9C%AF.pdf">2019-软件学报-基于中间层的可扩展学习索引技术</a></td><td><a href="https://www.jos.org.cn/html/2020/3/5910.htm">软件学报</a></td><td></td></tr></tbody></table><hr><h5 id="2020"><a href="#2020" class="headerlink" title="2020"></a>2020</h5><table><thead><tr><th>发表年份</th><th>原文地址</th><th>Code</th></tr></thead><tbody><tr><td><a href="papers/2020-%E8%BD%AF%E4%BB%B6%E5%AD%A6%E6%8A%A5-%E5%AD%A6%E4%B9%A0%E7%B4%A2%E5%BC%95%EF%BC%9A%E7%8E%B0%E7%8A%B6%E4%B8%8E%E7%A0%94%E7%A9%B6%E5%B1%95%E6%9C%9B.pdf">2020-软件学报-学习索引：现状与研究展望</a></td><td><a href="https://www.jos.org.cn/html/2021/4/6168.htm">软件学报</a></td><td></td></tr><tr><td><a href="papers/2020-aiDM-RadixSpline_A_Single-Pass_Learned_Index.pdf">2020-aiDM-RadixSpline_A Single-Pass Learned Index</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3401071.3401659">aiDM</a></td><td></td></tr><tr><td><a href="papers/2020-APSys-SIndex_A_Scalable_Learned_Index_for_String_Keys.pdf">2020-APSys-SIndex_A Scalable Learned Index for String Keys</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3409963.3410496">APSys</a></td><td></td></tr><tr><td><a href="papers/2020-ICDEW-START_Self-Tuning_Adaptive_Radix_Tree.pdf">2020-ICDEW-START_Self-Tuning_Adaptive_Radix_Tree</a></td><td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9094133">ICDE</a></td><td></td></tr><tr><td><a href="papers/2020-NeurIPS-Learned_Indexes_for_a_Google-scale_Disk-based_Database.pdf">2020-NeurIPS-Learned Indexes for a Google-scale Disk-based Database</a></td><td><a href="https://arxiv.org/pdf/2012.12501.pdf">arXiv</a></td><td></td></tr><tr><td><a href="papers/2020-OSDI-Bourbon_learned_LSM.pdf">2020-OSDI-Bourbon_learned_LSM</a></td><td><a href="https://dl.acm.org/doi/pdf/10.5555/3488766.3488775">OSDI</a></td><td></td></tr><tr><td><a href="papers/2020-OSDI-Bourbon_learned_LSM_slides.pdf">2020-OSDI-Bourbon_learned_LSM_slides</a></td><td><a href="https://dl.acm.org/doi/pdf/10.5555/3488766.3488775">OSDI</a></td><td></td></tr><tr><td><a href="papers/2020-PPoPP-XIndex_A_Scalable_Learned_Index_for_Multicore_Data_Storage.pdf">2020-PPoPP-XIndex_A Scalable Learned Index for Multicore Data Storage</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3332466.3374547">PPoPP</a></td><td></td></tr><tr><td><a href="papers/2020-VLDB-PGM-index_fully-dynamic_compressed_worst-case_bounds.pdf">2020-VLDB-PGM-index_fully-dynamic_compressed_worst-case_bounds</a></td><td><a href="https://dl.acm.org/doi/pdf/10.14778/3389133.3389135">PVLDB</a></td><td></td></tr><tr><td><a href="papers/2020-SIGMOD-ALEX_Updatable_Adaptive_Learned_Index.pdf">2020-SIGMOD-ALEX_Updatable_Adaptive_Learned_Index</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3318464.3389711">Sigmod</a></td><td></td></tr><tr><td><a href="papers/2020-SIGMOD-CDFShop_Exploring_and_Optimizing_Learned_Index_Structures.pdf">2020-SIGMOD-CDFShop Exploring and Optimizing Learned Index Structures</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3318464.3384706">Sigmod</a></td><td></td></tr><tr><td><a href="papers/2020-SIGMOD-Order-Preserving_Key_Compression_for_In-Memory_Search_Trees.pdf">2020-SIGMOD-Order-Preserving Key Compression for In-Memory Search Trees</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3318464.3380583">Sigmod</a></td><td></td></tr><tr><td><a href="papers/2020-SIGMOD-Order-Preserving_Key_Compression_for_In-Memory_Search_Trees_slides.pdf">2020-SIGMOD-Order-Preserving Key Compression for In-Memory Search Trees_slides</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3318464.3380583">Sigmod</a></td><td></td></tr><tr><td><a href="papers/2020-SIGMOD-The_Case_for_a_Learned_Sorting_Algorithm.pdf">2020-SIGMOD-The Case for a Learned Sorting Algorithm</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3318464.3389752">Sigmod</a></td><td></td></tr></tbody></table><hr><h5 id="2021"><a href="#2021" class="headerlink" title="2021"></a>2021</h5><table><thead><tr><th>发表年份</th><th>原文地址</th><th>Code</th></tr></thead><tbody><tr><td><a href="papers/2021-AIDB-PLEX_RS+CHT.pdf">2021-AIDB-PLEX_RS+CHT</a></td><td><a href="https://arxiv.org/pdf/2108.05117.pdf">AIDB</a></td><td></td></tr><tr><td><a href="papers/2021-AIDB-RSS_Bounding_the_Last_Mile-Efficient_Learned_String_Indexing.pdf">2021-AIDB-RSS_Bounding_the_Last_Mile-Efficient_Learned_String_Indexing</a></td><td><a href="https://arxiv.org/pdf/2111.14905.pdf">AIDB</a></td><td></td></tr><tr><td><a href="papers/2021-aiDM-RUSLI_Real-time_Updatable_Spline_Learned_Index.pdf">2021-aiDM-RUSLI_Real-time_Updatable_Spline_Learned_Index</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3464509.3464886">aiDM</a></td><td></td></tr><tr><td><a href="papers/2021-aiDM-Tailored_Regression_Learned_Indexes-Logarithmic-Error-Regression.pdf">2021-aiDM-Tailored_Regression_Learned_Indexes-Logarithmic-Error-Regression</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3464509.3464891">aiDM</a></td><td><a href="https://github.com/umatin/LogarithmicErrorRegression">LogarithmicErrorRegression</a></td></tr><tr><td><a href="papers/2021-arXiv-Micro-architectural_Analysis_of_a_Learned_Index.pdf">2021-arXiv-Micro-architectural Analysis of a Learned Index</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3533702.3534917">arXiv</a></td><td></td></tr><tr><td><a href="papers/2021-arXiv-Pluggable_Learned_Index_Method_via_Sampling_and_Gap_Insertion.pdf">2021-arXiv-Pluggable_Learned_Index_Method_via_Sampling_and_Gap_Insertion</a></td><td><a href="https://arxiv.org/pdf/2101.00808.pdf">arXiv</a></td><td></td></tr><tr><td><a href="papers/2021-EDBT-Shift-Table_A_Low-latency_Learned_Index_for_Range_Queries_using_Model_Correction.pdf">2021-EDBT-Shift-Table A Low-latency Learned Index for Range Queries using Model Correction</a></td><td><a href="https://arxiv.org/pdf/2101.10457.pdf">EDBT</a></td><td></td></tr><tr><td><a href="papers/2021-ICDEW-Towards_a_Benchmark_for_Learned_Systems.pdf">2021-ICDEW-Towards a Benchmark for Learned Systems</a></td><td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9438803">ICDE</a></td><td></td></tr><tr><td><a href="papers/2021-PVLDB-Benchmarking_Learned_Indexes.pdf">2021-PVLDB-Benchmarking_Learned_Indexes</a></td><td><a href="https://dl.acm.org/doi/pdf/10.14778/3421424.3421425">PVLDB</a></td><td></td></tr><tr><td><a href="papers/2021-PVLDB-Endownment-learned_cardition.pdf">2021-PVLDB-Endownment-learned_cardition</a></td><td><a href="https://dl.acm.org/doi/pdf/10.14778/3461535.3461552">PVLDB</a></td><td></td></tr><tr><td><a href="papers/2021-PVLDB-FINEdex-Fine-grained_for_Scalable_Concurrent_Memory_Systems.pdf">2021-PVLDB-FINEdex-Fine-grained_for_Scalable_Concurrent_Memory_Systems</a></td><td><a href="https://dl.acm.org/doi/pdf/10.14778/3489496.3489512">PVLDB</a></td><td></td></tr><tr><td><a href="papers/2021-PVLDB-LIPP_Updatable_Learned_Index_Precise_Positions.pdf">2021-PVLDB-LIPP_Updatable_Learned_Index_Precise_Positions</a></td><td><a href="https://dl.acm.org/doi/pdf/10.14778/3457390.3457393">PVLDB</a></td><td><a href="https://github.com/Jiacheng-WU/lipp">lipp</a></td></tr></tbody></table><hr><h5 id="2022"><a href="#2022" class="headerlink" title="2022"></a>2022</h5><table><thead><tr><th>发表年份</th><th>原文地址</th><th>Code</th></tr></thead><tbody><tr><td><a href="papers/2022-aiDB-AutoIndex_Automatically_Finding_Optimal_Index_Structure.pdf">2022-aiDB-AutoIndex_Automatically_Finding_Optimal_Index_Structure</a></td><td><a href="https://arxiv.org/pdf/2208.03823.pdf">aiDB</a></td><td></td></tr><tr><td><a href="papers/2022-aiDM-LSI-Learned_Secondary_Index_Structure.pdf">2022-aiDM-LSI-Learned_Secondary_Index_Structure</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3533702.3534912">aiDM</a></td><td></td></tr><tr><td><a href="papers/2022-ICLR_learned_index_with_dynamic_eps.pdf">2022-ICLR_learned_index_with_dynamic_eps</a></td><td><a href="https://openreview.net/pdf?id=VyZRObZ19kt">ICLR</a></td><td></td></tr><tr><td><a href="papers/2022-learned_Similarity_Search.pdf">2022-learned_Similarity_Search</a></td><td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9891778">IEEE</a></td><td></td></tr><tr><td><a href="papers/2022-PVLDB-are_updatable_learned_index_ready.pdf">2022-PVLDB-are_updatable_learned_index_ready </a></td><td><a href="https://dl.acm.org/doi/pdf/10.14778/3551793.3551848">PVLDB</a></td><td></td></tr><tr><td><a href="papers/2022-PVLDB-Endowment-Can_Learned_Models_Replace_Hash_Functions.pdf">2022-PVLDB-Endowment-Can_Learned_Models_Replace_Hash_Functions</a></td><td><a href="https://dl.acm.org/doi/pdf/10.14778/3570690.3570702">PVLDB</a></td><td></td></tr><tr><td><a href="papers/2022-PVLDB-Endowment-FILM-for-Larger-than-Memory-Databases.pdf">2022-PVLDB-Endowment-FILM-for-Larger-than-Memory-Databases</a></td><td><a href="https://dl.acm.org/doi/pdf/10.14778/3570690.3570704">PVLDB</a></td><td></td></tr><tr><td><a href="papers/2022-TOS-The_Concurrent_Learned_Indexes_for_Multicore_Data_Storage.pdf">2022-TOS-The Concurrent Learned Indexes for Multicore Data Storage</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3478289">TOS</a></td><td></td></tr><tr><td><a href="papers/2022-VLDB-APEX_Learned_Index_PM.pdf">2022-VLDB-APEX_Learned_Index_PM</a></td><td><a href="https://dl.acm.org/doi/pdf/10.14778/3494124.3494141">VLDB</a></td><td></td></tr><tr><td><a href="papers/2022-VLDB-NFL_Learned_Index_Distribution_Transformation.pdf">2022-VLDB-NFL_Learned_Index_Distribution_Transformation</a>: it transforms keys distribution to make it more linear.</td><td><a href="https://dl.acm.org/doi/pdf/10.14778/3547305.3547322">VLDB</a></td><td></td></tr></tbody></table><hr><h5 id="2023"><a href="#2023" class="headerlink" title="2023"></a>2023</h5><table><thead><tr><th>发表年份</th><th>原文地址</th><th>Code</th></tr></thead><tbody><tr><td><a href="papers/2023-arXiv-DILI_A_Distribution-Driven_Learned_Index.pdf">2023-arXiv-DILI_A Distribution-Driven Learned Index</a>  Using more bulk loading time for better lookup.</td><td><a href="https://arxiv.org/pdf/2304.08817.pdf">arXiv</a></td><td></td></tr><tr><td><a href="papers/2023-arxiv-Updatable_Learned_Indexes_Disk-Resident_DBMS.pdf">2023-arXiv-Updatable_Learned_Indexes_Disk-Resident_DBMS</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3589284">arXiv</a></td><td></td></tr><tr><td><a href="papers/2023-ASPLOS-LeaFTL-leared_FTL_for_SSD.pdf">2023-ASPLOS-LeaFTL- A Learning-Based Flash Translation Layer for Solid-State Drives</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3575693.3575744">ASPLOS</a></td><td></td></tr><tr><td><a href="papers/2023-FAST-ROLEX.pdf">2023-FAST-ROLEX</a></td><td></td><td></td></tr><tr><td><a href="papers/2023-PVLDB-Learned_Index_A_Comprehensive_Experimental_Evaluation.pdf">2023-PVLDB-Learned Index_A Comprehensive Experimental Evaluation</a></td><td><a href="https://dl.acm.org/doi/pdf/10.14778/3594512.3594528">PVLDB</a></td><td></td></tr></tbody></table><hr><h4 id="1-2-类别"><a href="#1-2-类别" class="headerlink" title="1.2 类别"></a>1.2 类别</h4><h5 id="Survey-Benchmark-Tuning"><a href="#Survey-Benchmark-Tuning" class="headerlink" title="Survey/Benchmark/Tuning"></a>Survey/Benchmark/Tuning</h5><ol><li> <a href="papers/2019-arXiv-A_Benchmark_for_Learned_Indexes.pdf">2019-arXiv-A Benchmark for Learned Indexes</a></li><li> <a href="papers/2020-SIGMOD-CDFShop_Exploring_and_Optimizing_Learned_Index_Structures.pdf">2020-SIGMOD-demo-CDFShop-tuning_RMI</a></li><li> <a href="papers/2021-PVLDB-Benchmarking_Learned_Indexes.pdf">2021-PVLDB-Benchmarking_Learned_Indexes</a></li><li> <a href="papers/2021-aiDM-Tailored_Regression_Learned_Indexes-Logarithmic-Error-Regression.pdf">2021-aiDM-Tailored_Regression_Learned_Indexes-Logarithmic-Error-Regression</a></li><li> <a href="papers/2022-PVLDB-are_updatable_learned_index_ready.pdf">2022-are_updatable_learned_index_ready</a></li><li> <a href="papers/2023-PVLDB-Learned_Index_A_Comprehensive_Experimental_Evaluation.pdf">2023-PVLDB-Endowment-Comprehensive_Experimental_Evaluation</a></li></ol><h5 id="Read-only"><a href="#Read-only" class="headerlink" title="Read-only"></a>Read-only</h5><ol><li> <a href="papers/2018-SIGMOD-The_Case_for_Learned_Index_Structures.pdf">2018-SIGMOD-The Case for Learned Index Structures</a></li><li> <a href="papers/2020-aiDM-RadixSpline_A_Single-Pass_Learned_Index.pdf">2020-aiDM-RadixSpline_A Single-Pass Learned Index</a></li><li> <a href="papers/2020-NeurIPS-Learned_Indexes_for_a_Google-scale_Disk-based_Database.pdf">2020-NeurIPS-Learned Indexes for a Google-scale Disk-based Database</a></li><li> <a href="papers/2021-AIDB-PLEX_RS+CHT.pdf">2021-AIDB-PLEX_RS+CHT</a>: RadixSpine as the top + Compact Hist-Tree as the bottom</li><li> <a href="papers/2021-AIDB-RSS_Bounding_the_Last_Mile-Efficient_Learned_String_Indexing.pdf">2021-AIDB-RSS_Bounding_the_Last_Mile-Efficient_Learned_String_Indexing</a></li></ol><h5 id="Updatable"><a href="#Updatable" class="headerlink" title="Updatable"></a>Updatable</h5><ol><li> <a href="papers/2019-SIGMOD-FITing-Tree_A_Data-aware_Index_Structure.pdf">2019-SIGMOD-FITing-Tree_A Data-aware Index Structure</a></li><li> <a href="papers/2020-SIGMOD-ALEX_Updatable_Adaptive_Learned_Index.pdf">2020-SIGMOD-ALEX_Updatable_Adaptive_Learned_Index</a> Use gapped array for SMO</li><li> <a href="papers/2020-VLDB-PGM-index_fully-dynamic_compressed_worst-case_bounds.pdf">2020-VLDB-PGM-index_fully-dynamic_compressed_worst-case_bounds</a></li><li> <a href="papers/2021-PVLDB-LIPP_Updatable_Learned_Index_Precise_Positions.pdf">2021-PVLDB-LIPP_Updatable_Learned_Index_Precise_Positions</a></li><li> <a href="papers/2021-aiDM-RUSLI_Real-time_Updatable_Spline_Learned_Index.pdf">2021-aiDM-RUSLI_Real-time_Updatable_Spline_Learned_Index</a></li><li> <a href="papers/2022-TOS-The_Concurrent_Learned_Indexes_for_Multicore_Data_Storage.pdf">2022-TOS-Xindex-most-recent</a></li><li> <a href="papers/2023-FAST-ROLEX.pdf">2023-FAST-ROLEX</a></li><li> <a href="papers/2023-arXiv-DILI_A_Distribution-Driven_Learned_Index.pdf">2023-arxiv-DILI-A Distribution-Driven Learned Index</a> Using more bulk loading time for better lookup.</li></ol><h5 id="Secondary-Storage-Persistent-Memory-LSM"><a href="#Secondary-Storage-Persistent-Memory-LSM" class="headerlink" title="Secondary Storage/Persistent Memory/LSM"></a>Secondary Storage/Persistent Memory/LSM</h5><ol><li> <a href="papers/2019-arXiv-A_Scalable_Learned_Index_Scheme_in_Storage_Systems.pdf">2019-arXiv-A Scalable Learned Index Scheme in Storage Systems</a>: the initial version of FINEdex</li><li> <a href="papers/2020-NeurIPS-Learned_Indexes_for_a_Google-scale_Disk-based_Database.pdf">2020-NeurIPS-Learned Indexes for a Google-scale Disk-based Database</a></li><li> <a href="papers/2020-OSDI-Bourbon_learned_LSM.pdf">2020-OSDI-Bourbon_learned_LSM</a></li><li> <a href="papers/2020-OSDI-Bourbon_learned_LSM_slides.pdf">2020-OSDI-Bourbon_learned_LSM_slides</a></li><li> <a href="papers/2022-aiDM-LSI-Learned_Secondary_Index_Structure.pdf">2022-aiDM-LSI-Learned_Secondary_Index_Structure</a></li><li> <a href="papers/2022-VLDB-APEX_Learned_Index_PM.pdf">2022-VLDB-APEX_Learned_Index_PM</a></li><li> <a href="papers/2022-PVLDB-Endowment-FILM-for-Larger-than-Memory-Databases.pdf">2022-PVLDB-Endowment-FILM-for-Larger-than-Memory-Databases</a></li><li> <a href="papers/2023-arxiv-Updatable_Learned_Indexes_Disk-Resident_DBMS.pdf">2023-arxiv-Updatable_Learned_Indexes_Disk-Resident_DBMS</a></li></ol><h5 id="Radix-Spine-based"><a href="#Radix-Spine-based" class="headerlink" title="Radix-Spine based"></a>Radix-Spine based</h5><ol><li> <a href="papers/2020-aiDM-RadixSpline_A_Single-Pass_Learned_Index.pdf">2020-aiDM-Radix_Spline</a>: Using linear spine fits to a CDF, then a flat radix table as an appoximate index.</li><li> <a href="papers/2021-AIDB-RSS_Bounding_the_Last_Mile-Efficient_Learned_String_Indexing.pdf">2021-AIDB-RSS_Bounding_the_Last_Mile-Efficient_Learned_String_Indexing</a></li><li> <a href="papers/2021-AIDB-PLEX_RS+CHT.pdf">2021-AIDB-PLEX_RS+CHT</a></li><li> <a href="papers/2021-aiDM-RUSLI_Real-time_Updatable_Spline_Learned_Index.pdf">2021-aiDM-RUSLI_Real-time_Updatable_Spline_Learned_Index</a></li></ol><h5 id="Variable-length-string-keys"><a href="#Variable-length-string-keys" class="headerlink" title="Variable length string keys"></a>Variable length string keys</h5><ol><li> <a href="papers/2020-APSys-SIndex_A_Scalable_Learned_Index_for_String_Keys.pdf">2020-APSys-SIndex_Scalable_Learned_Index__String_Keys</a></li><li> <a href="papers/2021-AIDB-RSS_Bounding_the_Last_Mile-Efficient_Learned_String_Indexing.pdf">2021-AIDB-RSS_Bounding_the_Last_Mile-Efficient_Learned_String_Indexing</a></li><li> <a href="papers/2020-SIGMOD-Order-Preserving_Key_Compression_for_In-Memory_Search_Trees.pdf">2020-SIGMOD-HOPE</a>: not learned index, but an encoding schme; order persevering encoding for string; can be used for string learned indexes</li><li> <a href="papers/2020-SIGMOD-Order-Preserving_Key_Compression_for_In-Memory_Search_Trees_slides.pdf">2020-SIGMOD-HOPE_slides</a></li></ol><h5 id="Concurrency"><a href="#Concurrency" class="headerlink" title="Concurrency"></a>Concurrency</h5><ol><li> <a href="papers/2020-PPoPP-XIndex_A_Scalable_Learned_Index_for_Multicore_Data_Storage.pdf">2020-PPoPP-XIndex_Scalable_Learned_Index_for_Multicore_Data_Storage</a></li><li> <a href="papers/2020-APSys-SIndex_A_Scalable_Learned_Index_for_String_Keys.pdf">2020-APSys-SIndex_Scalable_Learned_Index_String_Keys</a></li><li> <a href="papers/2021-PVLDB-FINEdex-Fine-grained_for_Scalable_Concurrent_Memory_Systems.pdf">2021-PVLDB-FINEdex-Fine-grained_for_Scalable_Concurrent_Memory_Systems</a></li><li> <a href="papers/2022-TOS-The_Concurrent_Learned_Indexes_for_Multicore_Data_Storage.pdf">2022-TOS-Xindex-most-recent</a></li></ol><h5 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h5><ol><li> <a href="papers/2020-SIGMOD-The_Case_for_a_Learned_Sorting_Algorithm.pdf">2020-SIGMOD_The_Case_for_a_Learned_Sorting_Algorithm</a></li><li> <a href="papers/2022-PVLDB-Endowment-Can_Learned_Models_Replace_Hash_Functions.pdf">2022-PVLDB-Endowment-Can_Learned_Models_Replace_Hash_Functions </a></li><li> <a href="papers/2022-learned_Similarity_Search.pdf">2022-learned_Similarity_Search</a></li><li> <a href="papers/2023-ASPLOS-LeaFTL-leared_FTL_for_SSD.pdf">2023-ASPLOS-LeaFTL-Learning-Based Flash Translation Layer for Solid-State Drives</a> Learned index for SSD FTL page-level memory mapping.</li></ol><hr><h3 id="1-2-教程"><a href="#1-2-教程" class="headerlink" title="1.2 教程"></a>1.2 教程</h3><p><a href="https://blog.csdn.net/weixin_44026604/article/details/120776283">SIndex 论文笔记：A Scalable Learned Index for String Keys-CSDN博客</a></p><p><a href="https://zhuanlan.zhihu.com/p/277979207">OSDI20 - Bourbon: Learned Index for LSM - 知乎 (zhihu.com)</a></p>]]></content>
      
      
      <categories>
          
          <category> 论文整理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文整理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MyHexoBlog</title>
      <link href="posts/22ca.html"/>
      <url>posts/22ca.html</url>
      
        <content type="html"><![CDATA[<h2 id="MyHexoBlog"><a href="#MyHexoBlog" class="headerlink" title="MyHexoBlog"></a>MyHexoBlog</h2><p>首先，GitHub Pages 是由 GitHub 官方提供的一种免费的静态站点托管服务，让我们可以在 GitHub 仓库里托管和发布自己的静态网站页面。同时，Hexo 是一个快速、简洁且高效的静态博客框架，它基于 Node.js 运行，可以将我们撰写的 Markdown 文档解析渲染成静态的 HTML 网页。所以，我们可以先在本地撰写 Markdown 格式文章后，通过 Hexo 解析文档，渲染生成具有主题样式的 HTML 静态网页，再推送到 GitHub 上完成博文的发布。这样别人就能通过网址访问啦。</p><p><strong>优点：</strong>完全免费；静态站点，轻量快速；可按需求自由定制改造；托管在 GitHub，安全省心；迁移方便。</p><p><strong>不足：</strong>发文不便，依赖于本地环境；更适合个人博客使用；GitHub 在国内访问速度有点不快【gitee亦可】。</p><hr><h3 id="1-环境搭建"><a href="#1-环境搭建" class="headerlink" title="1. 环境搭建"></a>1. 环境搭建</h3><h4 id="下载安装-Node-js"><a href="#下载安装-Node-js" class="headerlink" title="下载安装 Node.js"></a>下载安装 Node.js</h4><p> 下载官网：<a href="https://nodejs.org/en">Node.js — Run JavaScript Everywhere (nodejs.org)</a>。 </p><p>安装参考教程：<a href="https://zhuanlan.zhihu.com/p/442215189">node.js安装及环境配置超详细教程【Windows系统安装包方式】 - 知乎 (zhihu.com)</a>。</p><p><strong>注意：可以变更缓存及包下载路径，但C盘够用，则不推荐变更</strong>。</p><p>① 打开安装的目录（默认安装情况下在C:\Program Files\nodejs）</p><pre class=" language-bash"><code class="language-bash">E:\EnvironmentSetting\NodeJs</code></pre><p>② 在安装目录下新建两个文件夹【node_global】和【node_cache】</p><p>③ 使用管理员权限打开 cmd 命令窗口，输入 <code>npm config set prefix “你的路径\node_global”</code> ：</p><pre class=" language-bash"><code class="language-bash">// 原位置<span class="token function">npm</span> config <span class="token keyword">set</span> prefix <span class="token string">"C:\Users\用户名\AppData\Roaming\npm\node_global"</span>// 新位置<span class="token function">npm</span> config <span class="token keyword">set</span> prefix <span class="token string">"E:\EnvironmentSetting\NodeJs\node_global"</span>// 查看位置<span class="token function">npm</span> root -g // 重置<span class="token function">npm</span> config delete prefix</code></pre><p>④ <code>npm config set cache “你的路径\node_cache” </code> ：</p><pre class=" language-bash"><code class="language-bash">// 原位置<span class="token function">npm</span> config <span class="token keyword">set</span> cache <span class="token string">"C:\Users\用户名\AppData\Roaming\node_cache"</span>// 新位置<span class="token function">npm</span> config <span class="token keyword">set</span> cache <span class="token string">"E:\EnvironmentSetting\NodeJs\node_cache"</span></code></pre><p>⑤设置环境变量，打开【系统属性】-【高级】-【环境变量】，在<code>系统变量</code>中新建</p><p>变量名：<code>NODE_PATH</code> </p><p>变量值：<code>E:\EnvironmentSetting\NodeJs\node_global\node_modules</code> </p><p>⑥ 编辑<code>用户变量（环境变量）</code>的 path，将默认的 C 盘下 <code>APPData\Roaming\npm</code> 修改成  </p><p><code>C:\Program Files\nodejs\node_global</code>，点击确定。</p><p>最后在系统 <code>Path</code>里面添加<code>%NODE_PATH%</code> 。执行 <code>npm root -g</code> 查看位置是否变更。</p><p>⑦ 测试：配置完成后，安装个module测试下，咱们就安装最经常使用的express模块，打开cmd窗口，输入以下命令进行模块的全局安装：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> express -g   // -g是全局安装的意思</code></pre><hr><h4 id="下载安装-Git"><a href="#下载安装-Git" class="headerlink" title="下载安装 Git"></a>下载安装 Git</h4><p>下载官网：<a href="https://git-scm.com/book/zh/v2/%E8%B5%B7%E6%AD%A5-%E5%AE%89%E8%A3%85-Git">Git - 安装 Git (git-scm.com)</a>。</p><p>安装参考教程：<a href="https://zhuanlan.zhihu.com/p/443527549">Git下载安装教程：git安装步骤手把手图文【超详细】 - 知乎 (zhihu.com)</a>。</p><p>安装参考教程：<a href="https://blog.csdn.net/mukes/article/details/115693833">Git 详细安装教程（详解 Git 安装过程的每一个步骤）_git安装-CSDN博客</a>。</p><p>安装完成后，Win+R 输入 cmd ，输入 <code>node -v</code>、<code>npm -v</code> 和 <code>git --version</code> 并回车，查看程序版本号。</p><hr><h4 id="配置-Github"><a href="#配置-Github" class="headerlink" title="配置 Github"></a>配置 Github</h4><p>使用邮箱注册 <a href="https://link.zhihu.com/?target=https://github.com/">GitHub</a> 账户，选择免费账户（Free），并完成邮件验证。</p><p>右键 -&gt; Git Bash Here，<strong>设置用户名和邮箱</strong>：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">git</span> config --global user.name <span class="token string">"GitHub 用户名"</span><span class="token function">git</span> config --global user.email <span class="token string">"GitHub 邮箱"</span><span class="token comment" spellcheck="true"># 查看用户名及邮箱</span><span class="token function">git</span> config user.name <span class="token function">git</span> config user.email</code></pre><p><strong>创建 SSH 密匙</strong>：</p><p>输入 <code>ssh-keygen -t rsa -C &quot;GitHub 邮箱&quot;</code>，然后一路回车。</p><pre class=" language-bash"><code class="language-bash">ssh-keygen -t rsa -C <span class="token string">"GitHub 邮箱"</span></code></pre><p><strong>添加密匙</strong>：</p><p>进入 [C:\Users\用户名.ssh] 目录（要勾选显示“隐藏的项目”），用记事本打开公钥 id_rsa.pub 文件并复制里面的内容。</p><p>登陆 GitHub ，进入 Settings 页面，选择左边栏的 SSH and GPG keys，点击 New SSH key。Title 随便取个名字，粘贴复制的 id_rsa.pub 内容到 Key 中，点击 Add SSH key 完成添加。</p><p><strong>验证连接</strong>：</p><p>打开 Git Bash，输入 <code>ssh -T git@github.com</code> 出现 “Are you sure……”，输入 yes 回车确认。</p><pre class=" language-bash"><code class="language-bash"><span class="token function">ssh</span> -T git@github.com</code></pre><p>显示 “Hi xxx! You’ve successfully……” 即连接成功。</p><h4 id="创建公有仓库"><a href="#创建公有仓库" class="headerlink" title="创建公有仓库"></a>创建公有仓库</h4><p>GitHub 主页右上角加号 -&gt; New repository：</p><ul><li>Repository name 中输入 <code>用户名.github.io</code> </li><li>勾选 “Initialize this repository with a README”</li><li>Description 选填</li></ul><p>填好后点击 Create repository 创建，创建后默认自动启用 HTTPS.</p><p>博客地址为：<code>https://用户名.github.io</code>。</p><p>gitee配置Page可参考博客：[在gitee上免费部署静态网站_在gitee上部署静态网站](<a href="https://blog.csdn.net/weixin_38705239/article/details/100161188#:~:text=%E4%BD%A0%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8">https://blog.csdn.net/weixin_38705239/article/details/100161188#:~:text=你可以使用</a> Gitee 来 部署静态 网页。 以下是详细的步骤： 1. 在,2. 将你的 静态 网页代码上传到该仓库中。 3. 在仓库的主页，点击右上角的 “Settings” 进入仓库设置。)。</p><hr><h3 id="2-Hexo"><a href="#2-Hexo" class="headerlink" title="2. Hexo"></a>2. Hexo</h3><p>新建一个文件夹用来存放 Hexo 程序文件，如 <code>MyHexoBlog</code>。cmd 进入该文件夹。</p><h4 id="2-1-安装-Hexo"><a href="#2-1-安装-Hexo" class="headerlink" title="2.1 安装 Hexo"></a>2.1 安装 Hexo</h4><p><strong>使用 npm 一键安装 Hexo 博客程序</strong>：【<strong>若失败，要么开启VPN，要么更换node的镜像源</strong>】</p><pre class=" language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> -g hexo-cli</code></pre><h4 id="2-2-初始化和本地预览"><a href="#2-2-初始化和本地预览" class="headerlink" title="2.2 初始化和本地预览"></a>2.2 初始化和本地预览</h4><p><strong>初始化并安装所需组件</strong>：</p><pre class=" language-bash"><code class="language-bash">hexo init      <span class="token comment" spellcheck="true"># 初始化</span><span class="token function">npm</span> <span class="token function">install</span>    <span class="token comment" spellcheck="true"># 安装组件</span></code></pre><p>若报<code>hexo</code> 未识别，可参考该篇博客：<a href="https://blog.csdn.net/Deng872347348/article/details/121646375">安装hexo时出现的问题：‘hexo‘ 不是内部或外部命令。</a>。</p><p>完成后依次输入下面命令，<strong>启动本地服务器进行预览</strong>：</p><pre class=" language-bash"><code class="language-bash">hexo g   <span class="token comment" spellcheck="true"># 生成页面</span>hexo s   <span class="token comment" spellcheck="true"># 启动预览</span></code></pre><p><strong>访问</strong> <code>http://localhost:4000</code>，出现 Hexo 默认页面，本地博客安装成功！</p><p><strong>Tips：</strong>如果出现页面加载不出来，可能是端口被占用了。Ctrl+C 关闭服务器，运行 <code>hexo server -p 5000</code> 更改端口号后重试。Hexo 博客文件夹目录结构如下：</p><p><img src="https://pic1.zhimg.com/80/v2-264c75c0e493ae8cc5f283567c64ff2c_720w.webp"></p><h4 id="2-3-部署到-GitHub-Pages"><a href="#2-3-部署到-GitHub-Pages" class="headerlink" title="2.3 部署到 GitHub Pages"></a>2.3 部署到 GitHub Pages</h4><p>本地博客测试成功后，就是上传到 GitHub 进行部署，使其能够在网络上访问。</p><p>首先<strong>安装 hexo-deployer-git</strong>：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> hexo-deployer-git --save</code></pre><p>然后<strong>修改 _config.yml</strong> 文件末尾的 Deployment 部分，修改成如下：</p><pre class=" language-bash"><code class="language-bash">deploy:  type: <span class="token function">git</span>  repository: git@github.com:flyboy716/flyboy716.github.io.git  branch: master</code></pre><p>完成后运行 <code>hexo d</code> 将网站上传部署到 GitHub Pages。</p><pre class=" language-bash"><code class="language-bash">hexo d</code></pre><p>完成！这时访问我们的 GitHub 域名 <code>https://用户名.github.io</code> 就可以看到 Hexo 网站了。</p><hr><h4 id="2-4-绑定域名-可选"><a href="#2-4-绑定域名-可选" class="headerlink" title="2.4 绑定域名-可选"></a>2.4 绑定域名-可选</h4><p>博客搭建完成使用的是 GitHub 的子域名（用户名.<a href="https://link.zhihu.com/?target=http://github.io">http://github.io</a>），我们可以为 Hexo 博客绑定自己的域名替换 GitHub 域名，更加个性化和专业，也利于 SEO。</p><p>我们使用 <a href="https://link.zhihu.com/?target=https://www.namesilo.com/?rid=d27fa32do">Namesilo</a> 进行注册，便宜好用没啥套路，使用优惠码 <code>okoff</code> 优惠一美元，com 域名大概 50 块一年。</p><p><strong>域名注册和解析</strong>：</p><ul><li>域名注册和解析教程：<a href="https://zhuanlan.zhihu.com/p/33921436">Namesilo 域名购买及使用教程</a></li></ul><p>按上面教程注册并解析域名，在 DNS 设置部分，删除自带的记录，然后添加 CNAME 记录将 www 域名解析指向 <code>用户名.github.io</code>。</p><p><strong>绑定域名到 Hexo 博客</strong>：</p><p>进入本地博客文件夹的 source 目录，打开记事本，里面输入自己的域名，如 <a href="http://www.example.com,保存名称为/">http://www.example.com，保存名称为</a> “CNAME”，格式为 “所有文件”（无 .txt 后缀）。</p><p>清除缓存等文件并重新发布网站：</p><pre class=" language-bash"><code class="language-bash">hexo clean   <span class="token comment" spellcheck="true"># 清除缓存文件等</span>hexo g       <span class="token comment" spellcheck="true"># 生成页面</span>hexo s       <span class="token comment" spellcheck="true"># 启动预览</span></code></pre><p>现在就可以使用自己的域名访问 Hexo 博客了。</p><p><strong>开启 HTTPS</strong>：配置自己的域名后，需要我们手动开启 HTTPS。打开博客所在 GitHub 仓库，Settings -&gt; 下拉找到 GitHub Pages -&gt; 勾选 Enforce HTTPS。</p><p>HTTPS 证书部署成功需要一定时间，等大概几分钟再访问域名，就可以看到域名前面的小绿锁了，HTTPS 配置完成！</p><hr><h4 id="2-5-开始使用"><a href="#2-5-开始使用" class="headerlink" title="2.5 开始使用"></a>2.5 开始使用</h4><p><strong>发布文章</strong>：</p><p>进入博客所在目录，右键打开 Git Bash Here，创建博文：</p><pre class=" language-bash"><code class="language-bash">hexo new <span class="token string">"My New Post"</span></code></pre><p>然后 source 文件夹中会出现一个 My New Post.md 文件，就可以使用 Markdown 编辑器在该文件中撰写文章了。</p><p>写完后运行下面代码将文章渲染并部署到 GitHub Pages 上完成发布。<strong>以后每次发布文章都是这两条命令</strong>。</p><pre class=" language-bash"><code class="language-bash">hexo g   <span class="token comment" spellcheck="true"># 生成页面</span>hexo d   <span class="token comment" spellcheck="true"># 部署发布</span></code></pre><hr><p>也可以不使用命令自己创建 .md 文件，只需在文件开头手动加入如下格式 Front-matter 即可，写完后运行 <code>hexo g</code> 和 <code>hexo d</code> 发布。</p><pre class=" language-bash"><code class="language-bash">---title: Hello World <span class="token comment" spellcheck="true"># 标题</span>date: 2019/3/26 hh:mm:ss <span class="token comment" spellcheck="true"># 时间</span>categories: <span class="token comment" spellcheck="true"># 分类</span>- Diarytags: <span class="token comment" spellcheck="true"># 标签</span>- PS3- Games---摘要<span class="token operator">&lt;</span><span class="token operator">!</span>--more--<span class="token operator">></span>正文</code></pre><p><strong>网站设置</strong>：</p><p>包括网站名称、描述、作者、链接样式等，全部在网站目录下的 _config.yml 文件中，参考<a href="https://link.zhihu.com/?target=https://hexo.io/zh-cn/docs/configuration">官方文档</a>按需要编辑。注意：冒号后要加一个空格！</p><p><strong>更换主题</strong>：</p><p>在 <a href="https://link.zhihu.com/?target=https://hexo.io/themes/">Themes | Hexo</a> 选择一个喜欢的主题，比如 <a href="https://link.zhihu.com/?target=http://theme-next.iissnan.com/getting-started.html">NexT</a>，进入网站目录打开 Git Bash Here 下载主题：</p><pre class=" language-text"><code class="language-text">git clone https://github.com/iissnan/hexo-theme-next themes/next</code></pre><p>然后修改 _config.yml 中的 theme 为新主题名称 next，发布。（有的主题需要将 _config.yml 替换为主题自带的，参考主题说明。）</p><h4 id="2-6-常用命令"><a href="#2-6-常用命令" class="headerlink" title="2.6 常用命令"></a>2.6 常用命令</h4><pre class=" language-text"><code class="language-text">hexo new "name"       # 新建文章hexo new page "name"  # 新建页面hexo g                # 生成页面hexo d                # 部署hexo g -d             # 生成页面并部署hexo s                # 本地预览hexo clean            # 清除缓存和已生成的静态文件hexo help             # 帮助</code></pre><h4 id="2-7-常见问题"><a href="#2-7-常见问题" class="headerlink" title="2.7 常见问题"></a>2.7 常见问题</h4><p><strong>1、Hexo 设置显示文章摘要，首页不显示全文</strong>。</p><p>Hexo 主页文章列表默认会显示文章全文，浏览时很不方便，可以在文章中插入 <code>&lt;!--more--&gt;</code> 进行分段。</p><p>该代码前面的内容会作为摘要显示，而后面的内容会替换为 “Read More” 隐藏起来。</p><p><strong>2、设置网站图标</strong>。</p><p>进入 themes/主题 文件夹，打开 _config.yml 配置文件，找到 favicon 修改，一般格式为：<code>favicon: 图标地址</code>。（不同主题可能略有差别）</p><p><strong>3、修改并部署后没有效果</strong>。</p><p>使用 <code>hexo clean</code> 清理后重新部署。</p><p><strong>4、开启 HTTPS 后访问网站显示连接不安全？</strong></p><p>证书还未部署生效，等待一会儿，清除浏览器缓存再试。</p><p><strong>5、Mac 安装 Hexo 报错无法安装</strong>。</p><p>Mac 用户需要管理员权限运行，使用 <code>sudo npm install -g hexo-cli</code> 命令安装。</p><p><strong>6、npm 下载速度慢，甚至完全没反应</strong>。</p><p>使用 npm 安装程序等待很久也没反应，或者下载速度很慢，可以更换 npm 源为国内 npm 镜像。</p><p>临时更换方法：在 npm 安装命令后面加上：</p><pre class=" language-text"><code class="language-text">--registry https://registry.npm.taobao.org </code></pre><hr><h4 id="2-8-结语"><a href="#2-8-结语" class="headerlink" title="2.8 结语"></a>2.8 结语</h4><p>Hexo 是一种纯静态的博客，我们必须要在本地完成文章的编辑再部署到 GitHub 上，依赖于本地环境。不能像 WordPress 或 Typecho 那样的动态博客一样能直接在浏览器中完成撰文和发布。</p><p>可以说是一种比较极客的写博客方式，但是优势也是明显的——免费稳定省心，比较适合爱折腾研究的用户，或者没有在线发文需求的朋友。</p><hr><h3 id="3-主题"><a href="#3-主题" class="headerlink" title="3. 主题"></a>3. 主题</h3><h4 id="3-1-Matery"><a href="#3-1-Matery" class="headerlink" title="3.1 Matery"></a>3.1 Matery</h4><p>官方参考地址：<a href="https://github.com/blinkfox/hexo-theme-matery/blob/develop/README_CN.md">hexo-theme-matery/README_CN.md</a>。</p><p><a href="https://blinkfox.github.io/2018/09/28/qian-duan/hexo-bo-ke-zhu-ti-zhi-hexo-theme-matery-de-jie-shao/#searchModal">Hexo博客主题之hexo-theme-matery的介绍 | 闪烁之狐 </a>。</p><hr>]]></content>
      
      
      <categories>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 博客 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
