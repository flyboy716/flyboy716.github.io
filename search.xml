<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>LearnedIndex</title>
      <link href="posts/36c1.html"/>
      <url>posts/36c1.html</url>
      
        <content type="html"><![CDATA[<h2 id="The-Case-for-Learned-Index-Structures"><a href="#The-Case-for-Learned-Index-Structures" class="headerlink" title="The Case for Learned Index Structures"></a>The Case for Learned Index Structures</h2><p>注：作者认为索引即模型，故研究了如何通过 ML 模型 替换 传统数据库索引结构——B树、哈希、布隆过滤器。</p><h3 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h3><p>思想的来源：传统数据库</p><p>然而，所有这些索引仍然是通用数据结构；他们对数据分布没有任何假设，也没有利用现实世界数据中更常见的模式。例如，如果目标是构建一个高度调整的系统来存储和查询一组连续整数键（例如键 1 到 100M）上的固定长度记录范围，则不会使用传统的 B 树对键进行索引，因为键本身可以用作偏移量，这使其成为查找任何键或一系列键的开头的 O(1) 而不是 O(logn) 操作。同样，索引内存大小将从 O(n) 减少到 O(1)。也许令人惊讶的是，其他数据模式也可以进行类似的优化。换句话说，了解确切的数据分布可以高度优化几乎任何索引结构。</p><p>当然，在大多数现实用例中，数据并不完全遵循已知模式，并且为每个用例构建专门解决方案的工程工作量通常过高。然而，我们认为机器学习（ML）提供了学习反映数据模式的模型的机会，从而能够以较低的工程成本自动合成专门的索引结构（称为学习索引）。</p><p>在本文中，我们探讨了学习模型（包括神经网络）可在多大程度上用于增强甚至取代从 B 树到布隆过滤器的传统索引结构。这似乎有悖常理，因为机器学习无法提供我们传统上与这些索引相关联的语义保证，而且传统上认为最强大的机器学习模型、神经网络的计算成本非常高。然而，我们认为这些明显的障碍并不像它们看起来那样有问题。相反，我们使用学习模型的建议有可能带来显着的好处，特别是在下一代硬件上。</p><p>就语义保证而言，索引在很大程度上已经是学习模型，因此用其他类型的 ML 模型替换它们变得非常简单。例如，B 树可以被视为一种模型，它以键作为输入并预测数据记录在排序集中的位置（必须对数据进行排序以实现有效的范围请求）。布隆过滤器是一种二元分类器，它基于键来预测某个键是否存在于集合中。显然，存在微妙但重要的差异。例如，布隆过滤器可以有误报，但不能有误报。【For example, a Bloom filter can have false positives but not false negatives.】然而，正如我们将在本文中展示的那样，可以通过新颖的学习技术和/或简单的辅助数据结构来解决这些差异。</p><p>在性能方面，我们观察到每个CPU都已经具备强大的SIMD功能，并且我们推测许多笔记本电脑和手机很快就会拥有图形处理单元（GPU）或张量处理单元（TPU）。推测 CPU-SIMD/GPU/TPU 将变得越来越强大也是合理的，因为与通用指令集相比，扩展神经网络使用的有限（并行）数学运算集要容易得多。因此，执行神经网络或其他机器学习模型的高成本在未来实际上可能可以忽略不计。例如，Nvidia 和 Google 的 TPU 已经能够在单个周期内执行数千甚至数万个神经网络操作 [3]。</p><p>此外，据称到 2025 年 GPU 的性能将提高 1000 倍，而 CPU 的摩尔定律基本上已经失效 [5]。通过用神经网络取代分支密集的索引结构，数据库和其他系统可以从这些硬件趋势中受益。虽然我们看到了 TPU 等专用硬件上学习索引结构的未来，但本文完全关注 CPU，并且令人惊讶地表明，即使在这种情况下，我们也可以实现显着的优势。</p><p>值得注意的是，我们并不主张用学习索引完全取代传统索引结构。相反，本文的主要贡献是概述和评估一种建立索引的新方法的潜力，该方法补充了现有的工作，并且可以说为这个已有数十年历史的领域开辟了一个全新的研究方向。这是基于以下关键观察：许多数据结构可以分解为学习模型和辅助结构，以提供相同的语义保证。这种方法的潜在力量来自于这样一个事实：描述数据分布的连续函数可用于构建更高效的数据结构或算法。在针对只读分析工作负载的合成数据集和真实世界数据集评估我们的方法时，我们根据经验得到了非常有希望的结果。然而，许多开放的挑战仍然存在，例如如何处理写入繁重的工作负载，我们概述了未来工作的许多可能的方向。此外，我们相信我们可以使用相同的原理来替换（数据库）系统中常用的其他组件和操作。如果成功，将学习模型深度嵌入算法和数据结构的核心思想可能会导致系统开发方式的彻底背离。</p><p>本文的其余部分概述如下：在接下来的两节中，我们以 B 树为例介绍学习索引的一般思想。在第 4 节中，我们将这个想法扩展到哈希映射，并在第 5 节中扩展到布隆过滤器。所有部分都包含单独的评估。最后，在第 6 节中，我们讨论相关工作并在第 7 节中得出结论。</p><hr><h3 id="2-B树—范围查询"><a href="#2-B树—范围查询" class="headerlink" title="2. B树—范围查询"></a>2. B树—范围查询</h3><p>范围索引结构（如 B 树）已经是模型：给定一个键，它们“预测”键排序集中的值的位置。要看到这一点，请考虑分析内存数据库中已排序的主键列上的 B 树索引（即只读），如图 1(a) 所示。在这种情况下，B 树提供从查找键到已排序记录数组内的位置的映射，并保证该位置处的记录的键是等于或高于查找的第一个键钥匙。必须对数据进行排序以允许有效的范围请求。这个相同的一般概念也适用于二级索引，其中数据将是 &lt;key,record_pointer&gt; 对的列表，其中键是索引值，指针是对记录的引用。</p><p><img src="/posts/36c1.htm/image-20240412215411356.png" alt="image-20240412215411356"></p><p>出于效率原因，通常不会对已排序记录的每个键进行索引，而是仅对每个第 n 条记录的键（即页面的第一个键）进行索引。这里我们只假设固定长度记录和连续内存区域（即单个数组）上的逻辑分页，而不是位于不同内存区域的物理页（物理页和可变长度记录在附录 D.2 中讨论）。仅对每个页面的第一个键建立索引有助于显着减少索引必须存储的键数，而不会造成任何显着的性能损失。因此，B 树是一个模型，或者用 ML 术语来说，是一棵回归树：它将键映射到具有最小和最大误差的位置（最小误差为 0，最大误差为页面误差 page-size），并保证可以在该区域中找到该密钥（如果存在）。因此，我们可以用其他类型的 ML 模型（包括神经网络）替换该索引，只要它们也能够对最小和最大误差提供类似的强有力保证。</p><p>乍一看，似乎很难为其他类型的ML模型提供相同的保证，但实际上它非常简单。首先，B-Tree只对存储的键提供最小和最大错误保证，而不是对所有可能的键提供最大和最小错误保证。对于新数据，b树需要重新平衡，或者在机器学习术语中重新训练，以仍然能够提供相同的错误保证。也就是说，对于单调模型，我们唯一需要做的就是对每个键执行模型，并记住位置的最坏预测，以计算最小和最大误差；其次，更重要的是，甚至不需要强错误界限。</p><p>无论如何，数据必须排序以支持范围请求，因此任何错误都可以通过围绕预测的局部搜索(例如，使用指数搜索)轻松纠正，因此，甚至允许非单调模型。因此，我们可以用任何其他类型的回归模型代替b树，包括线性回归或神经网络(见图1(b))。</p><p>现在，在用学习索引取代 B 树之前，我们还需要解决其他技术挑战。例如，B 树的插入和查找成本有限，并且特别擅长利用缓存。此外，B 树可以将键映射到未连续映射到内存或磁盘的页面。所有这些都是有趣的挑战/研究问题，并在本节和附录中更详细地解释，以及潜在的解决方案。</p><p>同时，使用其他类型的模型作为索引可以提供巨大的好处。最重要的是，它有可能将 B 树查找的 logn 成本转换为常量操作。例如，假设一个数据集具有 1M 个唯一键，其值介于 1M 和 2M 之间（因此值 1,000,009 存储在位置 10）。在这种情况下，由单个乘法和加法组成的简单线性模型可以完美预测点查找或范围扫描的任何键的位置，而 B 树则需要 logn 操作。机器学习（尤其是神经网络）的美妙之处在于它们能够学习各种数据分布、混合以及其他数据特性和模式。挑战在于平衡模型的复杂性和准确性。</p><p>对于本文的大部分讨论，我们保留本节的简化假设：我们仅索引按键排序的内存中密集数组。这似乎是有限制的，但是许多现代硬件优化的 B 树，例如 FAST [44]，做出了完全相同的假设，并且这些索引对于内存数据库系统来说非常常见，因为它们在扫描方面具有卓越的性能 [44, 48]或二分查找。</p><p>然而，虽然我们的一些技术可以很好地适应某些场景（例如，具有非常大块的磁盘驻留数据，例如 Bigtable [23] 中使用的），但对于其他场景（细粒度分页、插入繁重的工作负载等） .）需要更多的研究。在附录 D.2 中，我们更详细地讨论了其中一些挑战和潜在的解决方案。</p><hr><h4 id="2-1-什么样的模型复杂度是可以接受的？"><a href="#2-1-什么样的模型复杂度是可以接受的？" class="headerlink" title="2.1 什么样的模型复杂度是可以接受的？"></a>2.1 什么样的模型复杂度是可以接受的？</h4><p>为了更好地理解模型的复杂性，重要的是要知道在遍历 B 树所需的相同时间内可以执行多少个操作，以及模型需要达到什么精度才能比 B 树更高效。</p><p>考虑一个索引 100M 记录、页大小为 100 的 B 树。我们可以将每个 B 树节点视为一种划分空间的方式，减少“错误”并缩小查找数据的区域。因此，我们说页面大小为 100 的 B 树每个节点的精度增益为 1/100，并且我们总共需要遍历 log100N 个节点。因此，第一个节点将空间从 100M 划分为 100M/100 = 1M，第二个节点从 1M 划分为 1M/100 = 10k，依此类推，直到找到记录。</p><p>现在，使用二分搜索遍历单个 B 树页面大约需要 50 个周期，并且众所周知很难并行化3。相比之下，现代 CPU 每个周期可以执行 8-16 个 SIMD 操作。</p><p>因此，只要每 50 * 8 = 400 次算术运算的精度增益高于 1/100，模型就会更快。请注意，此计算仍然假设所有 B-Tree 页面都在缓存中。一次缓存未命中会花费 50-100 个额外周期，因此可以支持更复杂的模型。</p><p>此外，机器学习加速器正在彻底改变游戏规则。它们允许在相同的时间内运行更复杂的模型，并减轻 CPU 的计算负担。例如，NVIDIA最新的Tesla V100 GPU能够实现120 TeraFlops的低精度深度学习算术运算（约每周期60, 000次运算）。</p><p>假设整个学习索引适合 GPU 的内存（我们在第 3.7 节中表明这是一个非常合理的假设），在短短 30 个周期内，我们可以执行 100 万个神经网络操作。当然，传输输入和从 GPU 检索结果的延迟仍然明显较高，但考虑到批处理和/或最近更紧密地集成 CPU/GPU/TPU 的趋势，这个问题并非无法克服 [4]。最后，可以预期的是，GPU/TPU 的能力和每秒浮点/整数运算的数量将继续增加，而提高 CPU 执行 if 语句性能的进展基本上已经停滞 [5]。尽管我们认为 GPU/TPU 是在实践中采用学习索引的主要原因之一，但在本文中，我们将重点放在更有限的 CPU 上，以更好地研究通过机器学习替换和增强索引的影响，而不会影响硬件变化。</p><hr><h4 id="2-2-范围索引模型是CDF模型"><a href="#2-2-范围索引模型是CDF模型" class="headerlink" title="2.2 范围索引模型是CDF模型"></a>2.2 范围索引模型是CDF模型</h4><p>正如本节开头所述，索引是一种以键作为输入并预测记录位置的模型。对于点查询，记录的顺序并不重要，而对于范围查询，必须根据查找键对数据进行排序，以便可以有效地检索范围内（例如，时间范围内）的所有数据项。这导致了一个有趣的观察：预测排序数组中给定键的位置的模型有效地近似了累积分布函数（CDF）。我们可以对数据的 CDF 进行建模来预测位置：        p = F (Key) ∗ N        (1)。</p><p>其中 p 是位置估计，F (Key) 是数据的估计累积分布函数，用于估计观察到小于或等于查找键 P(X ≤ Key) 的键的可能性，N 是键总数（另请参见图 2）。</p><p><img src="/posts/36c1.htm/image-20240412215445045.png" alt="image-20240412215445045"></p><p>这一观察开辟了一组全新的有趣方向：首先，它意味着索引实际上需要学习数据分布。 B 树通过构建回归树来“学习”数据分布。线性回归模型将通过最小化线性函数的（平方）误差来学习数据分布。其次，估计数据集的分布是一个众所周知的问题，学习的索引可以从数十年的研究中受益。第三，学习 CDF 在优化其他类型的索引结构和潜在算法方面也发挥着关键作用，我们将在本文后面概述研究 。第四，关于理论 CDF 与经验 CDF 的接近程度的研究由来已久，这为从理论上理解这种方法的好处提供了立足点 [28]。我们在附录 A 中对我们的方法的扩展程度进行了高级理论分析。</p><hr><h4 id="2-3-第一个学习型索引"><a href="#2-3-第一个学习型索引" class="headerlink" title="2.3 第一个学习型索引"></a>2.3 第一个学习型索引</h4><p>为了更好地理解通过学习模型替换 B 树的要求，我们使用了 200M 的 Web 服务器日志记录，目的是使用 Tensorflow [9] 在时间戳上构建二级索引。我们使用 ReLU 激活函数训练了一个每层 32 个神经元的两层全连接神经网络；时间戳是输入特征，排序数组中的位置是标签。之后，我们使用 Tensorflow 和 Python 作为前端，测量了随机选择的键的查找时间（在几次运行中平均，不考虑第一个数字）。</p><p>在此设置中，我们每秒实现了约 1250 次预测，即使用 Tensorflow 执行模型需要约 80, 000 纳秒 (ns)，而无需搜索时间（从预测位置找到实际记录的时间）。作为比较点，对相同数据进行 B 树遍历大约需要 300 纳秒，而对整个数据进行二分搜索大约需要 900 纳秒。仔细观察，我们发现我们的简单方法在几个关键方面受到限制：（1）Tensorflow 被设计为有效地运行更大的模型，而不是小模型，因此，具有显着的调用开销，特别是使用 Python 作为前端 -结尾。 (2) B 树，或者一般的决策树，非常擅长通过一些操作来过度拟合数据，因为它们使用简单的 if 语句递归地划分空间。相比之下，其他模型可以更有效地近似 CDF 的一般形状，但在单个数据实例级别上存在准确性问题。要了解这一点，请再次考虑图 2。该图表明，从顶层视图来看，CDF 函数显得非常平滑和规则。然而，如果放大到单条记录，就会发现越来越多的违规行为；众所周知的统计效应。因此，神经网络、多项式回归等模型可能会更高效地使用 CPU 和空间，将整个数据集中某个项目的位置缩小到数千个区域，但单个神经网络为了“最后一英里”，通常需要更多的空间和 CPU 时间将错误从数千进一步减少到数百。 (3) B 树的缓存和操作效率极高，因为它们始终将顶部节点保留在缓存中，并在需要时访问其他页面。相比之下，标准神经网络需要所有权重来计算预测，这在乘法次数方面具有很高的成本。</p><h3 id="3-RM-INDEX"><a href="#3-RM-INDEX" class="headerlink" title="3. RM-INDEX"></a>3. RM-INDEX</h3><p>为了克服挑战并探索模型作为索引替代或优化的潜力，我们开发了<strong>学习索引框架 (LIF)、递归模型索引 (RMI) 和基于标准误差的搜索策略</strong>。我们主要关注简单、完全连接的神经网络，因为它们简单且灵活，但我们相信其他类型的模型可能会提供额外的好处。</p><h4 id="3-1-学习索引框架（LIF）"><a href="#3-1-学习索引框架（LIF）" class="headerlink" title="3.1 学习索引框架（LIF）"></a>3.1 学习索引框架（LIF）</h4><p>LIF 可以看作是一个指标综合系统；给定索引规范，LIF 会生成不同的索引配置、优化它们并自动测试它们。虽然 LIF 可以即时学习简单模型（例如线性回归模型），但它依赖于 Tensorflow 来学习更复杂的模型（例如神经网络）。然而，它在推理时从不使用 Tensorflow。</p><p>相反，给定一个经过训练的 Tensorflow 模型，LIF 会自动从模型中提取所有权重，并根据模型规范在 C++ 中生成有效的索引结构。我们的代码生成是专门为小型模型设计的，消除了 Tensorflow 管理大型模型所需的所有不必要的开销和仪器。在这里，我们利用[25]中的想法，它已经展示了如何避免 Spark 运行时不必要的开销。因此，我们能够在 30 纳秒的时间内执行简单的模型。</p><p>然而，应该指出的是，LIF 仍然是一个实验性框架，用于快速评估不同的索引配置（例如，ML 模型、页面大小、搜索策略等），这会以额外计数器的形式引入额外的开销，虚拟函数调用等。除了编译器完成的矢量化之外，我们不使用特殊的 SIMD 内在函数。虽然这些低效率在我们的评估中并不重要，因为我们通过始终使用我们的框架来确保公平比较，但在生产环境中或在将报告的性能数据与其他实现进行比较时，应考虑/避免这些低效率</p><h4 id="3-2-递归模型索引"><a href="#3-2-递归模型索引" class="headerlink" title="3.2 递归模型索引"></a>3.2 递归模型索引</h4><p>正如第 2.3 节所述，构建替代学习模型来替代 B 树的主要挑战之一是最后一英里搜索的准确性。例如，使用单个模型将 100M 记录的预测误差降低到数百个量级通常很困难。同时，即使使用简单的模型，将误差从 100M 降低到 10k，例如通过模型替换 B-Tree 的前 2 层，精度增益为 100 * 100 = 10000，也更容易实现。同样，将误差从 10k 减少到 100 是一个更简单的问题，因为模型可以只关注数据的子集。</p><p>基于这一观察并受到专家工作的启发 [62]，我们提出了递归回归模型（见图 3）。也就是说，我们构建模型的层次结构，在每个阶段模型将密钥作为输入，并基于它选择另一个模型，直到最后阶段预测位置。</p><p><img src="/posts/36c1.htm/image-20240412215509381.png" alt="image-20240412215509381"></p><p>更正式地说，对于我们的模型 f (x)，其中 x 是键，y ∈ [0, N) 位置，我们假设在阶段 l 有 M<del>l</del> 个模型。我们在阶段 0 训练模型，f0(x) ≈ y。因此，阶段 中的模型 k（表示为 f (k) ）经过损失训练：</p><p><img src="/posts/36c1.htm/image-20240412215519648.png" alt="image-20240412215519648"></p><p>考虑不同模型的一种方法是，每个模型都会对密钥的位置做出具有一定误差的预测，并且该预测用于选择下一个模型，该模型负责密钥空间的特定区域以做出更好的预测和更低的误差。但是，递归模型索引不一定是树。如图3所示，一个阶段的不同模型有可能在下一个阶段选择相同的模型。此外，每个模型不一定像 B 树那样覆盖相同数量的记录（即页面大小为 100 的 B 树覆盖 100 条或更少的记录）。最后，根据所使用的模型，不同的阶段不一定被解释为位置估计，而应被视为选择对某些键有更好了解的专家（另见[62]）。</p><p>这种模型架构有几个好处：（1）它将模型大小和复杂性与执行成本分开。 (2)它利用了易于学习数据分布的整体形状的事实。 (3)它有效地将空间划分为更小的子范围，就像B树一样，可以更轻松地以更少的操作实现所需的“最后一英里”精度。 (4) 阶段之间不需要搜索过程。例如，模型1.1的输出直接用于下一阶段的模型选取。这不仅减少了管理结构的指令数量，还允许将整个索引表示为 TPU/GPU 的稀疏矩阵乘法。</p><h4 id="3-3-混合索引"><a href="#3-3-混合索引" class="headerlink" title="3.3 混合索引"></a>3.3 混合索引</h4><p>递归模型索引的另一个优点是，我们能够构建模型的混合。例如，虽然在顶层，小型 ReLU 神经网络可能是最佳选择，因为它们通常能够学习各种复杂的数据分布，但模型层次结构底部的模型可能是数千个简单的线性回归模型，因为它们在空间和执行时间上都很便宜。此外，如果数据特别难学习，我们甚至可以在底层使用传统的 B 树。</p><p>在本文中，我们重点关注两种类型的模型：具有零到两个全连接隐藏层和 ReLU 激活函数的简单神经网络以及最多 32 个神经元的层宽度和 B 树（又名决策树）。请注意，零隐藏层神经网络相当于线性回归。给定索引配置（将阶段数和每个阶段的模型数指定为大小数组），混合索引的端到端训练如算法 1 所示完成：</p><p><img src="/posts/36c1.htm/image-20240412215529772.png" alt="image-20240412215529772"></p><p>从整个数据集（第 3 行）开始，它首先训练顶部节点模型。根据此顶部节点模型的预测，它会从下一阶段（第 9 行和第 10 行）中选择模型，并添加属于该模型的所有键（第 10 行）。最后，在混合索引的情况下，如果绝对最小/最大误差高于预定义阈值（第 11-14 行），则通过用 B 树替换 NN 模型来优化索引。</p><p>请注意，我们在最后阶段存储每个模型的标准误差和最小误差和最大误差。这样做的好处是，我们可以根据每个键所使用的模型单独限制搜索空间。目前，我们通过简单的网格搜索来调整模型的各种参数（即阶段数、每个模型的隐藏层等）。然而，存在许多潜在的优化来加速从 ML 自动调整到采样的训练过程。</p><p>请注意，混合索引允许我们将学习索引的最坏情况性能与 B 树的性能结合起来。也就是说，在极难学习数据分布的情况下，所有模型都会自动替换为 B-Tree，使其几乎成为一整棵 B-Tree。</p><h4 id="3-4-搜索策略和单调性"><a href="#3-4-搜索策略和单调性" class="headerlink" title="3.4 搜索策略和单调性"></a>3.4 搜索策略和单调性</h4><p>范围索引通常实现 upper_bound(key) [lower_bound(key)] 接口来查找排序数组中第一个等于或高于[低于]查找键的键的位置，以有效地支持范围请求。因此，对于学习的范围索引，我们必须根据预测从查找键中找到第一个较高[较低]的键。尽管付出了很多努力，但人们一再报道[8]，二分搜索或扫描具有小有效负载的记录通常是在排序数组中查找键的最快策略，因为替代技术的额外复杂性很少得到回报。然而，学习索引在这里可能有一个优势：模型实际上预测键的位置，而不仅仅是键的区域（即页面）。在这里，我们讨论两种利用此信息的简单搜索策略：</p><p>模型偏向搜索：我们的默认搜索策略，与传统的二分搜索的唯一不同之处在于第一个中间点设置为模型预测的值。</p><p>偏向四元搜索：四元搜索采用三个点而不是一个分割点，希望硬件一次预取所有三个数据点，以便在数据不在缓存中时获得更好的性能。在我们的实现中，我们将四元搜索的初始三个中间点定义为 pos − σ,pos,pos + σ。也就是说，我们猜测我们的大多数预测都是准确的，并将我们的注意力首先集中在位置估计上，然后我们继续传统的四元搜索。</p><p>对于我们所有的实验，我们使用最小和最大误差作为所有技术的搜索区域。也就是说，我们对每个键执行 RMI 模型，并存储每个最后阶段模型最差的过度预测和不足预测。虽然此技术保证找到所有现有键，但对于不存在的键，如果 RMI 模型不是单调的，则可能会返回错误的上限或下限。为了克服这个问题，一种选择是强制我们的 RMI 模型是单调的，正如机器学习中所研究的那样 [41, 71]。</p><p>或者，对于非单调模型，我们可以自动调整搜索区域。也就是说，如果找到的上（下）界键位于由最小和最大误差定义的搜索区域的边界上，我们将逐步调整搜索区域。</p><p>然而，另一种可能性是使用指数搜索技术。假设误差呈正态分布，这些技术平均应该与替代搜索策略一样好，同时不需要存储任何最小和最大误差。</p><h4 id="3-5-索引字符串"><a href="#3-5-索引字符串" class="headerlink" title="3.5 索引字符串"></a>3.5 索引字符串</h4><p>我们主要关注于索引真实值键，但许多数据库依赖于索引字符串，幸运的是，重要的机器学习研究集中在建模字符串上。和以前一样，我们需要设计一个高效且富有表现力的字符串模型。对于字符串来说，做好这件事会带来许多独特的挑战。</p><p>第一个设计考虑因素是如何将字符串转换为模型的特征，通常称为标记化。为了简单和高效，我们将 n 长度的字符串视为特征向量 x ∈ Rn，其中 xi 是 ASCII 十进制值（或 Unicode 十进制值，具体取决于字符串）。此外，如果所有输入的大小相同，大多数机器学习模型的运行效率会更高。因此，我们将设置最大输入长度 N。因为数据是按字典顺序排序的，所以我们在标记化之前将键截断为长度 N。对于长度为 n &lt; N 的字符串，当 i &gt; n 时，我们设置 xi = 0。</p><p>为了提高效率，我们通常遵循与真实值输入类似的建模方法。我们学习相对较小的前馈神经网络的层次结构。唯一的区别是输入不是单个实值 x 而是向量 x。线性模型 w · x + b 随输入长度 N 线性缩放乘法和加法次数。即使具有宽度为 h 的单个隐藏层的前馈神经网络也将缩放 O(hN) 乘法和加法。</p><p>最终，我们相信未来有大量研究可以优化字符串键的学习索引。例如，我们可以轻松想象其他标记化算法。在自然语言处理方面，有大量关于字符串标记化的研究，旨在将字符串分解为对 ML 模型更有用的片段，例如翻译中的单词片段 [70]。此外，将后缀树的想法与学习索引相结合以及探索更复杂的模型架构（例如循环神经网络和卷积神经网络）可能会很有趣。</p><h4 id="3-6-训练"><a href="#3-6-训练" class="headerlink" title="3.6 训练"></a>3.6 训练</h4><p>虽然训练（即加载）时间不是本文的重点，但应该指出的是，我们所有的模型，浅层神经网络甚至简单的线性/多元回归模型，训练速度都相对较快。虽然简单的神经网络可以使用随机梯度下降进行有效训练，并且可以在随机数据上通过不到一到几次的时间收敛，但线性多变量模型（例如，0 层神经网络）存在封闭形式的解决方案，并且它们可以对排序后的数据进行一次训练。因此，对于200M的记录训练一个简单的RMI索引不会比几秒钟长多少时间（当然，这取决于执行多少自动调整）；神经网络可以对每个模型进行几分钟的训练，具体取决于复杂性。另请注意，通常不需要在整个数据上训练顶层模型，因为即使在对整个随机数据进行单次扫描之前，这些模型也经常会收敛。这部分是因为我们使用简单的模型，不太关心精度的最后几个数字点，因为它对索引性能影响很小。</p><p>最后，机器学习社区 [27, 72] 关于改善学习时间的研究适用于我们的背景，我们预计未来会有很多这方面的研究。</p><hr><h3 id="4-哈希—点查询"><a href="#4-哈希—点查询" class="headerlink" title="4. 哈希—点查询"></a>4. 哈希—点查询</h3><p>除了范围索引之外，用于点查找的哈希映射在 DBMS 中也发挥着同样重要的作用。从概念上讲，哈希映射使用哈希函数确定性地将键映射到数组内的位置（参见图 7(a)）。任何有效的哈希映射实现的关键挑战是防止太多不同的键被映射到哈希映射内的相同位置，下文称为冲突。例如，假设有 100M 条记录，哈希映射大小为 100M。对于统一随机化密钥的哈希函数，可以与生日悖论类似地导出预期冲突的数量，并且预期约为 33% 或 33M 个槽。对于每一个冲突，Hash-map架构都需要处理这个冲突。例如，单独的链接哈希映射将创建一个链接列表来处理冲突（参见图 7(a)）。然而，存在许多替代方案，包括二次探测、使用具有多个槽的桶，直至同时使用多个哈希函数（例如，如 Cuckoo Hashing [57] 所做的那样）。</p><p><img src="/posts/36c1.htm/image-20240412215542041.png" alt="image-20240412215542041"></p><p>然而，无论哈希映射架构如何，冲突都会对性能和/或存储要求产生重大影响，并且机器学习模型可能提供减少冲突数量的替代方案。虽然将学习模型作为哈希函数的想法并不新鲜，但现有技术并未利用底层数据分布。例如，各种完美哈希技术[26]也试图避免冲突，但用作哈希函数一部分的数据结构随着数据大小而增长；学习模型可能没有的属性（回想一下，对 1 到 100M 之间的所有键进行索引的示例）。据我们所知，尚未探索是否可以学习产生更有效点索引的模型。</p><h4 id="4-1-哈希模型索引"><a href="#4-1-哈希模型索引" class="headerlink" title="4.1 哈希模型索引"></a>4.1 哈希模型索引</h4><p>令人惊讶的是，学习密钥分布的 CDF 是学习更好的哈希函数的一种潜在方法。然而，与范围索引相比，我们的目的并不是紧凑地或严格排序地存储记录。相反，我们可以通过哈希映射的目标大小 M 来缩放 CDF，并使用 h(K) = F (K)*M，其中密钥 K 作为我们的哈希函数。如果模型 F 完美地学习了按键的经验 CDF，则不会存在冲突。此外，散列函数与实际的散列映射架构正交，并且可以与单独的链接或任何其他散列映射类型组合。</p><p>对于模型，我们可以再次利用上一节中的递归模型架构。显然，和以前一样，索引的大小和性能之间存在权衡，这受到模型和数据集的影响。</p><p>请注意，如何处理插入、查找和冲突取决于哈希映射架构。因此，学习的哈希函数相对于传统哈希函数（将键映射到均匀分布的空间）的优势取决于两个关键因素：（1）模型表示观察到的 CDF 的准确程度。例如，如果数据是由均匀分布生成的，简单的线性模型将能够学习一般的数据分布，但所得的哈希函数不会比任何充分随机的哈希函数更好。 (2) 哈希映射架构：取决于架构、实现细节、有效负载（即值）、冲突解决策略以及将或可以分配多少内存（即槽），都会显着影响性能。例如，对于小键和小值或没有值，采用 Cuckoo 散列的传统散列函数可能会很好地工作，而较大的有效负载或分布式散列映射可能会从避免冲突中获益更多，从而从学习的散列函数中获益更多。</p><hr><h3 id="5-布隆过滤器—存在性查询"><a href="#5-布隆过滤器—存在性查询" class="headerlink" title="5. 布隆过滤器—存在性查询"></a>5. 布隆过滤器—存在性查询</h3><p>DBMS 最后一种常见的索引类型是存在索引，最重要的是布隆过滤器，这是一种节省空间的概率数据结构，用于测试元素是否是集合的成员。</p><p>它们通常用于确定冷存储中是否存在密钥。例如，Bigtable 使用它们来确定 SSTable 中是否包含某个键 [23]。</p><p>在内部，布隆过滤器使用大小为 m 的位数组和 k 个哈希函数，每个函数将一个键映射到 m 个数组位置之一（参见图 9(a)）。为了将一个元素添加到集合中，将一个键输入到 k 个哈希函数中，并将返回位置的位设置为 1。为了测试某个键是否是该集合的成员，将该键再次输入到 k 个哈希函数中。用于接收 k 个数组位置的哈希函数。如果这 k 个位置上的任何一位为 0，则该密钥不是集合的成员。</p><p>换句话说，布隆过滤器确实保证不存在误报，但存在潜在的误报。 </p><p>In other words, a Bloom filter does guarantee that there exists no false negatives, but has potential<br>false positives.</p><p>虽然布隆过滤器具有很高的空间效率，但它们仍然会占用大量内存。例如，对于 10 亿条记录，大约需要 ≈ 1.76 GB。对于 0.01% 的 FPR，我们需要 ≈ 2.23 GB。人们曾多次尝试提高布隆过滤器的效率[52]，但一般观察结果仍然存在。</p><p><img src="/posts/36c1.htm/image-20240412215554614.png" alt="image-20240412215554614"></p><p>然而，如果存在某种可以学习的结构来确定集合内部和外部的内容，那么就有可能构建更有效的表示。有趣的是，对于数据库系统的存在索引，延迟和空间要求通常与我们之前看到的有很大不同。考虑到访问冷存储（例如磁盘甚至带）的高延迟，我们可以负担更复杂的模型，而主要目标是最小化索引空间和误报数量。我们概述了使用学习模型构建存在索引的两种潜在方法。</p>]]></content>
      
      
      <categories>
          
          <category> 学习型索引 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习型索引 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Alex</title>
      <link href="posts/53ff.html"/>
      <url>posts/53ff.html</url>
      
        <content type="html"><![CDATA[<h1 id="ALEX-An-Updatable-Adaptive-Learned-Index"><a href="#ALEX-An-Updatable-Adaptive-Learned-Index" class="headerlink" title="ALEX: An Updatable Adaptive Learned Index"></a>ALEX: An Updatable Adaptive Learned Index</h1><hr><h2 id="整体数据结构设计"><a href="#整体数据结构设计" class="headerlink" title="整体数据结构设计"></a>整体数据结构设计</h2><p>ALEX旨在实现以下目标：</p><ol><li>插入时间应与 B+Tree 相当;</li><li>查找时间应比 B+Tree 和 Learned Index 快;</li><li>索引存储空间应小于 B+Tree 和 Learned Index;</li><li>数据存储空间(叶级)应与 动态B+Tree 相当。</li></ol><p><img src="/posts/53ff.htm/image-20240412211819896.png" alt="image-20240412211819896"></p><hr><h3 id="设计概览"><a href="#设计概览" class="headerlink" title="设计概览"></a>设计概览</h3><p>ALEX：内存中的可更新的学习型索引。但与学习索引亦有许多不同之处：</p><p>区别一：叶节点的数据结构。像B+Tree一样，ALEX在每个叶子上使用一个节点。这允许更灵活地扩展和分割单个节点，并且还限制了插入期间所需的移位次数。在典型的B+树中，每个叶节点存储一个键和有效负载数组，并且在数组末尾有“空闲空间”来吸收插入。ALEX使用了类似的设计，但更仔细地选择了如何使用空闲空间。通过在数组元素之间策略性地引入间隙，可以实现更快的插入和查找时间。如图2所示，ALEX对每个数据节点使用间隙数组(GA)布局。</p><p>区别二：ALEX使用指数搜索来搜索叶级键，以纠正RMI的错误预测，如图2所示。我们通过实验验证了无边界的指数搜索比有边界的二叉搜索更快。这是因为如果模型是好的，它们的预测就足够接近正确的位置。指数搜索还消除了在RMI模型中存储错误边界的需要。</p><p>区别三：ALEX在模型预测键应该在的位置将键插入数据节点。我们称之为基于模型的插入。相反，学习索引在不改变数组中记录位置的情况下对记录数组生成RMI。基于模型的插入具有更好的搜索性能，因为它减少了模型误预测误差。</p><p>区别四：ALEX根据工作量动态调整RMI的形状和高度。</p><p>区别五：ALEX没有需要为每个数据集或工作负载重新调整的参数，不像学习索引，其中必须调整模型的数量。ALEX通过使用 cost模型 自动批量加载和调整 RMI 结构以实现高性能。</p><hr><h3 id="节点布局"><a href="#节点布局" class="headerlink" title="节点布局"></a>节点布局</h3><h4 id="数据节点"><a href="#数据节点" class="headerlink" title="数据节点"></a>数据节点</h4><p>与B+树一样，ALEX的叶节点存储数据记录，因此被称为数据节点，如图2中的圆圈所示。</p><p>数据节点<strong>存储一个线性回归模型</strong>(斜率和截距的两个双值)，它将一个键映射到一个位置，<strong>以及两个gap数组</strong>，<strong>一个用于键，一个用于有效载荷</strong>。我们在图2中只显示了keys数组。默认情况下，键和有效负载都是固定大小的。(注意，有效负载可以是记录或指向可变大小记录的指针，存储在内存中单独分配的空间中)。出于实际原因，我们还强制设置了最大节点大小。</p><p>ALEX使用间隙数组布局，它使用基于模型的插入来分配数组元素之间的额外空间，从而实现更快的插入和查找。相反，B+Tree将所有的空格放在数组的末尾。</p><p><strong>间隙数组用间隙右边最近的键来填充间隙，这有助于保持指数搜索性能</strong>。</p><p><strong>为了在扫描时有效地跳过间隙，每个数据节点维护一个位图，该位图跟踪节点中的每个位置是否被键占用或是否为间隙。与间隙数组相比，位图查询速度快，空间开销低</strong>。</p><hr><h4 id="内部节点"><a href="#内部节点" class="headerlink" title="内部节点"></a>内部节点</h4><p>我们将作为RMI结构部分的所有节点称为内部节点，如图2中矩形所示。内部节点存储一个线性回归模型和一个包含指向子节点指针的数组。像B+树一样，内部节点直接遍历树，<strong>但与B+树不同的是，ALEX中的内部节点使用模型来“计算”指针数组中下一个子指针的位置</strong>。与数据节点类似，我们施加了一个最大节点大小。</p><p>ALEX 的内部节点在概念上与学习索引的目的不同。 LearnedIndex的内部节点有适合数据的模型；<strong>具有完美模型的内部节点将键平均分配给其子节点，并且具有完美内部节点的 RMI 会导致每个数据节点中的键数量相同</strong>。然而，<strong>RMI结构的目标不是产生相同大小的数据节点，而是产生键分布大致线性的数据节点，以便线性模型可以精确地拟合其键</strong>。</p><p>因此，ALEX中内部节点的作用就是提供一种灵活的方式来划分键空间。假设图3中的内部节点A覆盖了键空间[0,1)并且有四个子指针。 LearnedIndex 将为每个指针分配一个节点，可以是所有内部节点，也可以是所有数据节点。然而，ALEX 更灵活地划分空间。内部节点A将键空间[0,1/4)和[1/2,1)分配给数据节点（因为这些空间中的CDF是线性的），并将[1/4,1/2)分配给另一个内部节点（因为 CDF 是非线性的，并且 RMI 需要对该key空间进行更精细的划分）。<strong>如图所示，多个指针可以指向同一个子节点</strong>。</p><p>将每个内部节点中的指针数量始终限制为 2 的幂。这允许节点在不重新训练其子树的情况下进行分裂。</p><p><img src="/posts/53ff.htm/image-20240412211959414.png" alt="image-20240412211959414"></p><hr><h2 id="算法设计"><a href="#算法设计" class="headerlink" title="算法设计"></a>算法设计</h2><h3 id="查找和范围查找"><a href="#查找和范围查找" class="headerlink" title="查找和范围查找"></a>查找和范围查找</h3><p>为了查找键，从 RMI 的根节点开始，迭代地使用模型来“计算”指针数组中的位置，然后沿着指针指向下一级的子节点，直到到达数据节点。通过构建，内部节点模型具有完美的精度，因此内部节点不涉及搜索。我们使用数据节点中的模型来预测键中搜索键的位置阵列，如果需要，进行指数搜索找到 key 的实际位置。如果找到键，我们从有效负载数组中读取同一位置的相应值并返回记录。否则，返回一个空记录。图2中直观地显示了（使用红色箭头）查找。范围查询首先执行查找以查找值不小于范围起始值的第一个键的位置和数据节点，然后向后扫描直到到达范围的结束值，<strong>使用节点的位图跳过间隙</strong>，并在必要时使用存储在节点中的指针跳转到下一个数据节点。【基于模型的查找O(1)+指数搜索】</p><p><img src="/posts/53ff.htm/image-20240412212047079.png" alt="image-20240412212047079"></p><hr><h3 id="插入未满的数据节点"><a href="#插入未满的数据节点" class="headerlink" title="插入未满的数据节点"></a>插入未满的数据节点</h3><p>对于插入算法，到达正确数据节点（即TraverseToLeaf）的逻辑与上述查找算法相同。在未满数据节点中，为了找到新元素的插入位置，可使用数据节点中的模型来预测插入位置。如果预测位置不正确（插入到那里将无法保持排序顺序），则会进行指数搜索来找到正确的插入位置。</p><p>如果插入位置是一个间隙，则直接将元素插入到间隙中。<strong>否则，便通过将元素在最近间隙的方向上移动一个位置以在插入位置处形成一个间隙</strong>。然后将元素插入到新创建的间隙中。间隙数组以高概率实现 O(logn) 插入时间。</p><hr><h3 id="插入已满的数据节点"><a href="#插入已满的数据节点" class="headerlink" title="插入已满的数据节点"></a>插入已满的数据节点</h3><p>当数据节点已满时，ALEX 使用两种机制来创建更多空间：扩展和拆分。 ALEX 依靠简单的 cost 模型在不同的机制之间进行选择。</p><h4 id="节点饱满度的标准"><a href="#节点饱满度的标准" class="headerlink" title="节点饱满度的标准"></a>节点饱满度的标准</h4><p>ALEX 不会等待数据节点达到 100% 满，因为间隙数组上的插入性能会随着间隙数量的减少而恶化。我们引入间隙数组的密度下限和上限：d<del>l</del>,d<del>u</del> ∈ (0,1]，约束条件是 d<del>l</del>&lt;d<del>u</del>。</p><p>密度定义为被元素填充的位置的分数。如果下一次插入导致超过 d<del>u</del> ，则节点已满。默认情况下，我们设置 d<del>l</del>=0.6 和 d<del>u</del>=0.8，以实现平均数据存储利用率 0.7，类似于 B+Tree [14]，根据我们的实验，这总是会产生良好的结果，并且不需要调整。相反，B+Tree 节点通常具有 d<del>l</del>=0.5 和 d<del>u</del>=1。</p><h4 id="节点扩展机制"><a href="#节点扩展机制" class="headerlink" title="节点扩展机制"></a>节点扩展机制</h4><p>为了扩展包含 n 个键的数据节点，我们分配一个具有 n/d<del>l</del> 个槽的新的更大的间隙数组。然后，我们缩放或重新训练线性回归模型，然后使用缩放或重新训练的模型对这个新的较大节点中的所有元素进行基于模型的插入。创建后，新数据节点处于密度下限 d<del>l</del> 。</p><h4 id="节点分裂机制"><a href="#节点分裂机制" class="headerlink" title="节点分裂机制"></a>节点分裂机制</h4><p>为了将数据节点一分为二，即将 key 分配给两个新的数据节点，使得每个新节点负责原始节点的 key 空间的一半。 ALEX支持两种分割节点的方式：</p><ol><li><p><strong>横向拆分</strong>在概念上类似于 B+Tree 使用拆分的方式。有两种情况：</p><p>a）如果分裂数据节点的父内部节点尚未达到最大节点大小，则将父节点指向分裂数据节点的指针替换为指向两个新数据节点的指针。父内部节点的指针数组可能有指向拆分数据节点的冗余指针。如果是这样，我们将一半的冗余指针分配给两个新节点中的每一个。否则，我们通过将父节点指针数组的大小加倍并为每个指针制作冗余副本来创建指向拆分数据节点的第二个指针，然后将冗余指针之一赋予两个新节点中的每一个。图 5a 显示了不需要扩展父内部节点的横向拆分的示例。 </p><p>b) 如果父内部节点已达到最大节点大小，则可以选择拆分父内部节点，如图 5b 所示。请注意，通过将所有内部节点大小限制为 2 的幂，便始终以“边界保留”方式分割节点，因此不需要重新训练分割内部节点下方的任何模型。请注意，分裂可以一直传播到根节点，就像在 B+ 树中一样。</p></li></ol><p><img src="/posts/53ff.htm/image-20240412212101573.png" alt="image-20240412212101573"></p><ol start="2"><li>纵向拆分将数据节点转换为具有两个子数据节点的内部节点，如图 5c 所示。<strong>两个子数据节点中的模型根据各自的键进行训练</strong>。 B+Tree 没有类似的分裂机制。</li></ol><p><img src="/posts/53ff.htm/image-20240412212111535.png" alt="image-20240412212111535"></p><hr><h4 id="cost-模型"><a href="#cost-模型" class="headerlink" title="cost 模型"></a>cost 模型</h4><p>为了决定应用哪种机制，ALEX 依赖于简单的线性 cost 模型，该模型根据在每个数据节点跟踪出来的两个简单统计数据 来预测平均查找时间和插入时间：（a）指数搜索迭代的平均数; （b) 插入的平均移位次数。查找性能与（a）直接相关，而插入性能与（a）和（b）直接相关（因为插入首先需要进行查找以找到正确的插入位置）。</p><p><strong>这两个统计数据在创建数据节点时是未知的。为了找到新数据节点的预期 cost ，我们在假设对现有键统一进行查找并根据现有键分布进行插入的情况下计算这些统计数据的预期值</strong>。</p><p>具体来说：</p><p>(a) 被计算为所有键的模型预测误差的平均以 2 为底的对数； </p><p>(b) 计算所有现有键到间隙数组中最近间隙的平均距离。</p><p>这些期望值可以在不创建数据节点的情况下计算。如果数据节点是使用现有数据节点的键子集创建的，我们可以使用查找与插入的经验比率来衡量两个统计数据的相对重要性以进行计算预期 cost 。</p><p>除了节点内 cost 模型之外，ALEX 还使用 TraverseToLeaf cost 模型来预测从根节点遍历到数据节点的时间。 TraverseToLeaf cost 模型使用两个统计数据：</p><ol><li>遍历到的数据节点的深度；</li><li>所有内部节点和数据节点元数据（除了key和有效负载之外的所有内容）的总大小（以B为单位）。</li></ol><p>这些统计数据捕获了遍历的 cost ：较深的数据节点需要更多的指针追踪来查找，较大的大小会降低 CPU 缓存局部性，从而减慢对数据节点的遍历。我们提供有关 cost 模型的更多详细信息并在 [10] 的附录 D 中显示了它们的低使用开销。【<strong>计算叶节点深度，以及遍历到叶节点所需公共元数据大小</strong>】</p><hr><h4 id="插入算法"><a href="#插入算法" class="headerlink" title="插入算法"></a>插入算法</h4><p>当查找和插入在数据节点上完成时，就计算指数搜索迭代和每次插入的移位 次数。<strong>根据这些统计数据，我们使用节点内 cost 模型计算数据节点的经验 cost **。</strong>一旦数据节点已满，就将预期 cost （在节点创建时计算）与经验 cost 进行比较。如果它们没有显著偏离，那么就断定模型仍然准确，就执行节点扩展（如果扩展后的大小小于最大节点大小），缩放模型而不是重新训练**。 RMI 内部节点中的模型不会重新训练或重新缩放。</p><p><strong>显著 cost 偏差定义为：经验 cost 比预期 cost 高出 50% 以上</strong>。根据我们的实验，50% 的 cost 偏差阈值始终会产生良好的结果，无需进行调整。</p><p>否则，<strong>如果经验 cost 偏离了预期 cost ，就必须（i）扩展数据节点并重新训练模型，（ii）横向分割数据节点，或（iii）纵向分割数据节点。根据节点内 cost 模型，选择导致最低预期 cost 的操作</strong>。</p><p><strong>为简单起见，ALEX 始终将数据节点一分为二。数据节点在概念上可以拆分为 2 的任意幂，但确定最佳扇出可能非常耗时，我们通过实验验证，在大多数情况下，根据 cost 模型，2 的扇出是最佳的</strong>。</p><hr><h4 id="为什么经验-cost-会偏离预期-cost-？"><a href="#为什么经验-cost-会偏离预期-cost-？" class="headerlink" title="为什么经验 cost 会偏离预期 cost ？"></a>为什么经验 cost 会偏离预期 cost ？</h4><p>当 插入键 的分布不遵循现有键的分布时，通常会发生这种情况，从而导致模型变得不准确。不准确的模型可能会导致没有任何间隙的长连续区域。插入这些完全填充的区域需要移动其中最多一半的元素以创建间隙，在最坏的情况下这需要 O(n) 时间。随着节点变大，或由于查找的访问模式发生变化，性能也可能仅仅由于随机噪声而降低。</p><hr><h3 id="删除、更新"><a href="#删除、更新" class="headerlink" title="删除、更新"></a>删除、更新</h3><p>要删除key，则进行查找以找到key的位置，然后删除它及其有效负载。删除不会移动任何现有键，因此删除是比插入更简单的操作，并且不会导致模型准确性降低。如果数据节点由于删除而达到密度下限 d<del>l</del>，则 <strong>收缩</strong> 该数据节点以避免空间利用率低。此外，使用节点内 cost 模型来确定两个数据节点应该合并在一起并可能向上增长，从而局部将 RMI 深度减少 1。但是，为了简单起见，我们不实现这些合并操作。</p><p>【<strong>如何收缩？简单起见，不合并，要合并怎么办？效果会好吗</strong>？】</p><p>更新：修改键是通过组合插入和删除来实现的。仅修改有效负载的更新将查找 key 并将新值写入有效负载。</p><hr><h3 id="处理越界插入"><a href="#处理越界插入" class="headerlink" title="处理越界插入"></a>处理越界插入</h3><p>低于或高于现有键空间的键将分别插入到最左边或最右边的数据节点中。一系列越界插入（如仅追加插入工作负载）将导致性能不佳，因为该数据节点没有分裂越界键空间的机制。因此，ALEX有两种方式可以顺利处理越界插入。</p><p>首先，当检测到现有键空间之外的插入时，ALEX 将扩展根节点，从而扩展键空间，如图 6 所示。可将子指针数组的大小向右扩展。指向现有子项的现有指针不会被修改。为扩展的指针数组中的每个新槽创建一个新的数据节点。若此扩展导致根节点超过最大节点大小，ALEX 将创建一个新的根节点。新根节点的第一个子指针将指向旧根节点，并为新根节点的其他每个指针槽创建一个新数据节点。最后，越界键将落入新创建的数据节点之一。</p><p><img src="/posts/53ff.htm/image-20240412212122272.png" alt="image-20240412212122272"></p><p>其次，ALEX 最右边的数据节点通过 维护节点中最大键的值 并 保留插入超过最大值的计数器 来检测追加插入行为。如果大多数【<strong>是根据计数器大小判断</strong>？】插入超过最大值，则意味着仅追加行为，因此数据节点向右扩展，而不进行基于模型的重新插入；扩展的空间最初保持为空，以期待更多类似附加的插入。</p><hr><h3 id="批量加载"><a href="#批量加载" class="headerlink" title="批量加载"></a>批量加载</h3><p>ALEX 支持批量加载操作，该操作在实践中用于在初始化时索引大量数据或重建索引。我们的目标是找到一个 cost 最小的 RMI 结构，cost 定义为在此 RMI 上执行操作（即查找或插入）的预期平均时间。任何 ALEX 操作都是由到数据节点的 TraverseToLeaf 和节点内操作组成，因此 RMI cost 是通过结合 TraverseToLeaf 和节点内 cost 模型来建模的。</p><h4 id="批量加载算法"><a href="#批量加载算法" class="headerlink" title="批量加载算法"></a>批量加载算法</h4><p>使用 cost 模型，从根节点开始贪婪地向下增长 RMI。在每个节点，独立地决定该节点应该是数据节点还是内部节点，以及在后一种情况下，扇出应该是什么。<strong>扇出必须是2的幂，子节点将平分当前节点的key空间</strong>。</p><p>在每个节点本地就可做出此决策，因为使用线性 cost 模型，决策将对 RMI 的总体 cost 产生纯粹的累加效应。如果该节点是内部节点，则将在其每个子节点上递归。这将持续下去，直到所有数据都加载到 ALEX 中。</p><p><img src="/posts/53ff.htm/image-20240412212129080.png" alt="image-20240412212129080"></p><h4 id="扇出树"><a href="#扇出树" class="headerlink" title="扇出树"></a>扇出树</h4><p>随着 RMI 的发展，主要挑战是确定每个节点的最佳扇出。</p><p>引入扇出树（FT）概念，它是一棵完全二叉树。 FT 将帮助决定单个 RMI 节点的扇出；在批量加载算法中，每次决定 RMI 节点的最佳扇出时，都构造一个 FT 树。<strong>扇出为 1 意味着 RMI 节点应该是数据节点</strong>。</p><p>图 7 显示了 FT 示例。每个 FT 节点代表 RMI 节点的一个可能的子节点。如果 RMI 节点的 key 空间为[0,1)，则在某个具有 n 个节点的层级上，第 i 个 FT 节点表示 key 空间为 [i/n,(i+1)/n) 的 子RMI节点。每个 FT 节点都与在其 key 空间上构建数据节点的预期 cost 相关联，如节点内 cost 模型所预测的那样。目标是以最小的总体 cost 找到一组覆盖 RMI 节点整个 key 空间的 FT 节点。</p><p>覆盖集的总体 cost 是其 FT 节点 cost 以及由于模型大小而导致的 TraverseToLeaf cost 之和（如在 FT 中深入一层意味着 RMI 节点必须具有两倍的指针）。该覆盖集决定了 RMI 节点的最佳扇出（即子指针的数量）以及分配子指针的最佳方式。</p><p>使用以下方法来找到低 cost 的覆盖集：</p><ol><li>从FT根开始，一次增长FT的整个级别，并计算使用每个级别作为覆盖集的 cost 。继续这样做，直到每个连续级别的 cost 开始增加。在图 7 中，级别 2 的综合 cost 最低，则不再继续增长级别 3 之后的。从概念上讲，更深的级别可能具有较低的 cost ，但计算每个 FT 节点的 cost 是昂贵的。 </li></ol><ol start="2"><li>从组合 cost 最低的 FT 级别开始，开始本地合并或分裂 FT 节点。如果两个相邻 FT 节点的 cost 高于其父节点的 cost ，则合并（例如， cost 为 20 和 25 的节点合并为 cost 为 40 的节点）；当两个节点的键很少或它们的分布相似时，可能会发生这种情况。另一方面，如果 FT 节点的 cost 高于其两个子节点的 cost ，则分割 FT 节点（例如， cost 为 10 的节点被分割为两个 cost 为 1 的节点）；当 key 空间的两半具有不同的分布时，可能会发生这种情况。继续在本地合并和分裂相邻节点的过程，直到不再可能为止。</li></ol><p>最后，返回生成的 FT 节点覆盖集。</p><p>【<strong>大大的问题：经验 cost 和预期 cost 是怎么计算的，FT树的每个层级中的 cost 是怎么计算的？怎么和RMI对应的？该问题中，层级2 cost 最小，完事怎么又合并分裂了</strong>？】</p><hr><h2 id="ALEX分析"><a href="#ALEX分析" class="headerlink" title="ALEX分析"></a>ALEX分析</h2><h3 id="RMI-深度的限制"><a href="#RMI-深度的限制" class="headerlink" title="RMI 深度的限制"></a>RMI 深度的限制</h3><p>让 m 为最大节点大小，以槽数定义（在内部节点的指针数组中，在数据节点的间隙数组中）。将节点大小限制为 2 的幂： m = 2^k^ 。内部节点最多可以有 m 多个子指针，数据节点包含的键不得超过 md<del>u</del> 个。让所有要索引的键都落在键空间 s 内。令 p 为分区的最小数量，使得当 key 空间 s 被划分为 p 个宽度相等的分区时，每个分区包含的 key 不超过 md<del>u</del> 个。定义根节点深度为0。</p><p>定理5.1：可以构造一个满足最大节点大小和密度上限约束的 RMI，其深度不大于 ⌈log<del>m</del>p⌉——称之为最大深度。此外，可以保持插入下的最大深度。（请注意，p 在插入下可能会发生变化）</p><p>换句话说，RMI 的深度受到 <strong>s 最密集子区域的密度的限制</strong>。相比之下，B+树将深度限制为键数量的函数。定理 5.1 也可以应用于 s 内的子空间，它对应于 RMI 内的某个子树。</p><p>证明。构建具有最大深度的 RMI 非常简单。跨越大小为 |s|/p 的键空间的<strong>最密集子区域</strong>【<strong>如何甄别</strong>？】被分配给数据节点。从根到这个最密集区域的遍历路径由内部节点组成，每个内部节点有 m 个子指针。它需要 ⌈log<del>m</del>p⌉ 内部节点来缩小 |s| 的 key 空间大小至 |s|/p。为了最小化 RMI 的其他子树的深度，我们将这种构造机制递归地应用于空间 s 的其余部分。</p><p>从满足最大深度的 RMI 开始，我们使用 4.3 节中的机制根据以下策略维持最大深度： (1) 数据节点扩展直到达到最大节点大小。 (2) 当数据节点由于最大节点大小而必须分裂时，它会横向分裂以维持当前深度（可能将分裂传播到某个祖先内部节点）。(3) 当不再可能横向分裂时（所有祖先节点都处于最大节点大小），向下分裂。</p><p><strong>通过遵循此策略，RMI 仅当 p 增长 m 倍时才向下分裂，从而保持最大深度</strong>。</p><h3 id="复杂性分析"><a href="#复杂性分析" class="headerlink" title="复杂性分析"></a>复杂性分析</h3><p>查找和插入都在 ⌈log<del>m</del> p⌉ 时间内完成 TraverseToLeaf。在数据节点内，查找的指数搜索在最坏情况下受 O(logm) 限制。在最好的情况下，数据节点模型完美地预测了 key 的位置，并且查找需要 O(1) 时间。</p><p>未满节点的插入由查找组成，后面可能会进行移位以引入新键的间隙。在最坏的情况下，这受到 O(m) 的限制，但由于间隙数组以高概率实现每次插入 O(logm) 次移位，因此预计大多数情况下的复杂性为 O(logm)。在最好的情况下，预测的插入位置是正确的并且是一个间隙，则将 key 准确地放置在模型预测的位置，插入复杂度为 O(1) ；此外，稍后基于模型的查找将导致直接命中 O(1)。</p><p>cost 由必须复制的元素数量来定义： (1)数据节点的扩展，其 cost 以O(m)为界。 (2)向下分裂成两个节点，其 cost 为O(m)。 (3) 横向分裂成两个节点，并在路径中向上传播到某个祖先节点，其 cost 受 O(m⌈log<del>m</del>p⌉) 限制，因为该路径上的每个内部节点也必须分裂。因此，插入全节点的最坏情况性能为 O(m⌈log<del>m</del>p⌉)。</p><hr><h2 id="实验评估"><a href="#实验评估" class="headerlink" title="实验评估"></a>实验评估</h2><p>使用各种数据集和工作负载将 ALEX 与学习索引、B+树、模型增强的 B+树 和 自适应基数树 (ART) 进行比较。此次评估表明：</p><ul><li><p>在只读工作负载上，ALEX 的吞吐量比 B+Tree、学习索引、模型B+树和ART 高出 4.1 倍、2.2 倍、2.9 倍、3.0 倍；索引大小小 800 倍、15 倍、160 倍、8000 倍。</p></li><li><p>在读写工作负载上，ALEX 的吞吐量分别比 B+Tree、模型 B+Tree 和 ART 高 4.0×、2.7×、2.7×，索引大小分别小 2000×、475×、36000×。</p></li><li><p>ALEX 具有竞争力的批量加载时间，并且在扩展到更大的数据集以及由于数据倾斜而导致分布变化时，保持优于其他索引的优势。</p></li><li><p>Gapped Array 和自适应 RMI 结构使 ALEX 能够适应不同的数据集和工作负载。</p></li></ul><hr><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><p>使用来自某些数据集的 8 字节密钥和随机生成的固定大小的有效负载来运行所有实验。我们在 4 个数据集上评估 ALEX，其特征和 CDF 如表 1 和图 8 所示。经度数据集由 Open Street Maps [2] 中世界各地位置的经度组成。 longlat 数据集由复合键组成，这些复合键通过对每对经度和纬度应用变换 k =180·floor(经度)+纬度来组合来自开放街道地图的经度和纬度。由此产生的 keys k 分布是高度非线性的。对数正态数据集的值根据对数正态分布生成，其中 µ =0 且 σ =2，乘以 10^9^ 并向下舍入到最接近的整数。 YCSB 数据集具有表示根据 YCSB 基准 [8] 生成的用户 ID 的值，这些值均匀分布在整个 64 位域中，并使用 80 字节的有效负载。这些数据集不包含重复值。除非另有说明，否则这些数据集会随机打乱以模拟随时间变化的均匀数据集分布.</p><h4 id="工作负载"><a href="#工作负载" class="headerlink" title="工作负载"></a>工作负载</h4><p>我们评估 ALEX 的主要指标是平均吞吐量。我们评估五种工作负载的吞吐量：(1) 只读工作负载，(2) 包含 95% 读取和 5% 插入的读密集型工作负载，(3) 包含 50% 读取和 50% 插入的写密集型工作负载， (4) 具有 95% 读取和 5% 插入的短程查询工作负载，以及 (5) 只写工作负载，以完成读写范围。对于前三个工作负载，读取由单个键的查找组成。对于短程工作负载，读取由键查找和后续键扫描组成。要扫描的键的数量是从最大扫描长度为 100 的均匀分布中随机选择的。对于所有工作负载，要查找的键是根据 Zipfian 分布从索引中现有键的集合中随机选择的。前四个工作负载大致分别对应于 YCSB 基准 [8] 中的工作负载 C、B、A 和 E。对于给定的数据集，我们用 1 亿个键初始化一个索引。然后，我们运行工作负载 60 秒，插入剩余的密钥。我们报告当时完成的操作的吞吐量，其中操作要么是插入，要么是读取。对于读写工作负载，我们交错操作：对于读密集型工作负载和短范围工作负载，我们执行 19 次读取/扫描，然后执行 1 次插入，然后重复该循环；<strong>对于写入密集型工作负载，我们执行 1 次读取，然后执行 1 次插入，然后重复该循环</strong>。</p><hr><h3 id="Drilldown-into-ALEX-Design-Trade-offs"><a href="#Drilldown-into-ALEX-Design-Trade-offs" class="headerlink" title="Drilldown into ALEX Design Trade-offs"></a>Drilldown into ALEX Design Trade-offs</h3><p>在本节中，我们将深入探讨节点布局和自适应 RMI 如何帮助 ALEX 实现其设计目标。</p><p>ALEX 相对于学习索引的部分优势来自于在数据节点中使用基于模型的插入和间隙数组，但 ALEX 对于动态工作负载的大部分优势来自于<strong>自适应 RMI</strong>。</p><p>为了演示每个贡献的效果，图 13 显示，采用 2 层学习索引并用每个叶的间隙数组（LI w/Gapped Array）替换单个密集值数组，对于只读工作负载，已经比学习索引实现了显着的加速。然而，带有间隙数组的学习索引在读写工作负载上的性能较差，因为存在完全填充的区域，需要为每次插入移动许多键。 ALEX 使 RMI 结构适应数据的能力对于良好的插入性能是必要的。</p><p>在查找过程中，大部分时间都花在围绕预测位置进行本地搜索。较小的预测误差直接有助于减少查找时间。为了分析 Learned Index 和 ALEX 的预测误差，我们用经度数据集中的 1 亿个键初始化一个索引，使用该索引来预测这 1 亿个键中每个键的位置，并跟踪预测位置与实际位置之间的距离。图 14a 显示学习索引在模式大约 8-32 个位置处存在预测误差，并且右侧有长尾。另一方面，ALEX 通过使用基于模型的插入实现了低得多的预测误差。</p><p>图14b显示，初始化后，ALEX通常没有预测误差，发生的误差通常很小，并且误差的长尾已经消失。图 14c 显示，即使在 2000 万次插入之后，ALEX 仍保持较低的预测误差。</p><p>一旦数据节点变满，就会发生以下四种操作之一：如果不存在 cost 偏差，则 (1) 扩展节点并缩放模型。否则，节点要么 (2) 扩展并重新训练其模型，(3) 横向分割，或 (4) 向下分割。表 3 显示，在绝大多数情况下，数据节点只是扩展，模型也进行了缩放，这意味着即使在插入之后模型通常仍然保持准确（假设没有发生根本的分布变化）。数据节点变满的次数与数据节点的数量相关（表2）。在YCSB上，通过模型再训练进行扩展更为常见，因为数据节点很大，因此 cost 偏差通常只是由随机性引起的。</p><p>如果需要，用户可以调整最大节点大小以实现目标尾部延迟。在图 15 中，我们在经度数据集上运行写入密集型工作负载，测量每个操作的延迟。随着我们增加最大节点大小，ALEX 的中值甚至 p99 延迟都会减少，因为 ALEX 具有更大的灵活性来构建性能更好的 RMI（例如，具有更高的内部节点扇出的能力）。然而，最大延迟会增加，因为触发大节点扩展或分裂的插入速度很慢。如果用户对延迟有严格要求，则可以相应减小最大节点大小。将最大节点大小增加到超过 64MB 后，延迟不会改变，因为 ALEX 从未决定使用大于 64MB 的节点。</p><hr><h4 id="搜索方法比较"><a href="#搜索方法比较" class="headerlink" title="搜索方法比较"></a>搜索方法比较</h4><p>为了展示指数搜索和其他搜索方法之间的权衡，我们对合成数据进行了微基准测试。我们创建一个包含 1 亿个完全均匀分布的双精度数的数据集。然后，我们从该数据集中搜索 1000 万个随机选择的值。我们使用三种搜索方法：二分搜索和有偏四元搜索（在[20]中提出，以利用准确的预测），每种方法都使用两种不同的误差范围大小进行评估，以及指数搜索。对于每次查找，搜索方法都会得到一个预测位置，该位置与实际位置值的距离存在一定的综合误差量。图 16 显示，指数搜索的搜索时间与错误大小的对数成比例增加，而二分搜索方法花费恒定的时间，而与错误大小无关。这是因为二分查找必须始终在其误差范围内开始搜索，并且无法利用误差较小的情况。因此，如果 ALEX 中 RMI 模型的预测误差较小，指数搜索应该优于二分搜索。正如我们在第 6.3 节中所示，ALEX 通过基于模型的插入保持较低的预测误差。</p><p>因此，ALEX 非常适合利用指数搜索。当误差低于 σ 时（我们在本实验中设置 σ = 8；详细信息请参阅[20]），有向四元搜索与指数搜索具有竞争力，因为搜索可以限制在较小的范围内，但当误差超过 σ 时，其性能与二分搜索类似因为必须搜索完整的错误界限。与有偏差的四元搜索相比，我们更喜欢指数搜索，因为它的性能下降更平滑且实现简单（例如，无需调整 σ）。</p><p>参考博文：<a href="https://zhuanlan.zhihu.com/p/435878936">指数搜索 - 知乎 (zhihu.com)</a> </p><hr><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>我们在学习索引令人兴奋的基础上提出了 ALEX，这是一种新的可更新学习索引，它有效地将学习索引的核心见解与经过验证的存储和索引技术结合起来。具体来说，我们提出了一种间隙数组节点布局，它使用基于模型的插入和指数搜索，结合简单 cost 模型驱动的自适应 RMI 结构，以在动态工作负载上实现高性能和低内存占用。我们深入的实验结果表明，ALEX 不仅在读写工作负载范围内始终优于 B+Tree，甚至在所有数据集上，对于只读工作负载，其性能比现有的学习索引高出 2.2 倍。</p><p>我们相信这篇论文为我们的社区提供了重要的经验教训，并为该领域的未来研究开辟了道路。</p><p>我们打算研究有关 ALEX 性能的开放理论问题，<strong>支持大于内存数据集的二级存储，以及针对 ALEX 设计量身定制的新并发控制技术</strong>。</p><p>【<strong>方向：二级学习型索引，并发控制的学习型索引</strong>】</p><hr><h2 id="代码结构"><a href="#代码结构" class="headerlink" title="代码结构"></a>代码结构</h2><h3 id="类及属性"><a href="#类及属性" class="headerlink" title="类及属性"></a>类及属性</h3><h4 id="alex-base-h"><a href="#alex-base-h" class="headerlink" title="alex_base.h"></a>alex_base.h</h4><h5 id="线性回归模型"><a href="#线性回归模型" class="headerlink" title="线性回归模型"></a>线性回归模型</h5><pre class=" language-c++"><code class="language-c++">template<class T>class LinearModel &#123;public:    double a_ = 0;  // 斜率    double b_ = 0;  // 截距    LinearModel() = default;    LinearModel(double a, double b) : a_(a), b_(b) &#123;&#125;    // 拷贝构造函数    explicit LinearModel(const LinearModel &other) : a_(other.a_), b_(other.b_) &#123;&#125;    // 根据给定的扩展因子（expansion_factor）来扩展线性模型的斜率和截距    void expand(double expansion_factor) &#123;        a_ *= expansion_factor;        b_ *= expansion_factor;    &#125;    // 预测 key 的 position_int    inline int predict(T key) const &#123;        return static_cast<int>(a_ * static_cast<double>(key) + b_);    &#125;    // 预测 key 的 position_double    inline double predict_double(T key) const &#123;        return a_ * static_cast<double>(key) + b_;    &#125;&#125;;</code></pre><h5 id="构造线性回归"><a href="#构造线性回归" class="headerlink" title="构造线性回归"></a>构造线性回归</h5><pre class=" language-c++"><code class="language-c++">template<class T>class LinearModelBuilder &#123;public:    // 指向LinearModel<T>对象的指针，用于存储构建的线性模型    LinearModel <T> *model_;    explicit LinearModelBuilder<T>(LinearModel <T> *model) : model_(model) &#123;&#125;    // 该函数会更新相关的统计信息，包括计数、x和y的总和、x的平方总和、x和y的乘积总和以及x和y的最小值和最大值    inline void add(T x, int y) &#123;        count_++;        x_sum_ += static_cast<long double>(x);y_sum_ += static_cast<long double>(y);        xx_sum_ += static_cast<long double>(x) * x;xy_sum_ += static_cast<long double>(x) * y;        x_min_ = std::min<T>(x, x_min_);x_max_ = std::max<T>(x, x_max_);        y_min_ = std::min<double>(y, y_min_);y_max_ = std::max<double>(y, y_max_);    &#125;private:    // 整型变量，表示已添加的数据点的数量    int count_ = 0;    // 长双精度浮点型变量，表示所有数据点的x值的总和    long double x_sum_ = 0;    // 长双精度浮点型变量，表示所有数据点的y值的总和    long double y_sum_ = 0;    // 长双精度浮点型变量，表示所有数据点的x值的平方总和    long double xx_sum_ = 0;    // 长双精度浮点型变量，表示所有数据点的x值和y值的乘积总和    long double xy_sum_ = 0;        T x_min_ = std::numeric_limits<T>::max();    T x_max_ = std::numeric_limits<T>::lowest();    double y_min_ = std::numeric_limits<double>::max();    double y_max_ = std::numeric_limits<double>::lowest();&#125;;</code></pre><p><strong>build 方法构建模型</strong>：</p><pre class=" language-c++"><code class="language-c++">void build() &#123;    if (count_ <= 1) &#123;        model_->a_ = 0;        model_->b_ = static_cast<double>(y_sum_);        return;    &#125;    if (static_cast<long double>(count_) * xx_sum_ - x_sum_ * x_sum_ == 0) &#123;        // all values in a bucket have the same key.        model_->a_ = 0;        model_->b_ = static_cast<double>(y_sum_) / count_;        return;    &#125;    auto slope = static_cast<double>(        (static_cast<long double>(count_) * xy_sum_ - x_sum_ * y_sum_) /        (static_cast<long double>(count_) * xx_sum_ - x_sum_ * x_sum_));        auto intercept = static_cast<double>(        (y_sum_ - static_cast<long double>(slope) * x_sum_) / count_);    model_->a_ = slope;    model_->b_ = intercept;    // If floating point precision errors, fit spline    if (model_->a_ <= 0) &#123;        if (x_max_ - x_min_ == 0) &#123;            model_->a_ = 0;            model_->b_ = static_cast<double>(y_sum_) / count_;        &#125; else &#123;            model_->a_ = (y_max_ - y_min_) / (x_max_ - x_min_);            model_->b_ = -static_cast<double>(x_min_) * model_->a_;        &#125;    &#125;&#125;</code></pre><hr><h5 id="工具方法"><a href="#工具方法" class="headerlink" title="工具方法"></a>工具方法</h5><pre class=" language-c++"><code class="language-c++">/*** Helper methods for bitmap ***/// Extract the rightmost 1 in the binary representation.// e.g. extract_rightmost_one(010100100) = 000000100inline uint64_t extract_rightmost_one(uint64_t value) &#123;    return value & -static_cast<int64_t>(value);&#125;// Remove the rightmost 1 in the binary representation.// e.g. remove_rightmost_one(010100100) = 010100000inline uint64_t remove_rightmost_one(uint64_t value) &#123;    return value & (value - 1);&#125;// Count the number of 1s in the binary representation.// e.g. count_ones(010100100) = 3inline int count_ones(uint64_t value) &#123;    return static_cast<int>(_mm_popcnt_u64(value));&#125;// Get the offset of a bit in a bitmap.// word_id is the word id of the bit in a bitmap// bit is the word that contains the bitinline int get_offset(int word_id, uint64_t bit) &#123;    // 返回该位在位图中的偏移量    return (word_id << 6) + count_ones(bit - 1);&#125;// https://stackoverflow.com/questions/364985/algorithm-for-finding-the-smallest-power-of-two-thats-greater-or-equal-to-a-giv// 返回大于或等于x的最小2的幂次方数inline int pow_2_round_up(int x) &#123;    --x;    x |= x >> 1;    x |= x >> 2;    x |= x >> 4;    x |= x >> 8;    x |= x >> 16;    return x + 1;&#125;// https://stackoverflow.com/questions/994593/how-to-do-an-integer-log2-in-c// 返回小于或等于x的最大2的幂次方数的指数inline int log_2_round_down(int x) &#123;    int res = 0;    while (x >>= 1) ++res;    return res;&#125;</code></pre><h5 id="统计方法"><a href="#统计方法" class="headerlink" title="统计方法"></a>统计方法</h5><pre class=" language-c++"><code class="language-c++">/*** Cost model weights ***/// Intra-node cost weightsconstexpr double kExpSearchIterationsWeight = 20;constexpr double kShiftsWeight = 0.5;// TraverseToLeaf cost weightsconstexpr double kNodeLookupsWeight = 20;constexpr double kModelSizeWeight = 5e-7;/*** Stat Accumulators ***/// 统计数据节点的搜索迭代次数和位移数量struct DataNodeStats &#123;    double num_search_iterations = 0;    double num_shifts = 0;&#125;;// Used when stats are computed using a sample// 用于在计算统计数据时使用样本struct SampleDataNodeStats &#123;    // 样本大小的对数    double log2_sample_size = 0;    // 搜索迭代次数    double num_search_iterations = 0;    // 位移数量的对数    double log2_num_shifts = 0;&#125;;// Accumulates stats that are used in the cost model, based on the actual vs// predicted position of a keyclass StatAccumulator &#123;    public:    virtual ~StatAccumulator() = default;    // 用于根据实际位置和预测位置来累积统计信息    virtual void accumulate(int actual_position, int predicted_position) = 0;    // 用于获取当前的统计信息    virtual double get_stat() = 0;    // 重置统计信息    virtual void reset() = 0;&#125;;</code></pre><p><strong>计算预期的指数搜索迭代次数</strong>：</p><pre class=" language-c++"><code class="language-c++">// Mean log error represents the expected number of exponential search iterations when doing a lookupclass ExpectedSearchIterationsAccumulator : public StatAccumulator &#123;    public:    // 计算预期的指数搜索迭代次数    void accumulate(int actual_position, int predicted_position) override &#123;        // 计算实际位置和预测位置之间的差的绝对值加1的对数        cumulative_log_error_ +=            std::log2(std::abs(predicted_position - actual_position) + 1);        count_++;    &#125;    double get_stat() override &#123;        if (count_ == 0) return 0;        // 返回当前的平均对数误差        return cumulative_log_error_ / count_;    &#125;    void reset() override &#123;        cumulative_log_error_ = 0;        count_ = 0;    &#125;    public:    double cumulative_log_error_ = 0;    int count_ = 0;&#125;;</code></pre><p><strong>计算预期的插入操作中的移位次数</strong>：</p><pre class=" language-c++"><code class="language-c++">// Mean shifts represents the expected number of shifts when doing an insertclass ExpectedShiftsAccumulator : public StatAccumulator &#123;public:    explicit ExpectedShiftsAccumulator(int data_capacity)        : data_capacity_(data_capacity) &#123;&#125;    // A dense region of n keys will contribute a total number of expected shifts    // of approximately    // ((n-1)/2)((n-1)/2 + 1) = n^2/4 - 1/4    // This is exact for odd n and off by 0.25 for even n.    // Therefore, we track n^2/4.    void accumulate(int actual_position, int) override &#123;        if (actual_position > last_position_ + 1) &#123;            long long dense_region_length = last_position_ - dense_region_start_idx_ + 1;            num_expected_shifts_ += (dense_region_length * dense_region_length) / 4;            dense_region_start_idx_ = actual_position;        &#125;        last_position_ = actual_position;        count_++;    &#125;    double get_stat() override &#123;        if (count_ == 0) return 0;        // first need to accumulate statistics for current packed region        long long dense_region_length = last_position_ - dense_region_start_idx_ + 1;        long long cur_num_expected_shifts =            num_expected_shifts_ + (dense_region_length * dense_region_length) / 4;        return cur_num_expected_shifts / static_cast<double>(count_);    &#125;    void reset() override &#123;        last_position_ = -1;        dense_region_start_idx_ = 0;        num_expected_shifts_ = 0;        count_ = 0;    &#125;public:    int last_position_ = -1;            // 最后一个位置    int dense_region_start_idx_ = 0;    // 密集区域的起始索引    long long num_expected_shifts_ = 0; // 预期的移位次数    int count_ = 0;    int data_capacity_ = -1;            // 节点的容量&#125;;</code></pre><hr><h4 id="alex-node-h"><a href="#alex-node-h" class="headerlink" title="alex_node.h"></a>alex_node.h</h4><p><strong>宏定义，控制编译条件</strong>：</p><pre class=" language-c++"><code class="language-c++">// Whether we store key and payload arrays separately in data nodes// By default, we store them separately#define ALEX_DATA_NODE_SEP_ARRAYS 1#if ALEX_DATA_NODE_SEP_ARRAYS#define ALEX_DATA_NODE_KEY_AT(i) key_slots_[i]#define ALEX_DATA_NODE_PAYLOAD_AT(i) payload_slots_[i]#else#define ALEX_DATA_NODE_KEY_AT(i) data_slots_[i].first#define ALEX_DATA_NODE_PAYLOAD_AT(i) data_slots_[i].second#endif// Whether we use lzcnt and tzcnt when manipulating a bitmap (e.g., when finding the closest gap).// If your hardware does not support lzcnt/tzcnt (e.g., your Intel CPU is pre-Haswell), set this to 0.#define ALEX_USE_LZCNT 1</code></pre><h5 id="AlexNode"><a href="#AlexNode" class="headerlink" title="AlexNode"></a>AlexNode</h5><pre class=" language-c++"><code class="language-c++">// A parent class for both types of ALEX nodestemplate<class T, class P>class AlexNode &#123;public:    // Whether this node is a leaf (data) node    bool is_leaf_ = false;    // Power of 2 to which the pointer to this node is duplicated in its parent model node    // For example, if duplication_factor_ is 3, then there are 8 redundant    // pointers to this node in its parent    // 指向这个节点的指针在其父模型节点中被复制的次数。    // 例如，如果duplication_factor_为3，那么在其父节点中有8个指向这个节点的冗余指针    uint8_t duplication_factor_ = 0;    // Node's level in the RMI. Root node is level 0    short level_ = 0;    // Both model nodes and data nodes nodes use models    LinearModel<T> model_;    // Could be either the expected or empirical cost, depending on how this field is used    double cost_ = 0.0;    AlexNode() = default;    explicit AlexNode(short level) : level_(level) &#123;&#125;    AlexNode(short level, bool is_leaf) : is_leaf_(is_leaf), level_(level) &#123;&#125;    virtual ~AlexNode() = default;    // The size in bytes of all member variables in this class    virtual long long node_size() const = 0;&#125;;</code></pre><h5 id="AlexModelNode"><a href="#AlexModelNode" class="headerlink" title="AlexModelNode"></a>AlexModelNode</h5><pre class=" language-c++"><code class="language-c++">template<class T, class P, class Alloc = std::allocator<std::pair<T, P>>>class AlexModelNode : public AlexNode<T, P> &#123;public:    // 类型别名: 当前类的类型    typedef AlexModelNode<T, P, Alloc> self_type;    // 定义一个类型别名 alloc_type,它是 Alloc 分配器类针对 self_type 类型的特化版本的类型    typedef typename Alloc::template rebind<self_type>::other alloc_type;    typedef typename Alloc::template rebind<AlexNode<T, P> *>::other pointer_alloc_type;    const Alloc &allocator_;    // Number of logical children. Must be a power of 2    int num_children_ = 0;    // Array of pointers to children    AlexNode<T, P> **children_ = nullptr;    // 指向子节点的指针数组    // 构造函数1：使用默认分配器初始化对象    explicit AlexModelNode(const Alloc &alloc = Alloc())            : AlexNode<T, P>(0, false), allocator_(alloc) &#123;&#125;    // 构造函数2：使用指定的层级和默认分配器初始化对象    explicit AlexModelNode(short level, const Alloc &alloc = Alloc())            : AlexNode<T, P>(level, false), allocator_(alloc) &#123;&#125;    ~AlexModelNode() &#123;        if (children_ == nullptr) &#123;            return;        &#125;        // 调用 pointer_allocator() 函数获取分配器对象,        // 并使用其 deallocate 方法释放 children_ 指向的内存空间. num_children_ 参数指定要释放的多少        pointer_allocator().deallocate(children_, num_children_);    &#125;    AlexModelNode(const self_type &other) : AlexNode<T, P>(other), allocator_(other.allocator_),                                            num_children_(other.num_children_) &#123;        // 使用分配器对象pointer_allocator()分配一块内存空间,大小为other.num_children_个指针的大小,        // 并将返回的指针赋值给children_        children_ = new(pointer_allocator().allocate(other.num_children_))                AlexNode<T, P> *[other.num_children_];        // 将other对象的子节点指针数组复制到当前对象的子节点指针数组中        std::copy(other.children_, other.children_ + other.num_children_, children_);    &#125;    // Given a key, traverses to the child node responsible for that key    // 根据给定的键（key）获取对应的子节点指针    inline AlexNode<T, P> *get_child_node(const T &key) &#123;        int bucketID = this->model_.predict(key);        bucketID = std::min<int>(std::max<int>(bucketID, 0), num_children_ - 1);        return children_[bucketID];    &#125;    pointer_alloc_type pointer_allocator() &#123;        return pointer_alloc_type(allocator_);    &#125;    // 计算节点的大小,包括当前对象的大小以及子节点指针数组的大小    long long node_size() const override &#123;        long long size = sizeof(self_type);        size += num_children_ * sizeof(AlexNode<T, P> *);  // pointers to children        return size;    &#125;&#125;;</code></pre><p><strong>内部节点扩展</strong>：</p><pre class=" language-c++"><code class="language-c++">// Expand by a power of 2 by creating duplicates of all existing child pointers.// Input is the base 2 log of the expansion factor, in order to guarantee expanding by a power of 2.// Returns the expansion factor.// 通过创建现有子指针的副本来将数据结构扩展到2的幂次方大小int expand(int log2_expansion_factor) &#123;    assert(log2_expansion_factor >= 0);    // 实际的扩展因子    int expansion_factor = 1 << log2_expansion_factor;    // 新的子节点数量    int num_new_children = num_children_ * expansion_factor;    auto new_children = new(pointer_allocator().allocate(num_new_children))        AlexNode<T, P> *[num_new_children];    int cur = 0;    // 遍历现有的子节点，并将每个子节点复制到新数组中相应的位置    while (cur < num_children_) &#123;        AlexNode<T, P> *cur_child = children_[cur];        // 复制的数量由当前子节点的duplication_factor_属性决定。        int cur_child_repeats = 1 << cur_child->duplication_factor_;        for (int i = expansion_factor * cur; i < expansion_factor * (cur + cur_child_repeats); i++) &#123;            new_children[i] = cur_child;        &#125;        // 更新当前子节点的duplication_factor_属性，以便记录其被复制的次数。        cur_child->duplication_factor_ += log2_expansion_factor;        cur += cur_child_repeats;    &#125;    pointer_allocator().deallocate(children_, num_children_);    children_ = new_children;    num_children_ = num_new_children;    this->model_.expand(expansion_factor);    return expansion_factor;&#125;</code></pre><h5 id="AlexDataNode"><a href="#AlexDataNode" class="headerlink" title="AlexDataNode"></a>AlexDataNode</h5><p><strong>封装类型别名</strong>：</p><pre class=" language-c++"><code class="language-c++">template<class T, class P, class Compare = AlexCompare, class Alloc = std::allocator<std::pair<T, P>>, bool allow_duplicates = true>class AlexDataNode : public AlexNode<T, P> &#123;    public:        // 类型别名V, 表示键值对的类型        typedef std::pair<T, P> V;        // 类型别名self_type, 表示当前类的类型        typedef AlexDataNode<T, P, Compare, Alloc, allow_duplicates> self_type;        // 类型别名alloc_type, 表示用于分配self_type对象的分配器类型        typedef typename Alloc::template rebind<self_type>::other alloc_type;        // 类型别名key_alloc_type, 表示用于分配键类型的分配器类型        typedef typename Alloc::template rebind<T>::other key_alloc_type;        // 类型别名payload_alloc_type, 表示用于分配值类型的分配器类型        typedef typename Alloc::template rebind<P>::other payload_alloc_type;        // 类型别名value_alloc_type, 表示用于分配键值对类型的分配器类型        typedef typename Alloc::template rebind<V>::other value_alloc_type;        // 类型别名bitmap_alloc_type, 表示用于分配位图类型的分配器类型        typedef typename Alloc::template rebind<uint64_t>::other bitmap_alloc_type;        // 常量引用key_less_, 表示用于比较键大小的比较函数对象        const Compare &key_less_;        // 常量引用allocator_, 表示用于分配内存的分配器对象        const Alloc &allocator_;        // Forward declaration        template<typename node_type = self_type, typename payload_return_type = P, typename value_return_type = V>        class Iterator;    // 迭代容器中的元素        // 类型别名iterator_type, 表示非const迭代器类型        typedef Iterator<> iterator_type;        // 类型别名const_iterator_type, 表示const迭代器类型        typedef Iterator<const self_type, const P, const V> const_iterator_type;        // 一个指向下一个叶子节点的指针next_leaf_, 初始值为nullptr        self_type *next_leaf_ = nullptr;        // 一个指向上一个叶子节点的指针prev_leaf_, 初始值为nullptr        self_type *prev_leaf_ = nullptr;&#125;;</code></pre><p>数据节点属性：</p><pre class=" language-c++"><code class="language-c++">#if ALEX_DATA_NODE_SEP_ARRAYS    T *key_slots_ = nullptr;        // holds keys    P *payload_slots_ = nullptr;    // holds payloads, must be same size as key_slots#else    V* data_slots_ = nullptr;  // holds key-payload pairs#endif// 键/数据槽数组的大小int data_capacity_ = 0;  // size of key/data_slots array// 已填充的键/数据槽的数量（与间隙相对）int num_keys_ = 0;  // number of filled key/data slots (as opposed to gaps)// Bitmap: each uint64_t represents 64 positions in reverse order// (i.e., each uint64_t is "read" from the right-most bit to the left-most bit)// bitmap_是一个位图，每个uint64_t代表64个位置，从右到左读取uint64_t *bitmap_ = nullptr;// 位图中的int64_t数量int bitmap_size_ = 0;  // number of int64_t in bitmap// Variables related to resizing (expansions and contractions)static constexpr double kMaxDensity_ = 0.8;  // density after contracting,// also determines the expansion // thresholdstatic constexpr double kInitDensity_ = 0.7;  // density of data nodes after bulk loadingstatic constexpr double kMinDensity_ = 0.6;  // density after expanding, also// determines the contraction // thresholddouble expansion_threshold_ = 1;  // expand after m_num_keys is >= this numberdouble contraction_threshold_ = 0;  // contract after m_num_keys is < this numberstatic constexpr int kDefaultMaxDataNodeBytes_ = 1 << 24;  // by default, maximum data node size is 16MB// 最大键/数据槽的数量，不能超过这个数量进行扩展int max_slots_ = kDefaultMaxDataNodeBytes_ / sizeof(V);  // cannot expand beyond this number of key/data slots// Counters used in cost modelslong long num_shifts_ = 0;                 // does not reset after resizinglong long num_exp_search_iterations_ = 0;  // does not reset after resizingint num_lookups_ = 0;                      // does not reset after resizingint num_inserts_ = 0;                      // does not reset after resizingint num_resizes_ = 0;  // technically not required, but nice to have// Variables for determining append-mostly behaviorT max_key_ = std::numeric_limits<T>::lowest();  // max key in node, updates after inserts but not erasesT min_key_ = std::numeric_limits<T>::max();  // min key in node, updates after// inserts but not erasesint num_right_out_of_bounds_inserts_ = 0;  // number of inserts that are larger than the max keyint num_left_out_of_bounds_inserts_ = 0;  // number of inserts that are smaller than the min key// Node is considered append-mostly if the fraction of inserts that are out of// bounds is above this threshold// Append-mostly nodes will expand in a manner that anticipates further// appendsstatic constexpr double kAppendMostlyThreshold = 0.9;// Purely for benchmark debugging purposesdouble expected_avg_exp_search_iterations_ = 0;double expected_avg_shifts_ = 0;// Placed at the end of the key/data slots if there are gaps after the max key// 一个哨兵值，放置在键/数据槽的末尾，用于标记最大键之后的任何间隙static constexpr T kEndSentinel_ = std::numeric_limits<T>::max();</code></pre><p><strong>构造函数</strong>：</p><pre class=" language-c++"><code class="language-c++"></code></pre><p><strong>迭代器</strong>：</p><pre class=" language-c++"><code class="language-c++"></code></pre><p><strong>插入</strong>：</p><pre class=" language-c++"><code class="language-c++"></code></pre><p><strong>查询</strong>：</p><pre class=" language-c++"><code class="language-c++"></code></pre>]]></content>
      
      
      <categories>
          
          <category> 学习型索引 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习型索引 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FINEdex</title>
      <link href="posts/323e.html"/>
      <url>posts/323e.html</url>
      
        <content type="html"><![CDATA[<h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><p>随着现有数据量的快速增长，人们考虑使用学习索引来替换传统的索引结构。但是传统的学习索引无法解决高并发需求，作者首先总结了现有的学习索引存在的短板：</p><ul><li>有限的可扩展性。针对插入更新操作，现有的学习索引很难拥有很好的性能。目前提出的一些学习索引结构只能单独的解决高并发的读，写，重训练。FITing-tree,ALEX,PGM-index都没有考虑数据一致性问题。XIndex通过把数据保存在不同的数据结构中来解决并发的数据一致性问题，这样会导致范围查询操作的性能低下。</li><li>高开销问题。XIndex和FITing-tree都是通过建立一个delta buffer来解决插入操作，该缓冲区是B+树或者Mass树。并且XIndex显示，当缓冲区过大时，会产生很大程度的性能下降。ALEX和PGM-index是通过在索引结构中保留了空槽来解决插入时产生的高开销问题。一旦发生多线程访问时，会发生多个线程的冲突访问情况，这会导致并发操作的性能下降。</li></ul><p>作者提出了FINEdex做出了如下贡献：</p><ol><li>高可伸缩性。提出了一种用于并发内存系统的非粒度学习索引方案，即FINEdex，它有效地满足了可伸缩性的要求。主要的贡献是通过充实的数据结构来减少数据依赖性，并同时在两个粒度中对模型进行重新训练。</li><li>低开销的索引操作和无阻塞的重训练操作。</li><li>系统应用以及评测。</li></ol><p><img src="https://yux20000304.github.io/2022/10/12/FINEdex%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/1.png" alt="img"></p><p>上图给出了目前现有的学习索引的功能。</p><h2 id="设计部分介绍"><a href="#设计部分介绍" class="headerlink" title="设计部分介绍"></a>设计部分介绍</h2><p>首先给出了数据结构设计部分。</p><p><img src="https://yux20000304.github.io/2022/10/12/FINEdex%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/2.png" alt="img"></p><p>作者通过对XIndex的设计结构进行修改，实现了一个高效的并发性数据索引结构。实现了一个两级的有序数组结构。该结构能够很好的避免数据依赖性并且能够保持数据的有序性。并且在插入的过程中，旧数据的访问不会受到影响。只有当缓冲区满了的时候，才会发生数据合并以及模型重训练操作。作者把整个设计分成了两个部分：</p><ol><li>模型部分：这些独立的模型可以通过并发的再训练来适应新的数据分布。</li><li>数据部分：通过层级结构，建立了两层有序数组，提供能快速重训练的方案。</li></ol><ul><li>模型部分，首先作者设计了一个方案来提高模型的准确率。学习探针算法LPA。通过在f(x)函数周围建立一个平行四边形囊括所有的训练数据，之后再使用线性回归模型进行拟合。传统的RMI模型，由于无法针对不同分布的数据来决定模型的个数，RMI模型的准确率一直是一个潜在的问题。LPA算法通过设置一个错误率阈值，来判断某一组数据是否为线性分布的。如果大于这个阈值，那么移除该数据集靠后的一部分数据，如果小于阈值，那么添加新的数据进入原来的数据集中再重新训练。这个过程一共需要提供如下几个参数：</li></ul><ol><li>threshold：错误率阈值。</li><li>learning_step：学习步长，用来决定学习速度的参数之一。</li><li>learning_rate：学习率，用来决定学习速度的参数之一。<br>整个过程是一个贪心算法。最终得到的输出是一串错误率为threshold的线性模型链。</li></ol><p>除了以上的几个参数设置，作者还提出了优化模型输出的方案。上面提到了LPA算法得到的是一串线性模型链，在执行查找等操作时，需要顺序遍历整个模型链。最终作者选择使用优化后的b树来存储这些模型&lt;key，model&gt;，key代表了一个模型的最大元素。</p><ul><li><p>数据部分，主要解决了并发性问题。每一个模型都有一个小型的缓冲区来解决修改和插入等操作。整个小型缓冲区被设计成了两层的b树结构，在训练好的模型中，每个数组中的元素都有一个bin指针，用来指向level bin缓冲区，该缓冲区分为root层和child层，首先插入的数据会被放到root层，随着root层满了，会创建child节点来存储新插入的节点，之后插入的节点会优先插入到靠前的child节点来节约空间使用。<strong>缺陷：在插入的过程中需要不断地调整树形结构，这可能会造成很大的性能开销</strong></p></li><li><p>紧接着，作者提出了在并发情况下的模型重训练设计思路。作者把重训练分成了两个部分：level-bin重训练和模型重训练。</p><ul><li>level-bin重训练<br>作者提出每当一个level-bin层满了之后，只需要对一个该层进行重训练，不需要影响其余的level-bin节点，产生的时间开销仅有27us</li><li>模型重训练<br>作者通过使用RCU技术保证了新模型的训练过程中，旧模型的访问操作不被影响。由于level-bins是通过指针指向的，我们只需要在复制的过程中使用新的指针指向level-bin即可。模型重训练会在模型需要重新训练一个更小的模型时被触发。</li></ul></li><li><p>并发：作者通过提出一系列方案来解决并发冲突</p><ul><li>写写冲突：当不同线程需要对同样的数据或者是bin进行修改会产生写写冲突。作者提出使用细粒度的锁分别对记录和bin进行上锁。对记录上锁的操作比较简单，对于bin上锁，需要根据情况进行上锁，有的时候可能只需要对child bin进行上锁，有的情况需要对root bin也进行上锁。</li><li>读写冲突：通过使用版本控制实现数据一致性。每次完成训练后，都会给模型和数据分配一个版本号。在一次访问过程中，首先会获取版本号，如果在读取完成后版本号没有发生改变，那么说明读取操作的数据是有效的，如果无效那么重新发起读请求。</li></ul></li></ul><h2 id="测试部分介绍"><a href="#测试部分介绍" class="headerlink" title="测试部分介绍"></a>测试部分介绍</h2><p>在redis中进行测试。只修改了基础的排序数据结构。一共提供了六个常见的api（train，get，put，update，remove，scan）。一共分配了4个线程来执行模型的重训练，访问过程一共分为三步：在模型中进行查找，计算区间，操作level-bin。使用24线程进行测试。使用了Masstree，Xindex，LI+Δ三个索引设计方案进行对比。没有采用redis原生的跳表结构进行对比，因为其与树形结构的性能比较类似。使用LI+Δ方案时，为了让其支持并发操作，使用了XIndex的delta buffer方案，使用masstree管理该缓冲区。FIT树由于不支持并发没有进行比较。</p><ul><li>配置部分：学习索引结构都使用了两层RMI模型，第二层最多容纳250k的模型数量（XIndex中得出的结论）。FINEdex的threshold我们设置成32，level-bin层的root设置成8，child bin大小设置为16。</li><li>使用的测试工具：<ol><li>YCSB测试工具：一共提供了六种不同的测试负载。</li><li>网络博客负载</li><li>文档id负载</li></ol></li></ul>]]></content>
      
      
      <categories>
          
          <category> 学习型索引 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习型索引 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LearnedIndex-Papers</title>
      <link href="posts/5da4.html"/>
      <url>posts/5da4.html</url>
      
        <content type="html"><![CDATA[<h2 id="学习型索引"><a href="#学习型索引" class="headerlink" title="学习型索引"></a>学习型索引</h2><h3 id="1-论文"><a href="#1-论文" class="headerlink" title="1. 论文"></a>1. 论文</h3><h4 id="1-1-年份"><a href="#1-1-年份" class="headerlink" title="1.1 年份"></a>1.1 年份</h4><h5 id="2018"><a href="#2018" class="headerlink" title="2018"></a>2018</h5><table><thead><tr><th>发表年份</th><th>原文地址</th><th>Code</th></tr></thead><tbody><tr><td><a href="papers/2018-SIGMOD-The_Case_for_Learned_Index_Structures.pdf">2018-SIGMOD-The Case for Learned Index Structures</a> : 开山之作</td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3183713.3196909">Sigmod</a></td><td><a href="https://github.com/learnedsystems/RMI">learnedsystems/RMI</a></td></tr></tbody></table><hr><h5 id="2019"><a href="#2019" class="headerlink" title="2019"></a>2019</h5><table><thead><tr><th>发表年份</th><th>原文地址</th><th>Code</th></tr></thead><tbody><tr><td><a href="papers/2019-arXiv-A_Benchmark_for_Learned_Indexes.pdf">2019-arXiv-SOSD: A Benchmark for Learned Indexes</a>  : 测试基准</td><td><a href="https://arxiv.org/pdf/1911.13014.pdf">arXiv</a></td><td><a href="https://github.com/learnedsystems/SOSD">learnedsystems/SOSD</a></td></tr><tr><td><a href="papers/2019-arXiv-A_Scalable_Learned_Index_Scheme_in_Storage_Systems.pdf">2019-arXiv-A Scalable Learned Index Scheme in Storage Systems</a>:  FINEdex 版之始</td><td><a href="https://arxiv.org/pdf/1905.06256.pdf">arXiv</a></td><td></td></tr><tr><td><a href="papers/2019-SIGMOD-FITing-Tree_A_Data-aware_Index_Structure.pdf">2019-SIGMOD-FITing-Tree_A Data-aware Index Structure</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3299869.3319860">Sigmod</a></td><td></td></tr><tr><td><a href="papers/2019-%E8%BD%AF%E4%BB%B6%E5%AD%A6%E6%8A%A5-%E5%9F%BA%E4%BA%8E%E4%B8%AD%E9%97%B4%E5%B1%82%E7%9A%84%E5%8F%AF%E6%89%A9%E5%B1%95%E5%AD%A6%E4%B9%A0%E7%B4%A2%E5%BC%95%E6%8A%80%E6%9C%AF.pdf">2019-软件学报-基于中间层的可扩展学习索引技术</a></td><td><a href="https://www.jos.org.cn/html/2020/3/5910.htm">软件学报</a></td><td></td></tr></tbody></table><hr><h5 id="2020"><a href="#2020" class="headerlink" title="2020"></a>2020</h5><table><thead><tr><th>发表年份</th><th>原文地址</th><th>Code</th></tr></thead><tbody><tr><td><a href="papers/2020-%E8%BD%AF%E4%BB%B6%E5%AD%A6%E6%8A%A5-%E5%AD%A6%E4%B9%A0%E7%B4%A2%E5%BC%95%EF%BC%9A%E7%8E%B0%E7%8A%B6%E4%B8%8E%E7%A0%94%E7%A9%B6%E5%B1%95%E6%9C%9B.pdf">2020-软件学报-学习索引：现状与研究展望</a></td><td><a href="https://www.jos.org.cn/html/2021/4/6168.htm">软件学报</a></td><td></td></tr><tr><td><a href="papers/2020-aiDM-RadixSpline_A_Single-Pass_Learned_Index.pdf">2020-aiDM-RadixSpline_A Single-Pass Learned Index</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3401071.3401659">aiDM</a></td><td></td></tr><tr><td><a href="papers/2020-APSys-SIndex_A_Scalable_Learned_Index_for_String_Keys.pdf">2020-APSys-SIndex_A Scalable Learned Index for String Keys</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3409963.3410496">APSys</a></td><td></td></tr><tr><td><a href="papers/2020-ICDEW-START_Self-Tuning_Adaptive_Radix_Tree.pdf">2020-ICDEW-START_Self-Tuning_Adaptive_Radix_Tree</a></td><td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9094133">ICDE</a></td><td></td></tr><tr><td><a href="papers/2020-NeurIPS-Learned_Indexes_for_a_Google-scale_Disk-based_Database.pdf">2020-NeurIPS-Learned Indexes for a Google-scale Disk-based Database</a></td><td><a href="https://arxiv.org/pdf/2012.12501.pdf">arXiv</a></td><td></td></tr><tr><td><a href="papers/2020-OSDI-Bourbon_learned_LSM.pdf">2020-OSDI-Bourbon_learned_LSM</a></td><td><a href="https://dl.acm.org/doi/pdf/10.5555/3488766.3488775">OSDI</a></td><td></td></tr><tr><td><a href="papers/2020-OSDI-Bourbon_learned_LSM_slides.pdf">2020-OSDI-Bourbon_learned_LSM_slides</a></td><td><a href="https://dl.acm.org/doi/pdf/10.5555/3488766.3488775">OSDI</a></td><td></td></tr><tr><td><a href="papers/2020-PPoPP-XIndex_A_Scalable_Learned_Index_for_Multicore_Data_Storage.pdf">2020-PPoPP-XIndex_A Scalable Learned Index for Multicore Data Storage</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3332466.3374547">PPoPP</a></td><td></td></tr><tr><td><a href="papers/2020-VLDB-PGM-index_fully-dynamic_compressed_worst-case_bounds.pdf">2020-VLDB-PGM-index_fully-dynamic_compressed_worst-case_bounds</a></td><td><a href="https://dl.acm.org/doi/pdf/10.14778/3389133.3389135">PVLDB</a></td><td></td></tr><tr><td><a href="papers/2020-SIGMOD-ALEX_Updatable_Adaptive_Learned_Index.pdf">2020-SIGMOD-ALEX_Updatable_Adaptive_Learned_Index</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3318464.3389711">Sigmod</a></td><td></td></tr><tr><td><a href="papers/2020-SIGMOD-CDFShop_Exploring_and_Optimizing_Learned_Index_Structures.pdf">2020-SIGMOD-CDFShop Exploring and Optimizing Learned Index Structures</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3318464.3384706">Sigmod</a></td><td></td></tr><tr><td><a href="papers/2020-SIGMOD-Order-Preserving_Key_Compression_for_In-Memory_Search_Trees.pdf">2020-SIGMOD-Order-Preserving Key Compression for In-Memory Search Trees</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3318464.3380583">Sigmod</a></td><td></td></tr><tr><td><a href="papers/2020-SIGMOD-Order-Preserving_Key_Compression_for_In-Memory_Search_Trees_slides.pdf">2020-SIGMOD-Order-Preserving Key Compression for In-Memory Search Trees_slides</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3318464.3380583">Sigmod</a></td><td></td></tr><tr><td><a href="papers/2020-SIGMOD-The_Case_for_a_Learned_Sorting_Algorithm.pdf">2020-SIGMOD-The Case for a Learned Sorting Algorithm</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3318464.3389752">Sigmod</a></td><td></td></tr></tbody></table><hr><h5 id="2021"><a href="#2021" class="headerlink" title="2021"></a>2021</h5><table><thead><tr><th>发表年份</th><th>原文地址</th><th>Code</th></tr></thead><tbody><tr><td><a href="papers/2021-AIDB-PLEX_RS+CHT.pdf">2021-AIDB-PLEX_RS+CHT</a></td><td><a href="https://arxiv.org/pdf/2108.05117.pdf">AIDB</a></td><td></td></tr><tr><td><a href="papers/2021-AIDB-RSS_Bounding_the_Last_Mile-Efficient_Learned_String_Indexing.pdf">2021-AIDB-RSS_Bounding_the_Last_Mile-Efficient_Learned_String_Indexing</a></td><td><a href="https://arxiv.org/pdf/2111.14905.pdf">AIDB</a></td><td></td></tr><tr><td><a href="papers/2021-aiDM-RUSLI_Real-time_Updatable_Spline_Learned_Index.pdf">2021-aiDM-RUSLI_Real-time_Updatable_Spline_Learned_Index</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3464509.3464886">aiDM</a></td><td></td></tr><tr><td><a href="papers/2021-aiDM-Tailored_Regression_Learned_Indexes-Logarithmic-Error-Regression.pdf">2021-aiDM-Tailored_Regression_Learned_Indexes-Logarithmic-Error-Regression</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3464509.3464891">aiDM</a></td><td><a href="https://github.com/umatin/LogarithmicErrorRegression">LogarithmicErrorRegression</a></td></tr><tr><td><a href="papers/2021-arXiv-Micro-architectural_Analysis_of_a_Learned_Index.pdf">2021-arXiv-Micro-architectural Analysis of a Learned Index</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3533702.3534917">arXiv</a></td><td></td></tr><tr><td><a href="papers/2021-arXiv-Pluggable_Learned_Index_Method_via_Sampling_and_Gap_Insertion.pdf">2021-arXiv-Pluggable_Learned_Index_Method_via_Sampling_and_Gap_Insertion</a></td><td><a href="https://arxiv.org/pdf/2101.00808.pdf">arXiv</a></td><td></td></tr><tr><td><a href="papers/2021-EDBT-Shift-Table_A_Low-latency_Learned_Index_for_Range_Queries_using_Model_Correction.pdf">2021-EDBT-Shift-Table A Low-latency Learned Index for Range Queries using Model Correction</a></td><td><a href="https://arxiv.org/pdf/2101.10457.pdf">EDBT</a></td><td></td></tr><tr><td><a href="papers/2021-ICDEW-Towards_a_Benchmark_for_Learned_Systems.pdf">2021-ICDEW-Towards a Benchmark for Learned Systems</a></td><td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9438803">ICDE</a></td><td></td></tr><tr><td><a href="papers/2021-PVLDB-Benchmarking_Learned_Indexes.pdf">2021-PVLDB-Benchmarking_Learned_Indexes</a></td><td><a href="https://dl.acm.org/doi/pdf/10.14778/3421424.3421425">PVLDB</a></td><td></td></tr><tr><td><a href="papers/2021-PVLDB-Endownment-learned_cardition.pdf">2021-PVLDB-Endownment-learned_cardition</a></td><td><a href="https://dl.acm.org/doi/pdf/10.14778/3461535.3461552">PVLDB</a></td><td></td></tr><tr><td><a href="papers/2021-PVLDB-FINEdex-Fine-grained_for_Scalable_Concurrent_Memory_Systems.pdf">2021-PVLDB-FINEdex-Fine-grained_for_Scalable_Concurrent_Memory_Systems</a></td><td><a href="https://dl.acm.org/doi/pdf/10.14778/3489496.3489512">PVLDB</a></td><td></td></tr><tr><td><a href="papers/2021-PVLDB-LIPP_Updatable_Learned_Index_Precise_Positions.pdf">2021-PVLDB-LIPP_Updatable_Learned_Index_Precise_Positions</a></td><td><a href="https://dl.acm.org/doi/pdf/10.14778/3457390.3457393">PVLDB</a></td><td><a href="https://github.com/Jiacheng-WU/lipp">lipp</a></td></tr></tbody></table><hr><h5 id="2022"><a href="#2022" class="headerlink" title="2022"></a>2022</h5><table><thead><tr><th>发表年份</th><th>原文地址</th><th>Code</th></tr></thead><tbody><tr><td><a href="papers/2022-aiDB-AutoIndex_Automatically_Finding_Optimal_Index_Structure.pdf">2022-aiDB-AutoIndex_Automatically_Finding_Optimal_Index_Structure</a></td><td><a href="https://arxiv.org/pdf/2208.03823.pdf">aiDB</a></td><td></td></tr><tr><td><a href="papers/2022-aiDM-LSI-Learned_Secondary_Index_Structure.pdf">2022-aiDM-LSI-Learned_Secondary_Index_Structure</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3533702.3534912">aiDM</a></td><td></td></tr><tr><td><a href="papers/2022-ICLR_learned_index_with_dynamic_eps.pdf">2022-ICLR_learned_index_with_dynamic_eps</a></td><td><a href="https://openreview.net/pdf?id=VyZRObZ19kt">ICLR</a></td><td></td></tr><tr><td><a href="papers/2022-learned_Similarity_Search.pdf">2022-learned_Similarity_Search</a></td><td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9891778">IEEE</a></td><td></td></tr><tr><td><a href="papers/2022-PVLDB-are_updatable_learned_index_ready.pdf">2022-PVLDB-are_updatable_learned_index_ready </a></td><td><a href="https://dl.acm.org/doi/pdf/10.14778/3551793.3551848">PVLDB</a></td><td></td></tr><tr><td><a href="papers/2022-PVLDB-Endowment-Can_Learned_Models_Replace_Hash_Functions.pdf">2022-PVLDB-Endowment-Can_Learned_Models_Replace_Hash_Functions</a></td><td><a href="https://dl.acm.org/doi/pdf/10.14778/3570690.3570702">PVLDB</a></td><td></td></tr><tr><td><a href="papers/2022-PVLDB-Endowment-FILM-for-Larger-than-Memory-Databases.pdf">2022-PVLDB-Endowment-FILM-for-Larger-than-Memory-Databases</a></td><td><a href="https://dl.acm.org/doi/pdf/10.14778/3570690.3570704">PVLDB</a></td><td></td></tr><tr><td><a href="papers/2022-TOS-The_Concurrent_Learned_Indexes_for_Multicore_Data_Storage.pdf">2022-TOS-The Concurrent Learned Indexes for Multicore Data Storage</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3478289">TOS</a></td><td></td></tr><tr><td><a href="papers/2022-VLDB-APEX_Learned_Index_PM.pdf">2022-VLDB-APEX_Learned_Index_PM</a></td><td><a href="https://dl.acm.org/doi/pdf/10.14778/3494124.3494141">VLDB</a></td><td></td></tr><tr><td><a href="papers/2022-VLDB-NFL_Learned_Index_Distribution_Transformation.pdf">2022-VLDB-NFL_Learned_Index_Distribution_Transformation</a>: it transforms keys distribution to make it more linear.</td><td><a href="https://dl.acm.org/doi/pdf/10.14778/3547305.3547322">VLDB</a></td><td></td></tr></tbody></table><hr><h5 id="2023"><a href="#2023" class="headerlink" title="2023"></a>2023</h5><table><thead><tr><th>发表年份</th><th>原文地址</th><th>Code</th></tr></thead><tbody><tr><td><a href="papers/2023-arXiv-DILI_A_Distribution-Driven_Learned_Index.pdf">2023-arXiv-DILI_A Distribution-Driven Learned Index</a>  Using more bulk loading time for better lookup.</td><td><a href="https://arxiv.org/pdf/2304.08817.pdf">arXiv</a></td><td></td></tr><tr><td><a href="papers/2023-arxiv-Updatable_Learned_Indexes_Disk-Resident_DBMS.pdf">2023-arXiv-Updatable_Learned_Indexes_Disk-Resident_DBMS</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3589284">arXiv</a></td><td></td></tr><tr><td><a href="papers/2023-ASPLOS-LeaFTL-leared_FTL_for_SSD.pdf">2023-ASPLOS-LeaFTL- A Learning-Based Flash Translation Layer for Solid-State Drives</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3575693.3575744">ASPLOS</a></td><td></td></tr><tr><td><a href="papers/2023-FAST-ROLEX.pdf">2023-FAST-ROLEX</a></td><td></td><td></td></tr><tr><td><a href="papers/2023-PVLDB-Learned_Index_A_Comprehensive_Experimental_Evaluation.pdf">2023-PVLDB-Learned Index_A Comprehensive Experimental Evaluation</a></td><td><a href="https://dl.acm.org/doi/pdf/10.14778/3594512.3594528">PVLDB</a></td><td></td></tr></tbody></table><hr><h4 id="1-2-类别"><a href="#1-2-类别" class="headerlink" title="1.2 类别"></a>1.2 类别</h4><h5 id="Survey-Benchmark-Tuning"><a href="#Survey-Benchmark-Tuning" class="headerlink" title="Survey/Benchmark/Tuning"></a>Survey/Benchmark/Tuning</h5><ol><li> <a href="papers/2019-arXiv-A_Benchmark_for_Learned_Indexes.pdf">2019-arXiv-A Benchmark for Learned Indexes</a></li><li> <a href="papers/2020-SIGMOD-CDFShop_Exploring_and_Optimizing_Learned_Index_Structures.pdf">2020-SIGMOD-demo-CDFShop-tuning_RMI</a></li><li> <a href="papers/2021-PVLDB-Benchmarking_Learned_Indexes.pdf">2021-PVLDB-Benchmarking_Learned_Indexes</a></li><li> <a href="papers/2021-aiDM-Tailored_Regression_Learned_Indexes-Logarithmic-Error-Regression.pdf">2021-aiDM-Tailored_Regression_Learned_Indexes-Logarithmic-Error-Regression</a></li><li> <a href="papers/2022-PVLDB-are_updatable_learned_index_ready.pdf">2022-are_updatable_learned_index_ready</a></li><li> <a href="papers/2023-PVLDB-Learned_Index_A_Comprehensive_Experimental_Evaluation.pdf">2023-PVLDB-Endowment-Comprehensive_Experimental_Evaluation</a></li></ol><h5 id="Read-only"><a href="#Read-only" class="headerlink" title="Read-only"></a>Read-only</h5><ol><li> <a href="papers/2018-SIGMOD-The_Case_for_Learned_Index_Structures.pdf">2018-SIGMOD-The Case for Learned Index Structures</a></li><li> <a href="papers/2020-aiDM-RadixSpline_A_Single-Pass_Learned_Index.pdf">2020-aiDM-RadixSpline_A Single-Pass Learned Index</a></li><li> <a href="papers/2020-NeurIPS-Learned_Indexes_for_a_Google-scale_Disk-based_Database.pdf">2020-NeurIPS-Learned Indexes for a Google-scale Disk-based Database</a></li><li> <a href="papers/2021-AIDB-PLEX_RS+CHT.pdf">2021-AIDB-PLEX_RS+CHT</a>: RadixSpine as the top + Compact Hist-Tree as the bottom</li><li> <a href="papers/2021-AIDB-RSS_Bounding_the_Last_Mile-Efficient_Learned_String_Indexing.pdf">2021-AIDB-RSS_Bounding_the_Last_Mile-Efficient_Learned_String_Indexing</a></li></ol><h5 id="Updatable"><a href="#Updatable" class="headerlink" title="Updatable"></a>Updatable</h5><ol><li> <a href="papers/2019-SIGMOD-FITing-Tree_A_Data-aware_Index_Structure.pdf">2019-SIGMOD-FITing-Tree_A Data-aware Index Structure</a></li><li> <a href="papers/2020-SIGMOD-ALEX_Updatable_Adaptive_Learned_Index.pdf">2020-SIGMOD-ALEX_Updatable_Adaptive_Learned_Index</a> Use gapped array for SMO</li><li> <a href="papers/2020-VLDB-PGM-index_fully-dynamic_compressed_worst-case_bounds.pdf">2020-VLDB-PGM-index_fully-dynamic_compressed_worst-case_bounds</a></li><li> <a href="papers/2021-PVLDB-LIPP_Updatable_Learned_Index_Precise_Positions.pdf">2021-PVLDB-LIPP_Updatable_Learned_Index_Precise_Positions</a></li><li> <a href="papers/2021-aiDM-RUSLI_Real-time_Updatable_Spline_Learned_Index.pdf">2021-aiDM-RUSLI_Real-time_Updatable_Spline_Learned_Index</a></li><li> <a href="papers/2022-TOS-The_Concurrent_Learned_Indexes_for_Multicore_Data_Storage.pdf">2022-TOS-Xindex-most-recent</a></li><li> <a href="papers/2023-FAST-ROLEX.pdf">2023-FAST-ROLEX</a></li><li> <a href="papers/2023-arXiv-DILI_A_Distribution-Driven_Learned_Index.pdf">2023-arxiv-DILI-A Distribution-Driven Learned Index</a> Using more bulk loading time for better lookup.</li></ol><h5 id="Secondary-Storage-Persistent-Memory-LSM"><a href="#Secondary-Storage-Persistent-Memory-LSM" class="headerlink" title="Secondary Storage/Persistent Memory/LSM"></a>Secondary Storage/Persistent Memory/LSM</h5><ol><li> <a href="papers/2019-arXiv-A_Scalable_Learned_Index_Scheme_in_Storage_Systems.pdf">2019-arXiv-A Scalable Learned Index Scheme in Storage Systems</a>: the initial version of FINEdex</li><li> <a href="papers/2020-NeurIPS-Learned_Indexes_for_a_Google-scale_Disk-based_Database.pdf">2020-NeurIPS-Learned Indexes for a Google-scale Disk-based Database</a></li><li> <a href="papers/2020-OSDI-Bourbon_learned_LSM.pdf">2020-OSDI-Bourbon_learned_LSM</a></li><li> <a href="papers/2020-OSDI-Bourbon_learned_LSM_slides.pdf">2020-OSDI-Bourbon_learned_LSM_slides</a></li><li> <a href="papers/2022-aiDM-LSI-Learned_Secondary_Index_Structure.pdf">2022-aiDM-LSI-Learned_Secondary_Index_Structure</a></li><li> <a href="papers/2022-VLDB-APEX_Learned_Index_PM.pdf">2022-VLDB-APEX_Learned_Index_PM</a></li><li> <a href="papers/2022-PVLDB-Endowment-FILM-for-Larger-than-Memory-Databases.pdf">2022-PVLDB-Endowment-FILM-for-Larger-than-Memory-Databases</a></li><li> <a href="papers/2023-arxiv-Updatable_Learned_Indexes_Disk-Resident_DBMS.pdf">2023-arxiv-Updatable_Learned_Indexes_Disk-Resident_DBMS</a></li></ol><h5 id="Radix-Spine-based"><a href="#Radix-Spine-based" class="headerlink" title="Radix-Spine based"></a>Radix-Spine based</h5><ol><li> <a href="papers/2020-aiDM-RadixSpline_A_Single-Pass_Learned_Index.pdf">2020-aiDM-Radix_Spline</a>: Using linear spine fits to a CDF, then a flat radix table as an appoximate index.</li><li> <a href="papers/2021-AIDB-RSS_Bounding_the_Last_Mile-Efficient_Learned_String_Indexing.pdf">2021-AIDB-RSS_Bounding_the_Last_Mile-Efficient_Learned_String_Indexing</a></li><li> <a href="papers/2021-AIDB-PLEX_RS+CHT.pdf">2021-AIDB-PLEX_RS+CHT</a></li><li> <a href="papers/2021-aiDM-RUSLI_Real-time_Updatable_Spline_Learned_Index.pdf">2021-aiDM-RUSLI_Real-time_Updatable_Spline_Learned_Index</a></li></ol><h5 id="Variable-length-string-keys"><a href="#Variable-length-string-keys" class="headerlink" title="Variable length string keys"></a>Variable length string keys</h5><ol><li> <a href="papers/2020-APSys-SIndex_A_Scalable_Learned_Index_for_String_Keys.pdf">2020-APSys-SIndex_Scalable_Learned_Index__String_Keys</a></li><li> <a href="papers/2021-AIDB-RSS_Bounding_the_Last_Mile-Efficient_Learned_String_Indexing.pdf">2021-AIDB-RSS_Bounding_the_Last_Mile-Efficient_Learned_String_Indexing</a></li><li> <a href="papers/2020-SIGMOD-Order-Preserving_Key_Compression_for_In-Memory_Search_Trees.pdf">2020-SIGMOD-HOPE</a>: not learned index, but an encoding schme; order persevering encoding for string; can be used for string learned indexes</li><li> <a href="papers/2020-SIGMOD-Order-Preserving_Key_Compression_for_In-Memory_Search_Trees_slides.pdf">2020-SIGMOD-HOPE_slides</a></li></ol><h5 id="Concurrency"><a href="#Concurrency" class="headerlink" title="Concurrency"></a>Concurrency</h5><ol><li> <a href="papers/2020-PPoPP-XIndex_A_Scalable_Learned_Index_for_Multicore_Data_Storage.pdf">2020-PPoPP-XIndex_Scalable_Learned_Index_for_Multicore_Data_Storage</a></li><li> <a href="papers/2020-APSys-SIndex_A_Scalable_Learned_Index_for_String_Keys.pdf">2020-APSys-SIndex_Scalable_Learned_Index_String_Keys</a></li><li> <a href="papers/2021-PVLDB-FINEdex-Fine-grained_for_Scalable_Concurrent_Memory_Systems.pdf">2021-PVLDB-FINEdex-Fine-grained_for_Scalable_Concurrent_Memory_Systems</a></li><li> <a href="papers/2022-TOS-The_Concurrent_Learned_Indexes_for_Multicore_Data_Storage.pdf">2022-TOS-Xindex-most-recent</a></li></ol><h5 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h5><ol><li> <a href="papers/2020-SIGMOD-The_Case_for_a_Learned_Sorting_Algorithm.pdf">2020-SIGMOD_The_Case_for_a_Learned_Sorting_Algorithm</a></li><li> <a href="papers/2022-PVLDB-Endowment-Can_Learned_Models_Replace_Hash_Functions.pdf">2022-PVLDB-Endowment-Can_Learned_Models_Replace_Hash_Functions </a></li><li> <a href="papers/2022-learned_Similarity_Search.pdf">2022-learned_Similarity_Search</a></li><li> <a href="papers/2023-ASPLOS-LeaFTL-leared_FTL_for_SSD.pdf">2023-ASPLOS-LeaFTL-Learning-Based Flash Translation Layer for Solid-State Drives</a> Learned index for SSD FTL page-level memory mapping.</li></ol><hr><h3 id="1-2-教程"><a href="#1-2-教程" class="headerlink" title="1.2 教程"></a>1.2 教程</h3><p><a href="https://blog.csdn.net/weixin_44026604/article/details/120776283">SIndex 论文笔记：A Scalable Learned Index for String Keys-CSDN博客</a></p><p><a href="https://zhuanlan.zhihu.com/p/277979207">OSDI20 - Bourbon: Learned Index for LSM - 知乎 (zhihu.com)</a></p>]]></content>
      
      
      <categories>
          
          <category> 论文整理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文整理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MyHexoBlog</title>
      <link href="posts/22ca.html"/>
      <url>posts/22ca.html</url>
      
        <content type="html"><![CDATA[<h2 id="MyHexoBlog"><a href="#MyHexoBlog" class="headerlink" title="MyHexoBlog"></a>MyHexoBlog</h2><p>首先，GitHub Pages 是由 GitHub 官方提供的一种免费的静态站点托管服务，让我们可以在 GitHub 仓库里托管和发布自己的静态网站页面。同时，Hexo 是一个快速、简洁且高效的静态博客框架，它基于 Node.js 运行，可以将我们撰写的 Markdown 文档解析渲染成静态的 HTML 网页。所以，我们可以先在本地撰写 Markdown 格式文章后，通过 Hexo 解析文档，渲染生成具有主题样式的 HTML 静态网页，再推送到 GitHub 上完成博文的发布。这样别人就能通过网址访问啦。</p><p><strong>优点：</strong>完全免费；静态站点，轻量快速；可按需求自由定制改造；托管在 GitHub，安全省心；迁移方便。</p><p><strong>不足：</strong>发文不便，依赖于本地环境；更适合个人博客使用；GitHub 在国内访问速度有点不快【gitee亦可】。</p><hr><h3 id="1-环境搭建"><a href="#1-环境搭建" class="headerlink" title="1. 环境搭建"></a>1. 环境搭建</h3><h4 id="下载安装-Node-js"><a href="#下载安装-Node-js" class="headerlink" title="下载安装 Node.js"></a>下载安装 Node.js</h4><p> 下载官网：<a href="https://nodejs.org/en">Node.js — Run JavaScript Everywhere (nodejs.org)</a>。 </p><p>安装参考教程：<a href="https://zhuanlan.zhihu.com/p/442215189">node.js安装及环境配置超详细教程【Windows系统安装包方式】 - 知乎 (zhihu.com)</a>。</p><p><strong>注意：可以变更缓存及包下载路径，但C盘够用，则不推荐变更</strong>。</p><p>① 打开安装的目录（默认安装情况下在C:\Program Files\nodejs）</p><pre class=" language-bash"><code class="language-bash">E:\EnvironmentSetting\NodeJs</code></pre><p>② 在安装目录下新建两个文件夹【node_global】和【node_cache】</p><p>③ 使用管理员权限打开 cmd 命令窗口，输入 <code>npm config set prefix “你的路径\node_global”</code> ：</p><pre class=" language-bash"><code class="language-bash">// 原位置<span class="token function">npm</span> config <span class="token keyword">set</span> prefix <span class="token string">"C:\Users\用户名\AppData\Roaming\npm\node_global"</span>// 新位置<span class="token function">npm</span> config <span class="token keyword">set</span> prefix <span class="token string">"E:\EnvironmentSetting\NodeJs\node_global"</span>// 查看位置<span class="token function">npm</span> root -g // 重置<span class="token function">npm</span> config delete prefix</code></pre><p>④ <code>npm config set cache “你的路径\node_cache” </code> ：</p><pre class=" language-bash"><code class="language-bash">// 原位置<span class="token function">npm</span> config <span class="token keyword">set</span> cache <span class="token string">"C:\Users\用户名\AppData\Roaming\node_cache"</span>// 新位置<span class="token function">npm</span> config <span class="token keyword">set</span> cache <span class="token string">"E:\EnvironmentSetting\NodeJs\node_cache"</span></code></pre><p>⑤设置环境变量，打开【系统属性】-【高级】-【环境变量】，在<code>系统变量</code>中新建</p><p>变量名：<code>NODE_PATH</code> </p><p>变量值：<code>E:\EnvironmentSetting\NodeJs\node_global\node_modules</code> </p><p>⑥ 编辑<code>用户变量（环境变量）</code>的 path，将默认的 C 盘下 <code>APPData\Roaming\npm</code> 修改成  </p><p><code>C:\Program Files\nodejs\node_global</code>，点击确定。</p><p>最后在系统 <code>Path</code>里面添加<code>%NODE_PATH%</code> 。执行 <code>npm root -g</code> 查看位置是否变更。</p><p>⑦ 测试：配置完成后，安装个module测试下，咱们就安装最经常使用的express模块，打开cmd窗口，输入以下命令进行模块的全局安装：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> express -g   // -g是全局安装的意思</code></pre><hr><h4 id="下载安装-Git"><a href="#下载安装-Git" class="headerlink" title="下载安装 Git"></a>下载安装 Git</h4><p>下载官网：<a href="https://git-scm.com/book/zh/v2/%E8%B5%B7%E6%AD%A5-%E5%AE%89%E8%A3%85-Git">Git - 安装 Git (git-scm.com)</a>。</p><p>安装参考教程：<a href="https://zhuanlan.zhihu.com/p/443527549">Git下载安装教程：git安装步骤手把手图文【超详细】 - 知乎 (zhihu.com)</a>。</p><p>安装参考教程：<a href="https://blog.csdn.net/mukes/article/details/115693833">Git 详细安装教程（详解 Git 安装过程的每一个步骤）_git安装-CSDN博客</a>。</p><p>安装完成后，Win+R 输入 cmd ，输入 <code>node -v</code>、<code>npm -v</code> 和 <code>git --version</code> 并回车，查看程序版本号。</p><hr><h4 id="配置-Github"><a href="#配置-Github" class="headerlink" title="配置 Github"></a>配置 Github</h4><p>使用邮箱注册 <a href="https://link.zhihu.com/?target=https://github.com/">GitHub</a> 账户，选择免费账户（Free），并完成邮件验证。</p><p>右键 -&gt; Git Bash Here，<strong>设置用户名和邮箱</strong>：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">git</span> config --global user.name <span class="token string">"GitHub 用户名"</span><span class="token function">git</span> config --global user.email <span class="token string">"GitHub 邮箱"</span><span class="token comment" spellcheck="true"># 查看用户名及邮箱</span><span class="token function">git</span> config user.name <span class="token function">git</span> config user.email</code></pre><p><strong>创建 SSH 密匙</strong>：</p><p>输入 <code>ssh-keygen -t rsa -C &quot;GitHub 邮箱&quot;</code>，然后一路回车。</p><pre class=" language-bash"><code class="language-bash">ssh-keygen -t rsa -C <span class="token string">"GitHub 邮箱"</span></code></pre><p><strong>添加密匙</strong>：</p><p>进入 [C:\Users\用户名.ssh] 目录（要勾选显示“隐藏的项目”），用记事本打开公钥 id_rsa.pub 文件并复制里面的内容。</p><p>登陆 GitHub ，进入 Settings 页面，选择左边栏的 SSH and GPG keys，点击 New SSH key。Title 随便取个名字，粘贴复制的 id_rsa.pub 内容到 Key 中，点击 Add SSH key 完成添加。</p><p><strong>验证连接</strong>：</p><p>打开 Git Bash，输入 <code>ssh -T git@github.com</code> 出现 “Are you sure……”，输入 yes 回车确认。</p><pre class=" language-bash"><code class="language-bash"><span class="token function">ssh</span> -T git@github.com</code></pre><p>显示 “Hi xxx! You’ve successfully……” 即连接成功。</p><h4 id="创建公有仓库"><a href="#创建公有仓库" class="headerlink" title="创建公有仓库"></a>创建公有仓库</h4><p>GitHub 主页右上角加号 -&gt; New repository：</p><ul><li>Repository name 中输入 <code>用户名.github.io</code> </li><li>勾选 “Initialize this repository with a README”</li><li>Description 选填</li></ul><p>填好后点击 Create repository 创建，创建后默认自动启用 HTTPS.</p><p>博客地址为：<code>https://用户名.github.io</code>。</p><p>gitee配置Page可参考博客：[在gitee上免费部署静态网站_在gitee上部署静态网站](<a href="https://blog.csdn.net/weixin_38705239/article/details/100161188#:~:text=%E4%BD%A0%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8">https://blog.csdn.net/weixin_38705239/article/details/100161188#:~:text=你可以使用</a> Gitee 来 部署静态 网页。 以下是详细的步骤： 1. 在,2. 将你的 静态 网页代码上传到该仓库中。 3. 在仓库的主页，点击右上角的 “Settings” 进入仓库设置。)。</p><hr><h3 id="2-Hexo"><a href="#2-Hexo" class="headerlink" title="2. Hexo"></a>2. Hexo</h3><p>新建一个文件夹用来存放 Hexo 程序文件，如 <code>MyHexoBlog</code>。cmd 进入该文件夹。</p><h4 id="2-1-安装-Hexo"><a href="#2-1-安装-Hexo" class="headerlink" title="2.1 安装 Hexo"></a>2.1 安装 Hexo</h4><p><strong>使用 npm 一键安装 Hexo 博客程序</strong>：【<strong>若失败，要么开启VPN，要么更换node的镜像源</strong>】</p><pre class=" language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> -g hexo-cli</code></pre><h4 id="2-2-初始化和本地预览"><a href="#2-2-初始化和本地预览" class="headerlink" title="2.2 初始化和本地预览"></a>2.2 初始化和本地预览</h4><p><strong>初始化并安装所需组件</strong>：</p><pre class=" language-bash"><code class="language-bash">hexo init      <span class="token comment" spellcheck="true"># 初始化</span><span class="token function">npm</span> <span class="token function">install</span>    <span class="token comment" spellcheck="true"># 安装组件</span></code></pre><p>若报<code>hexo</code> 未识别，可参考该篇博客：<a href="https://blog.csdn.net/Deng872347348/article/details/121646375">安装hexo时出现的问题：‘hexo‘ 不是内部或外部命令。</a>。</p><p>完成后依次输入下面命令，<strong>启动本地服务器进行预览</strong>：</p><pre class=" language-bash"><code class="language-bash">hexo g   <span class="token comment" spellcheck="true"># 生成页面</span>hexo s   <span class="token comment" spellcheck="true"># 启动预览</span></code></pre><p><strong>访问</strong> <code>http://localhost:4000</code>，出现 Hexo 默认页面，本地博客安装成功！</p><p><strong>Tips：</strong>如果出现页面加载不出来，可能是端口被占用了。Ctrl+C 关闭服务器，运行 <code>hexo server -p 5000</code> 更改端口号后重试。Hexo 博客文件夹目录结构如下：</p><p><img src="https://pic1.zhimg.com/80/v2-264c75c0e493ae8cc5f283567c64ff2c_720w.webp"></p><h4 id="2-3-部署到-GitHub-Pages"><a href="#2-3-部署到-GitHub-Pages" class="headerlink" title="2.3 部署到 GitHub Pages"></a>2.3 部署到 GitHub Pages</h4><p>本地博客测试成功后，就是上传到 GitHub 进行部署，使其能够在网络上访问。</p><p>首先<strong>安装 hexo-deployer-git</strong>：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> hexo-deployer-git --save</code></pre><p>然后<strong>修改 _config.yml</strong> 文件末尾的 Deployment 部分，修改成如下：</p><pre class=" language-bash"><code class="language-bash">deploy:  type: <span class="token function">git</span>  repository: git@github.com:flyboy716/flyboy716.github.io.git  branch: master</code></pre><p>完成后运行 <code>hexo d</code> 将网站上传部署到 GitHub Pages。</p><pre class=" language-bash"><code class="language-bash">hexo d</code></pre><p>完成！这时访问我们的 GitHub 域名 <code>https://用户名.github.io</code> 就可以看到 Hexo 网站了。</p><hr><h4 id="2-4-绑定域名-可选"><a href="#2-4-绑定域名-可选" class="headerlink" title="2.4 绑定域名-可选"></a>2.4 绑定域名-可选</h4><p>博客搭建完成使用的是 GitHub 的子域名（用户名.<a href="https://link.zhihu.com/?target=http://github.io">http://github.io</a>），我们可以为 Hexo 博客绑定自己的域名替换 GitHub 域名，更加个性化和专业，也利于 SEO。</p><p>我们使用 <a href="https://link.zhihu.com/?target=https://www.namesilo.com/?rid=d27fa32do">Namesilo</a> 进行注册，便宜好用没啥套路，使用优惠码 <code>okoff</code> 优惠一美元，com 域名大概 50 块一年。</p><p><strong>域名注册和解析</strong>：</p><ul><li>域名注册和解析教程：<a href="https://zhuanlan.zhihu.com/p/33921436">Namesilo 域名购买及使用教程</a></li></ul><p>按上面教程注册并解析域名，在 DNS 设置部分，删除自带的记录，然后添加 CNAME 记录将 www 域名解析指向 <code>用户名.github.io</code>。</p><p><strong>绑定域名到 Hexo 博客</strong>：</p><p>进入本地博客文件夹的 source 目录，打开记事本，里面输入自己的域名，如 <a href="http://www.example.com,保存名称为/">http://www.example.com，保存名称为</a> “CNAME”，格式为 “所有文件”（无 .txt 后缀）。</p><p>清除缓存等文件并重新发布网站：</p><pre class=" language-bash"><code class="language-bash">hexo clean   <span class="token comment" spellcheck="true"># 清除缓存文件等</span>hexo g       <span class="token comment" spellcheck="true"># 生成页面</span>hexo s       <span class="token comment" spellcheck="true"># 启动预览</span></code></pre><p>现在就可以使用自己的域名访问 Hexo 博客了。</p><p><strong>开启 HTTPS</strong>：配置自己的域名后，需要我们手动开启 HTTPS。打开博客所在 GitHub 仓库，Settings -&gt; 下拉找到 GitHub Pages -&gt; 勾选 Enforce HTTPS。</p><p>HTTPS 证书部署成功需要一定时间，等大概几分钟再访问域名，就可以看到域名前面的小绿锁了，HTTPS 配置完成！</p><hr><h4 id="2-5-开始使用"><a href="#2-5-开始使用" class="headerlink" title="2.5 开始使用"></a>2.5 开始使用</h4><p><strong>发布文章</strong>：</p><p>进入博客所在目录，右键打开 Git Bash Here，创建博文：</p><pre class=" language-bash"><code class="language-bash">hexo new <span class="token string">"My New Post"</span></code></pre><p>然后 source 文件夹中会出现一个 My New Post.md 文件，就可以使用 Markdown 编辑器在该文件中撰写文章了。</p><p>写完后运行下面代码将文章渲染并部署到 GitHub Pages 上完成发布。<strong>以后每次发布文章都是这两条命令</strong>。</p><pre class=" language-bash"><code class="language-bash">hexo g   <span class="token comment" spellcheck="true"># 生成页面</span>hexo d   <span class="token comment" spellcheck="true"># 部署发布</span></code></pre><hr><p>也可以不使用命令自己创建 .md 文件，只需在文件开头手动加入如下格式 Front-matter 即可，写完后运行 <code>hexo g</code> 和 <code>hexo d</code> 发布。</p><pre class=" language-bash"><code class="language-bash">---title: Hello World <span class="token comment" spellcheck="true"># 标题</span>date: 2019/3/26 hh:mm:ss <span class="token comment" spellcheck="true"># 时间</span>categories: <span class="token comment" spellcheck="true"># 分类</span>- Diarytags: <span class="token comment" spellcheck="true"># 标签</span>- PS3- Games---摘要<span class="token operator">&lt;</span><span class="token operator">!</span>--more--<span class="token operator">></span>正文</code></pre><p><strong>网站设置</strong>：</p><p>包括网站名称、描述、作者、链接样式等，全部在网站目录下的 _config.yml 文件中，参考<a href="https://link.zhihu.com/?target=https://hexo.io/zh-cn/docs/configuration">官方文档</a>按需要编辑。注意：冒号后要加一个空格！</p><p><strong>更换主题</strong>：</p><p>在 <a href="https://link.zhihu.com/?target=https://hexo.io/themes/">Themes | Hexo</a> 选择一个喜欢的主题，比如 <a href="https://link.zhihu.com/?target=http://theme-next.iissnan.com/getting-started.html">NexT</a>，进入网站目录打开 Git Bash Here 下载主题：</p><pre class=" language-text"><code class="language-text">git clone https://github.com/iissnan/hexo-theme-next themes/next</code></pre><p>然后修改 _config.yml 中的 theme 为新主题名称 next，发布。（有的主题需要将 _config.yml 替换为主题自带的，参考主题说明。）</p><h4 id="2-6-常用命令"><a href="#2-6-常用命令" class="headerlink" title="2.6 常用命令"></a>2.6 常用命令</h4><pre class=" language-text"><code class="language-text">hexo new "name"       # 新建文章hexo new page "name"  # 新建页面hexo g                # 生成页面hexo d                # 部署hexo g -d             # 生成页面并部署hexo s                # 本地预览hexo clean            # 清除缓存和已生成的静态文件hexo help             # 帮助</code></pre><h4 id="2-7-常见问题"><a href="#2-7-常见问题" class="headerlink" title="2.7 常见问题"></a>2.7 常见问题</h4><p><strong>1、Hexo 设置显示文章摘要，首页不显示全文</strong>。</p><p>Hexo 主页文章列表默认会显示文章全文，浏览时很不方便，可以在文章中插入 <code>&lt;!--more--&gt;</code> 进行分段。</p><p>该代码前面的内容会作为摘要显示，而后面的内容会替换为 “Read More” 隐藏起来。</p><p><strong>2、设置网站图标</strong>。</p><p>进入 themes/主题 文件夹，打开 _config.yml 配置文件，找到 favicon 修改，一般格式为：<code>favicon: 图标地址</code>。（不同主题可能略有差别）</p><p><strong>3、修改并部署后没有效果</strong>。</p><p>使用 <code>hexo clean</code> 清理后重新部署。</p><p><strong>4、开启 HTTPS 后访问网站显示连接不安全？</strong></p><p>证书还未部署生效，等待一会儿，清除浏览器缓存再试。</p><p><strong>5、Mac 安装 Hexo 报错无法安装</strong>。</p><p>Mac 用户需要管理员权限运行，使用 <code>sudo npm install -g hexo-cli</code> 命令安装。</p><p><strong>6、npm 下载速度慢，甚至完全没反应</strong>。</p><p>使用 npm 安装程序等待很久也没反应，或者下载速度很慢，可以更换 npm 源为国内 npm 镜像。</p><p>临时更换方法：在 npm 安装命令后面加上：</p><pre class=" language-text"><code class="language-text">--registry https://registry.npm.taobao.org </code></pre><hr><h4 id="2-8-结语"><a href="#2-8-结语" class="headerlink" title="2.8 结语"></a>2.8 结语</h4><p>Hexo 是一种纯静态的博客，我们必须要在本地完成文章的编辑再部署到 GitHub 上，依赖于本地环境。不能像 WordPress 或 Typecho 那样的动态博客一样能直接在浏览器中完成撰文和发布。</p><p>可以说是一种比较极客的写博客方式，但是优势也是明显的——免费稳定省心，比较适合爱折腾研究的用户，或者没有在线发文需求的朋友。</p><hr><h3 id="3-主题"><a href="#3-主题" class="headerlink" title="3. 主题"></a>3. 主题</h3><h4 id="3-1-Matery"><a href="#3-1-Matery" class="headerlink" title="3.1 Matery"></a>3.1 Matery</h4><p>官方参考地址：<a href="https://github.com/blinkfox/hexo-theme-matery/blob/develop/README_CN.md">hexo-theme-matery/README_CN.md</a>。</p><p><a href="https://blinkfox.github.io/2018/09/28/qian-duan/hexo-bo-ke-zhu-ti-zhi-hexo-theme-matery-de-jie-shao/#searchModal">Hexo博客主题之hexo-theme-matery的介绍 | 闪烁之狐 </a>。</p><hr>]]></content>
      
      
      <categories>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 博客 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
