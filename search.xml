<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Alex</title>
      <link href="posts/53ff.html"/>
      <url>posts/53ff.html</url>
      
        <content type="html"><![CDATA[<h1 id="ALEX-An-Updatable-Adaptive-Learned-Index"><a href="#ALEX-An-Updatable-Adaptive-Learned-Index" class="headerlink" title="ALEX: An Updatable Adaptive Learned Index"></a>ALEX: An Updatable Adaptive Learned Index</h1><hr><h2 id="整体数据结构设计"><a href="#整体数据结构设计" class="headerlink" title="整体数据结构设计"></a>整体数据结构设计</h2><p>ALEX旨在实现以下目标：</p><ol><li>插入时间应与 B+Tree 相当;</li><li>查找时间应比 B+Tree 和 Learned Index 快;</li><li>索引存储空间应小于 B+Tree 和 Learned Index;</li><li>数据存储空间(叶级)应与 动态B+Tree 相当。</li></ol><p><img src="/posts/53ff.htm/image-20240412211819896.png" alt="image-20240412211819896"></p><hr><h3 id="设计概览"><a href="#设计概览" class="headerlink" title="设计概览"></a>设计概览</h3><p>ALEX：内存中的可更新的学习型索引。但与学习索引亦有许多不同之处：</p><p>区别一：叶节点的数据结构。像B+Tree一样，ALEX在每个叶子上使用一个节点。这允许更灵活地扩展和分割单个节点，并且还限制了插入期间所需的移位次数。在典型的B+树中，每个叶节点存储一个键和有效负载数组，并且在数组末尾有“空闲空间”来吸收插入。ALEX使用了类似的设计，但更仔细地选择了如何使用空闲空间。通过在数组元素之间策略性地引入间隙，可以实现更快的插入和查找时间。如图2所示，ALEX对每个数据节点使用间隙数组(GA)布局。</p><p>区别二：ALEX使用指数搜索来搜索叶级键，以纠正RMI的错误预测，如图2所示。我们通过实验验证了无边界的指数搜索比有边界的二叉搜索更快。这是因为如果模型是好的，它们的预测就足够接近正确的位置。指数搜索还消除了在RMI模型中存储错误边界的需要。</p><p>区别三：ALEX在模型预测键应该在的位置将键插入数据节点。我们称之为基于模型的插入。相反，学习索引在不改变数组中记录位置的情况下对记录数组生成RMI。基于模型的插入具有更好的搜索性能，因为它减少了模型误预测误差。</p><p>区别四：ALEX根据工作量动态调整RMI的形状和高度。</p><p>区别五：ALEX没有需要为每个数据集或工作负载重新调整的参数，不像学习索引，其中必须调整模型的数量。ALEX通过使用 cost模型 自动批量加载和调整 RMI 结构以实现高性能。</p><hr><h3 id="节点布局"><a href="#节点布局" class="headerlink" title="节点布局"></a>节点布局</h3><h4 id="数据节点"><a href="#数据节点" class="headerlink" title="数据节点"></a>数据节点</h4><p>与B+树一样，ALEX的叶节点存储数据记录，因此被称为数据节点，如图2中的圆圈所示。</p><p>数据节点<strong>存储一个线性回归模型</strong>(斜率和截距的两个双值)，它将一个键映射到一个位置，<strong>以及两个gap数组</strong>，<strong>一个用于键，一个用于有效载荷</strong>。我们在图2中只显示了keys数组。默认情况下，键和有效负载都是固定大小的。(注意，有效负载可以是记录或指向可变大小记录的指针，存储在内存中单独分配的空间中)。出于实际原因，我们还强制设置了最大节点大小。</p><p>ALEX使用间隙数组布局，它使用基于模型的插入来分配数组元素之间的额外空间，从而实现更快的插入和查找。相反，B+Tree将所有的空格放在数组的末尾。</p><p><strong>间隙数组用间隙右边最近的键来填充间隙，这有助于保持指数搜索性能</strong>。</p><p><strong>为了在扫描时有效地跳过间隙，每个数据节点维护一个位图，该位图跟踪节点中的每个位置是否被键占用或是否为间隙。与间隙数组相比，位图查询速度快，空间开销低</strong>。</p><hr><h4 id="内部节点"><a href="#内部节点" class="headerlink" title="内部节点"></a>内部节点</h4><p>我们将作为RMI结构部分的所有节点称为内部节点，如图2中矩形所示。内部节点存储一个线性回归模型和一个包含指向子节点指针的数组。像B+树一样，内部节点直接遍历树，<strong>但与B+树不同的是，ALEX中的内部节点使用模型来“计算”指针数组中下一个子指针的位置</strong>。与数据节点类似，我们施加了一个最大节点大小。</p><p>ALEX 的内部节点在概念上与学习索引的目的不同。 LearnedIndex的内部节点有适合数据的模型；<strong>具有完美模型的内部节点将键平均分配给其子节点，并且具有完美内部节点的 RMI 会导致每个数据节点中的键数量相同</strong>。然而，<strong>RMI结构的目标不是产生相同大小的数据节点，而是产生键分布大致线性的数据节点，以便线性模型可以精确地拟合其键</strong>。</p><p>因此，ALEX中内部节点的作用就是提供一种灵活的方式来划分键空间。假设图3中的内部节点A覆盖了键空间[0,1)并且有四个子指针。 LearnedIndex 将为每个指针分配一个节点，可以是所有内部节点，也可以是所有数据节点。然而，ALEX 更灵活地划分空间。内部节点A将键空间[0,1/4)和[1/2,1)分配给数据节点（因为这些空间中的CDF是线性的），并将[1/4,1/2)分配给另一个内部节点（因为 CDF 是非线性的，并且 RMI 需要对该key空间进行更精细的划分）。<strong>如图所示，多个指针可以指向同一个子节点</strong>。</p><p>将每个内部节点中的指针数量始终限制为 2 的幂。这允许节点在不重新训练其子树的情况下进行分裂。</p><p><img src="/posts/53ff.htm/image-20240412211959414.png" alt="image-20240412211959414"></p><hr><h2 id="算法设计"><a href="#算法设计" class="headerlink" title="算法设计"></a>算法设计</h2><h3 id="查找和范围查找"><a href="#查找和范围查找" class="headerlink" title="查找和范围查找"></a>查找和范围查找</h3><p>为了查找键，从 RMI 的根节点开始，迭代地使用模型来“计算”指针数组中的位置，然后沿着指针指向下一级的子节点，直到到达数据节点。通过构建，内部节点模型具有完美的精度，因此内部节点不涉及搜索。我们使用数据节点中的模型来预测键中搜索键的位置阵列，如果需要，进行指数搜索找到 key 的实际位置。如果找到键，我们从有效负载数组中读取同一位置的相应值并返回记录。否则，返回一个空记录。图2中直观地显示了（使用红色箭头）查找。范围查询首先执行查找以查找值不小于范围起始值的第一个键的位置和数据节点，然后向后扫描直到到达范围的结束值，<strong>使用节点的位图跳过间隙</strong>，并在必要时使用存储在节点中的指针跳转到下一个数据节点。【基于模型的查找O(1)+指数搜索】</p><p><img src="/posts/53ff.htm/image-20240412212047079.png" alt="image-20240412212047079"></p><hr><h3 id="插入未满的数据节点"><a href="#插入未满的数据节点" class="headerlink" title="插入未满的数据节点"></a>插入未满的数据节点</h3><p>对于插入算法，到达正确数据节点（即TraverseToLeaf）的逻辑与上述查找算法相同。在未满数据节点中，为了找到新元素的插入位置，可使用数据节点中的模型来预测插入位置。如果预测位置不正确（插入到那里将无法保持排序顺序），则会进行指数搜索来找到正确的插入位置。</p><p>如果插入位置是一个间隙，则直接将元素插入到间隙中。<strong>否则，便通过将元素在最近间隙的方向上移动一个位置以在插入位置处形成一个间隙</strong>。然后将元素插入到新创建的间隙中。间隙数组以高概率实现 O(logn) 插入时间。</p><hr><h3 id="插入已满的数据节点"><a href="#插入已满的数据节点" class="headerlink" title="插入已满的数据节点"></a>插入已满的数据节点</h3><p>当数据节点已满时，ALEX 使用两种机制来创建更多空间：扩展和拆分。 ALEX 依靠简单的 cost 模型在不同的机制之间进行选择。</p><h4 id="节点饱满度的标准"><a href="#节点饱满度的标准" class="headerlink" title="节点饱满度的标准"></a>节点饱满度的标准</h4><p>ALEX 不会等待数据节点达到 100% 满，因为间隙数组上的插入性能会随着间隙数量的减少而恶化。我们引入间隙数组的密度下限和上限：d<del>l</del>,d<del>u</del> ∈ (0,1]，约束条件是 d<del>l</del>&lt;d<del>u</del>。</p><p>密度定义为被元素填充的位置的分数。如果下一次插入导致超过 d<del>u</del> ，则节点已满。默认情况下，我们设置 d<del>l</del>=0.6 和 d<del>u</del>=0.8，以实现平均数据存储利用率 0.7，类似于 B+Tree [14]，根据我们的实验，这总是会产生良好的结果，并且不需要调整。相反，B+Tree 节点通常具有 d<del>l</del>=0.5 和 d<del>u</del>=1。</p><h4 id="节点扩展机制"><a href="#节点扩展机制" class="headerlink" title="节点扩展机制"></a>节点扩展机制</h4><p>为了扩展包含 n 个键的数据节点，我们分配一个具有 n/d<del>l</del> 个槽的新的更大的间隙数组。然后，我们缩放或重新训练线性回归模型，然后使用缩放或重新训练的模型对这个新的较大节点中的所有元素进行基于模型的插入。创建后，新数据节点处于密度下限 d<del>l</del> 。</p><h4 id="节点分裂机制"><a href="#节点分裂机制" class="headerlink" title="节点分裂机制"></a>节点分裂机制</h4><p>为了将数据节点一分为二，即将 key 分配给两个新的数据节点，使得每个新节点负责原始节点的 key 空间的一半。 ALEX支持两种分割节点的方式：</p><ol><li><p><strong>横向拆分</strong>在概念上类似于 B+Tree 使用拆分的方式。有两种情况：</p><p>a）如果分裂数据节点的父内部节点尚未达到最大节点大小，则将父节点指向分裂数据节点的指针替换为指向两个新数据节点的指针。父内部节点的指针数组可能有指向拆分数据节点的冗余指针。如果是这样，我们将一半的冗余指针分配给两个新节点中的每一个。否则，我们通过将父节点指针数组的大小加倍并为每个指针制作冗余副本来创建指向拆分数据节点的第二个指针，然后将冗余指针之一赋予两个新节点中的每一个。图 5a 显示了不需要扩展父内部节点的横向拆分的示例。 </p><p>b) 如果父内部节点已达到最大节点大小，则可以选择拆分父内部节点，如图 5b 所示。请注意，通过将所有内部节点大小限制为 2 的幂，便始终以“边界保留”方式分割节点，因此不需要重新训练分割内部节点下方的任何模型。请注意，分裂可以一直传播到根节点，就像在 B+ 树中一样。</p></li></ol><p><img src="/posts/53ff.htm/image-20240412212101573.png" alt="image-20240412212101573"></p><ol start="2"><li>纵向拆分将数据节点转换为具有两个子数据节点的内部节点，如图 5c 所示。<strong>两个子数据节点中的模型根据各自的键进行训练</strong>。 B+Tree 没有类似的分裂机制。</li></ol><p><img src="/posts/53ff.htm/image-20240412212111535.png" alt="image-20240412212111535"></p><hr><h4 id="cost-模型"><a href="#cost-模型" class="headerlink" title="cost 模型"></a>cost 模型</h4><p>为了决定应用哪种机制，ALEX 依赖于简单的线性 cost 模型，该模型根据在每个数据节点跟踪出来的两个简单统计数据 来预测平均查找时间和插入时间：（a）指数搜索迭代的平均数; （b) 插入的平均移位次数。查找性能与（a）直接相关，而插入性能与（a）和（b）直接相关（因为插入首先需要进行查找以找到正确的插入位置）。</p><p><strong>这两个统计数据在创建数据节点时是未知的。为了找到新数据节点的预期 cost ，我们在假设对现有键统一进行查找并根据现有键分布进行插入的情况下计算这些统计数据的预期值</strong>。</p><p>具体来说：</p><p>(a) 被计算为所有键的模型预测误差的平均以 2 为底的对数； </p><p>(b) 计算所有现有键到间隙数组中最近间隙的平均距离。</p><p>这些期望值可以在不创建数据节点的情况下计算。如果数据节点是使用现有数据节点的键子集创建的，我们可以使用查找与插入的经验比率来衡量两个统计数据的相对重要性以进行计算预期 cost 。</p><p>除了节点内 cost 模型之外，ALEX 还使用 TraverseToLeaf cost 模型来预测从根节点遍历到数据节点的时间。 TraverseToLeaf cost 模型使用两个统计数据：</p><ol><li>遍历到的数据节点的深度；</li><li>所有内部节点和数据节点元数据（除了key和有效负载之外的所有内容）的总大小（以B为单位）。</li></ol><p>这些统计数据捕获了遍历的 cost ：较深的数据节点需要更多的指针追踪来查找，较大的大小会降低 CPU 缓存局部性，从而减慢对数据节点的遍历。我们提供有关 cost 模型的更多详细信息并在 [10] 的附录 D 中显示了它们的低使用开销。【<strong>计算叶节点深度，以及遍历到叶节点所需公共元数据大小</strong>】</p><hr><h4 id="插入算法"><a href="#插入算法" class="headerlink" title="插入算法"></a>插入算法</h4><p>当查找和插入在数据节点上完成时，就计算指数搜索迭代和每次插入的移位 次数。<strong>根据这些统计数据，我们使用节点内 cost 模型计算数据节点的经验 cost **。</strong>一旦数据节点已满，就将预期 cost （在节点创建时计算）与经验 cost 进行比较。如果它们没有显著偏离，那么就断定模型仍然准确，就执行节点扩展（如果扩展后的大小小于最大节点大小），缩放模型而不是重新训练**。 RMI 内部节点中的模型不会重新训练或重新缩放。</p><p><strong>显著 cost 偏差定义为：经验 cost 比预期 cost 高出 50% 以上</strong>。根据我们的实验，50% 的 cost 偏差阈值始终会产生良好的结果，无需进行调整。</p><p>否则，<strong>如果经验 cost 偏离了预期 cost ，就必须（i）扩展数据节点并重新训练模型，（ii）横向分割数据节点，或（iii）纵向分割数据节点。根据节点内 cost 模型，选择导致最低预期 cost 的操作</strong>。</p><p><strong>为简单起见，ALEX 始终将数据节点一分为二。数据节点在概念上可以拆分为 2 的任意幂，但确定最佳扇出可能非常耗时，我们通过实验验证，在大多数情况下，根据 cost 模型，2 的扇出是最佳的</strong>。</p><hr><h4 id="为什么经验-cost-会偏离预期-cost-？"><a href="#为什么经验-cost-会偏离预期-cost-？" class="headerlink" title="为什么经验 cost 会偏离预期 cost ？"></a>为什么经验 cost 会偏离预期 cost ？</h4><p>当 插入键 的分布不遵循现有键的分布时，通常会发生这种情况，从而导致模型变得不准确。不准确的模型可能会导致没有任何间隙的长连续区域。插入这些完全填充的区域需要移动其中最多一半的元素以创建间隙，在最坏的情况下这需要 O(n) 时间。随着节点变大，或由于查找的访问模式发生变化，性能也可能仅仅由于随机噪声而降低。</p><hr><h3 id="删除、更新"><a href="#删除、更新" class="headerlink" title="删除、更新"></a>删除、更新</h3><p>要删除key，则进行查找以找到key的位置，然后删除它及其有效负载。删除不会移动任何现有键，因此删除是比插入更简单的操作，并且不会导致模型准确性降低。如果数据节点由于删除而达到密度下限 d<del>l</del>，则 <strong>收缩</strong> 该数据节点以避免空间利用率低。此外，使用节点内 cost 模型来确定两个数据节点应该合并在一起并可能向上增长，从而局部将 RMI 深度减少 1。但是，为了简单起见，我们不实现这些合并操作。</p><p>【<strong>如何收缩？简单起见，不合并，要合并怎么办？效果会好吗</strong>？】</p><p>更新：修改键是通过组合插入和删除来实现的。仅修改有效负载的更新将查找 key 并将新值写入有效负载。</p><hr><h3 id="处理越界插入"><a href="#处理越界插入" class="headerlink" title="处理越界插入"></a>处理越界插入</h3><p>低于或高于现有键空间的键将分别插入到最左边或最右边的数据节点中。一系列越界插入（如仅追加插入工作负载）将导致性能不佳，因为该数据节点没有分裂越界键空间的机制。因此，ALEX有两种方式可以顺利处理越界插入。</p><p>首先，当检测到现有键空间之外的插入时，ALEX 将扩展根节点，从而扩展键空间，如图 6 所示。可将子指针数组的大小向右扩展。指向现有子项的现有指针不会被修改。为扩展的指针数组中的每个新槽创建一个新的数据节点。若此扩展导致根节点超过最大节点大小，ALEX 将创建一个新的根节点。新根节点的第一个子指针将指向旧根节点，并为新根节点的其他每个指针槽创建一个新数据节点。最后，越界键将落入新创建的数据节点之一。</p><p><img src="/posts/53ff.htm/image-20240412212122272.png" alt="image-20240412212122272"></p><p>其次，ALEX 最右边的数据节点通过 维护节点中最大键的值 并 保留插入超过最大值的计数器 来检测追加插入行为。如果大多数【<strong>是根据计数器大小判断</strong>？】插入超过最大值，则意味着仅追加行为，因此数据节点向右扩展，而不进行基于模型的重新插入；扩展的空间最初保持为空，以期待更多类似附加的插入。</p><hr><h3 id="批量加载"><a href="#批量加载" class="headerlink" title="批量加载"></a>批量加载</h3><p>ALEX 支持批量加载操作，该操作在实践中用于在初始化时索引大量数据或重建索引。我们的目标是找到一个 cost 最小的 RMI 结构，cost 定义为在此 RMI 上执行操作（即查找或插入）的预期平均时间。任何 ALEX 操作都是由到数据节点的 TraverseToLeaf 和节点内操作组成，因此 RMI cost 是通过结合 TraverseToLeaf 和节点内 cost 模型来建模的。</p><h4 id="批量加载算法"><a href="#批量加载算法" class="headerlink" title="批量加载算法"></a>批量加载算法</h4><p>使用 cost 模型，从根节点开始贪婪地向下增长 RMI。在每个节点，独立地决定该节点应该是数据节点还是内部节点，以及在后一种情况下，扇出应该是什么。<strong>扇出必须是2的幂，子节点将平分当前节点的key空间</strong>。</p><p>在每个节点本地就可做出此决策，因为使用线性 cost 模型，决策将对 RMI 的总体 cost 产生纯粹的累加效应。如果该节点是内部节点，则将在其每个子节点上递归。这将持续下去，直到所有数据都加载到 ALEX 中。</p><p><img src="/posts/53ff.htm/image-20240412212129080.png" alt="image-20240412212129080"></p><h4 id="扇出树"><a href="#扇出树" class="headerlink" title="扇出树"></a>扇出树</h4><p>随着 RMI 的发展，主要挑战是确定每个节点的最佳扇出。</p><p>引入扇出树（FT）概念，它是一棵完全二叉树。 FT 将帮助决定单个 RMI 节点的扇出；在批量加载算法中，每次决定 RMI 节点的最佳扇出时，都构造一个 FT 树。<strong>扇出为 1 意味着 RMI 节点应该是数据节点</strong>。</p><p>图 7 显示了 FT 示例。每个 FT 节点代表 RMI 节点的一个可能的子节点。如果 RMI 节点的 key 空间为[0,1)，则在某个具有 n 个节点的层级上，第 i 个 FT 节点表示 key 空间为 [i/n,(i+1)/n) 的 子RMI节点。每个 FT 节点都与在其 key 空间上构建数据节点的预期 cost 相关联，如节点内 cost 模型所预测的那样。目标是以最小的总体 cost 找到一组覆盖 RMI 节点整个 key 空间的 FT 节点。</p><p>覆盖集的总体 cost 是其 FT 节点 cost 以及由于模型大小而导致的 TraverseToLeaf cost 之和（如在 FT 中深入一层意味着 RMI 节点必须具有两倍的指针）。该覆盖集决定了 RMI 节点的最佳扇出（即子指针的数量）以及分配子指针的最佳方式。</p><p>使用以下方法来找到低 cost 的覆盖集：</p><ol><li>从FT根开始，一次增长FT的整个级别，并计算使用每个级别作为覆盖集的 cost 。继续这样做，直到每个连续级别的 cost 开始增加。在图 7 中，级别 2 的综合 cost 最低，则不再继续增长级别 3 之后的。从概念上讲，更深的级别可能具有较低的 cost ，但计算每个 FT 节点的 cost 是昂贵的。 </li></ol><ol start="2"><li>从组合 cost 最低的 FT 级别开始，开始本地合并或分裂 FT 节点。如果两个相邻 FT 节点的 cost 高于其父节点的 cost ，则合并（例如， cost 为 20 和 25 的节点合并为 cost 为 40 的节点）；当两个节点的键很少或它们的分布相似时，可能会发生这种情况。另一方面，如果 FT 节点的 cost 高于其两个子节点的 cost ，则分割 FT 节点（例如， cost 为 10 的节点被分割为两个 cost 为 1 的节点）；当 key 空间的两半具有不同的分布时，可能会发生这种情况。继续在本地合并和分裂相邻节点的过程，直到不再可能为止。</li></ol><p>最后，返回生成的 FT 节点覆盖集。</p><p>【<strong>大大的问题：经验 cost 和预期 cost 是怎么计算的，FT树的每个层级中的 cost 是怎么计算的？怎么和RMI对应的？该问题中，层级2 cost 最小，完事怎么又合并分裂了</strong>？】</p><hr><h2 id="ALEX分析"><a href="#ALEX分析" class="headerlink" title="ALEX分析"></a>ALEX分析</h2><h3 id="RMI-深度的限制"><a href="#RMI-深度的限制" class="headerlink" title="RMI 深度的限制"></a>RMI 深度的限制</h3><p>让 m 为最大节点大小，以槽数定义（在内部节点的指针数组中，在数据节点的间隙数组中）。将节点大小限制为 2 的幂： m = 2^k^ 。内部节点最多可以有 m 多个子指针，数据节点包含的键不得超过 md<del>u</del> 个。让所有要索引的键都落在键空间 s 内。令 p 为分区的最小数量，使得当 key 空间 s 被划分为 p 个宽度相等的分区时，每个分区包含的 key 不超过 md<del>u</del> 个。定义根节点深度为0。</p><p>定理5.1：可以构造一个满足最大节点大小和密度上限约束的 RMI，其深度不大于 ⌈log<del>m</del>p⌉——称之为最大深度。此外，可以保持插入下的最大深度。（请注意，p 在插入下可能会发生变化）</p><p>换句话说，RMI 的深度受到 <strong>s 最密集子区域的密度的限制</strong>。相比之下，B+树将深度限制为键数量的函数。定理 5.1 也可以应用于 s 内的子空间，它对应于 RMI 内的某个子树。</p><p>证明。构建具有最大深度的 RMI 非常简单。跨越大小为 |s|/p 的键空间的<strong>最密集子区域</strong>【<strong>如何甄别</strong>？】被分配给数据节点。从根到这个最密集区域的遍历路径由内部节点组成，每个内部节点有 m 个子指针。它需要 ⌈log<del>m</del>p⌉ 内部节点来缩小 |s| 的 key 空间大小至 |s|/p。为了最小化 RMI 的其他子树的深度，我们将这种构造机制递归地应用于空间 s 的其余部分。</p><p>从满足最大深度的 RMI 开始，我们使用 4.3 节中的机制根据以下策略维持最大深度： (1) 数据节点扩展直到达到最大节点大小。 (2) 当数据节点由于最大节点大小而必须分裂时，它会横向分裂以维持当前深度（可能将分裂传播到某个祖先内部节点）。(3) 当不再可能横向分裂时（所有祖先节点都处于最大节点大小），向下分裂。</p><p><strong>通过遵循此策略，RMI 仅当 p 增长 m 倍时才向下分裂，从而保持最大深度</strong>。</p><h3 id="复杂性分析"><a href="#复杂性分析" class="headerlink" title="复杂性分析"></a>复杂性分析</h3><p>查找和插入都在 ⌈log<del>m</del> p⌉ 时间内完成 TraverseToLeaf。在数据节点内，查找的指数搜索在最坏情况下受 O(logm) 限制。在最好的情况下，数据节点模型完美地预测了 key 的位置，并且查找需要 O(1) 时间。</p><p>未满节点的插入由查找组成，后面可能会进行移位以引入新键的间隙。在最坏的情况下，这受到 O(m) 的限制，但由于间隙数组以高概率实现每次插入 O(logm) 次移位，因此预计大多数情况下的复杂性为 O(logm)。在最好的情况下，预测的插入位置是正确的并且是一个间隙，则将 key 准确地放置在模型预测的位置，插入复杂度为 O(1) ；此外，稍后基于模型的查找将导致直接命中 O(1)。</p><p>cost 由必须复制的元素数量来定义： (1)数据节点的扩展，其 cost 以O(m)为界。 (2)向下分裂成两个节点，其 cost 为O(m)。 (3) 横向分裂成两个节点，并在路径中向上传播到某个祖先节点，其 cost 受 O(m⌈log<del>m</del>p⌉) 限制，因为该路径上的每个内部节点也必须分裂。因此，插入全节点的最坏情况性能为 O(m⌈log<del>m</del>p⌉)。</p><hr><h2 id="实验评估"><a href="#实验评估" class="headerlink" title="实验评估"></a>实验评估</h2><p>使用各种数据集和工作负载将 ALEX 与学习索引、B+树、模型增强的 B+树 和 自适应基数树 (ART) 进行比较。此次评估表明：</p><ul><li><p>在只读工作负载上，ALEX 的吞吐量比 B+Tree、学习索引、模型B+树和ART 高出 4.1 倍、2.2 倍、2.9 倍、3.0 倍；索引大小小 800 倍、15 倍、160 倍、8000 倍。</p></li><li><p>在读写工作负载上，ALEX 的吞吐量分别比 B+Tree、模型 B+Tree 和 ART 高 4.0×、2.7×、2.7×，索引大小分别小 2000×、475×、36000×。</p></li><li><p>ALEX 具有竞争力的批量加载时间，并且在扩展到更大的数据集以及由于数据倾斜而导致分布变化时，保持优于其他索引的优势。</p></li><li><p>Gapped Array 和自适应 RMI 结构使 ALEX 能够适应不同的数据集和工作负载。</p></li></ul><hr><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><p>使用来自某些数据集的 8 字节密钥和随机生成的固定大小的有效负载来运行所有实验。我们在 4 个数据集上评估 ALEX，其特征和 CDF 如表 1 和图 8 所示。经度数据集由 Open Street Maps [2] 中世界各地位置的经度组成。 longlat 数据集由复合键组成，这些复合键通过对每对经度和纬度应用变换 k =180·floor(经度)+纬度来组合来自开放街道地图的经度和纬度。由此产生的 keys k 分布是高度非线性的。对数正态数据集的值根据对数正态分布生成，其中 µ =0 且 σ =2，乘以 10^9^ 并向下舍入到最接近的整数。 YCSB 数据集具有表示根据 YCSB 基准 [8] 生成的用户 ID 的值，这些值均匀分布在整个 64 位域中，并使用 80 字节的有效负载。这些数据集不包含重复值。除非另有说明，否则这些数据集会随机打乱以模拟随时间变化的均匀数据集分布.</p><h4 id="工作负载"><a href="#工作负载" class="headerlink" title="工作负载"></a>工作负载</h4><p>我们评估 ALEX 的主要指标是平均吞吐量。我们评估五种工作负载的吞吐量：(1) 只读工作负载，(2) 包含 95% 读取和 5% 插入的读密集型工作负载，(3) 包含 50% 读取和 50% 插入的写密集型工作负载， (4) 具有 95% 读取和 5% 插入的短程查询工作负载，以及 (5) 只写工作负载，以完成读写范围。对于前三个工作负载，读取由单个键的查找组成。对于短程工作负载，读取由键查找和后续键扫描组成。要扫描的键的数量是从最大扫描长度为 100 的均匀分布中随机选择的。对于所有工作负载，要查找的键是根据 Zipfian 分布从索引中现有键的集合中随机选择的。前四个工作负载大致分别对应于 YCSB 基准 [8] 中的工作负载 C、B、A 和 E。对于给定的数据集，我们用 1 亿个键初始化一个索引。然后，我们运行工作负载 60 秒，插入剩余的密钥。我们报告当时完成的操作的吞吐量，其中操作要么是插入，要么是读取。对于读写工作负载，我们交错操作：对于读密集型工作负载和短范围工作负载，我们执行 19 次读取/扫描，然后执行 1 次插入，然后重复该循环；<strong>对于写入密集型工作负载，我们执行 1 次读取，然后执行 1 次插入，然后重复该循环</strong>。</p><hr><h3 id="Drilldown-into-ALEX-Design-Trade-offs"><a href="#Drilldown-into-ALEX-Design-Trade-offs" class="headerlink" title="Drilldown into ALEX Design Trade-offs"></a>Drilldown into ALEX Design Trade-offs</h3><p>在本节中，我们将深入探讨节点布局和自适应 RMI 如何帮助 ALEX 实现其设计目标。</p><p>ALEX 相对于学习索引的部分优势来自于在数据节点中使用基于模型的插入和间隙数组，但 ALEX 对于动态工作负载的大部分优势来自于<strong>自适应 RMI</strong>。</p><p>为了演示每个贡献的效果，图 13 显示，采用 2 层学习索引并用每个叶的间隙数组（LI w/Gapped Array）替换单个密集值数组，对于只读工作负载，已经比学习索引实现了显着的加速。然而，带有间隙数组的学习索引在读写工作负载上的性能较差，因为存在完全填充的区域，需要为每次插入移动许多键。 ALEX 使 RMI 结构适应数据的能力对于良好的插入性能是必要的。</p><p>在查找过程中，大部分时间都花在围绕预测位置进行本地搜索。较小的预测误差直接有助于减少查找时间。为了分析 Learned Index 和 ALEX 的预测误差，我们用经度数据集中的 1 亿个键初始化一个索引，使用该索引来预测这 1 亿个键中每个键的位置，并跟踪预测位置与实际位置之间的距离。图 14a 显示学习索引在模式大约 8-32 个位置处存在预测误差，并且右侧有长尾。另一方面，ALEX 通过使用基于模型的插入实现了低得多的预测误差。</p><p>图14b显示，初始化后，ALEX通常没有预测误差，发生的误差通常很小，并且误差的长尾已经消失。图 14c 显示，即使在 2000 万次插入之后，ALEX 仍保持较低的预测误差。</p><p>一旦数据节点变满，就会发生以下四种操作之一：如果不存在 cost 偏差，则 (1) 扩展节点并缩放模型。否则，节点要么 (2) 扩展并重新训练其模型，(3) 横向分割，或 (4) 向下分割。表 3 显示，在绝大多数情况下，数据节点只是扩展，模型也进行了缩放，这意味着即使在插入之后模型通常仍然保持准确（假设没有发生根本的分布变化）。数据节点变满的次数与数据节点的数量相关（表2）。在YCSB上，通过模型再训练进行扩展更为常见，因为数据节点很大，因此 cost 偏差通常只是由随机性引起的。</p><p>如果需要，用户可以调整最大节点大小以实现目标尾部延迟。在图 15 中，我们在经度数据集上运行写入密集型工作负载，测量每个操作的延迟。随着我们增加最大节点大小，ALEX 的中值甚至 p99 延迟都会减少，因为 ALEX 具有更大的灵活性来构建性能更好的 RMI（例如，具有更高的内部节点扇出的能力）。然而，最大延迟会增加，因为触发大节点扩展或分裂的插入速度很慢。如果用户对延迟有严格要求，则可以相应减小最大节点大小。将最大节点大小增加到超过 64MB 后，延迟不会改变，因为 ALEX 从未决定使用大于 64MB 的节点。</p><hr><h4 id="搜索方法比较"><a href="#搜索方法比较" class="headerlink" title="搜索方法比较"></a>搜索方法比较</h4><p>为了展示指数搜索和其他搜索方法之间的权衡，我们对合成数据进行了微基准测试。我们创建一个包含 1 亿个完全均匀分布的双精度数的数据集。然后，我们从该数据集中搜索 1000 万个随机选择的值。我们使用三种搜索方法：二分搜索和有偏四元搜索（在[20]中提出，以利用准确的预测），每种方法都使用两种不同的误差范围大小进行评估，以及指数搜索。对于每次查找，搜索方法都会得到一个预测位置，该位置与实际位置值的距离存在一定的综合误差量。图 16 显示，指数搜索的搜索时间与错误大小的对数成比例增加，而二分搜索方法花费恒定的时间，而与错误大小无关。这是因为二分查找必须始终在其误差范围内开始搜索，并且无法利用误差较小的情况。因此，如果 ALEX 中 RMI 模型的预测误差较小，指数搜索应该优于二分搜索。正如我们在第 6.3 节中所示，ALEX 通过基于模型的插入保持较低的预测误差。</p><p>因此，ALEX 非常适合利用指数搜索。当误差低于 σ 时（我们在本实验中设置 σ = 8；详细信息请参阅[20]），有向四元搜索与指数搜索具有竞争力，因为搜索可以限制在较小的范围内，但当误差超过 σ 时，其性能与二分搜索类似因为必须搜索完整的错误界限。与有偏差的四元搜索相比，我们更喜欢指数搜索，因为它的性能下降更平滑且实现简单（例如，无需调整 σ）。</p><p>参考博文：<a href="https://zhuanlan.zhihu.com/p/435878936">指数搜索 - 知乎 (zhihu.com)</a> </p><hr><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>我们在学习索引令人兴奋的基础上提出了 ALEX，这是一种新的可更新学习索引，它有效地将学习索引的核心见解与经过验证的存储和索引技术结合起来。具体来说，我们提出了一种间隙数组节点布局，它使用基于模型的插入和指数搜索，结合简单 cost 模型驱动的自适应 RMI 结构，以在动态工作负载上实现高性能和低内存占用。我们深入的实验结果表明，ALEX 不仅在读写工作负载范围内始终优于 B+Tree，甚至在所有数据集上，对于只读工作负载，其性能比现有的学习索引高出 2.2 倍。</p><p>我们相信这篇论文为我们的社区提供了重要的经验教训，并为该领域的未来研究开辟了道路。</p><p>我们打算研究有关 ALEX 性能的开放理论问题，<strong>支持大于内存数据集的二级存储，以及针对 ALEX 设计量身定制的新并发控制技术</strong>。</p><p>【<strong>方向：二级学习型索引，并发控制的学习型索引</strong>】</p><hr><h2 id="代码结构"><a href="#代码结构" class="headerlink" title="代码结构"></a>代码结构</h2><h3 id="类及属性"><a href="#类及属性" class="headerlink" title="类及属性"></a>类及属性</h3><h4 id="alex-base-h"><a href="#alex-base-h" class="headerlink" title="alex_base.h"></a>alex_base.h</h4><h5 id="线性回归模型"><a href="#线性回归模型" class="headerlink" title="线性回归模型"></a>线性回归模型</h5><pre class=" language-c++"><code class="language-c++">template<class T>class LinearModel &#123;public:    double a_ = 0;  // 斜率    double b_ = 0;  // 截距    LinearModel() = default;    LinearModel(double a, double b) : a_(a), b_(b) &#123;&#125;    // 拷贝构造函数    explicit LinearModel(const LinearModel &other) : a_(other.a_), b_(other.b_) &#123;&#125;    // 根据给定的扩展因子（expansion_factor）来扩展线性模型的斜率和截距    void expand(double expansion_factor) &#123;        a_ *= expansion_factor;        b_ *= expansion_factor;    &#125;    // 预测 key 的 position_int    inline int predict(T key) const &#123;        return static_cast<int>(a_ * static_cast<double>(key) + b_);    &#125;    // 预测 key 的 position_double    inline double predict_double(T key) const &#123;        return a_ * static_cast<double>(key) + b_;    &#125;&#125;;</code></pre><h5 id="构造线性回归"><a href="#构造线性回归" class="headerlink" title="构造线性回归"></a>构造线性回归</h5><pre class=" language-c++"><code class="language-c++">template<class T>class LinearModelBuilder &#123;public:    // 指向LinearModel<T>对象的指针，用于存储构建的线性模型    LinearModel <T> *model_;    explicit LinearModelBuilder<T>(LinearModel <T> *model) : model_(model) &#123;&#125;    // 该函数会更新相关的统计信息，包括计数、x和y的总和、x的平方总和、x和y的乘积总和以及x和y的最小值和最大值    inline void add(T x, int y) &#123;        count_++;        x_sum_ += static_cast<long double>(x);y_sum_ += static_cast<long double>(y);        xx_sum_ += static_cast<long double>(x) * x;xy_sum_ += static_cast<long double>(x) * y;        x_min_ = std::min<T>(x, x_min_);x_max_ = std::max<T>(x, x_max_);        y_min_ = std::min<double>(y, y_min_);y_max_ = std::max<double>(y, y_max_);    &#125;private:    // 整型变量，表示已添加的数据点的数量    int count_ = 0;    // 长双精度浮点型变量，表示所有数据点的x值的总和    long double x_sum_ = 0;    // 长双精度浮点型变量，表示所有数据点的y值的总和    long double y_sum_ = 0;    // 长双精度浮点型变量，表示所有数据点的x值的平方总和    long double xx_sum_ = 0;    // 长双精度浮点型变量，表示所有数据点的x值和y值的乘积总和    long double xy_sum_ = 0;        T x_min_ = std::numeric_limits<T>::max();    T x_max_ = std::numeric_limits<T>::lowest();    double y_min_ = std::numeric_limits<double>::max();    double y_max_ = std::numeric_limits<double>::lowest();&#125;;</code></pre><p><strong>build 方法构建模型</strong>：</p><pre class=" language-c++"><code class="language-c++">void build() &#123;    if (count_ <= 1) &#123;        model_->a_ = 0;        model_->b_ = static_cast<double>(y_sum_);        return;    &#125;    if (static_cast<long double>(count_) * xx_sum_ - x_sum_ * x_sum_ == 0) &#123;        // all values in a bucket have the same key.        model_->a_ = 0;        model_->b_ = static_cast<double>(y_sum_) / count_;        return;    &#125;    auto slope = static_cast<double>(        (static_cast<long double>(count_) * xy_sum_ - x_sum_ * y_sum_) /        (static_cast<long double>(count_) * xx_sum_ - x_sum_ * x_sum_));        auto intercept = static_cast<double>(        (y_sum_ - static_cast<long double>(slope) * x_sum_) / count_);    model_->a_ = slope;    model_->b_ = intercept;    // If floating point precision errors, fit spline    if (model_->a_ <= 0) &#123;        if (x_max_ - x_min_ == 0) &#123;            model_->a_ = 0;            model_->b_ = static_cast<double>(y_sum_) / count_;        &#125; else &#123;            model_->a_ = (y_max_ - y_min_) / (x_max_ - x_min_);            model_->b_ = -static_cast<double>(x_min_) * model_->a_;        &#125;    &#125;&#125;</code></pre><hr><h5 id="工具方法"><a href="#工具方法" class="headerlink" title="工具方法"></a>工具方法</h5><pre class=" language-c++"><code class="language-c++">/*** Helper methods for bitmap ***/// Extract the rightmost 1 in the binary representation.// e.g. extract_rightmost_one(010100100) = 000000100inline uint64_t extract_rightmost_one(uint64_t value) &#123;    return value & -static_cast<int64_t>(value);&#125;// Remove the rightmost 1 in the binary representation.// e.g. remove_rightmost_one(010100100) = 010100000inline uint64_t remove_rightmost_one(uint64_t value) &#123;    return value & (value - 1);&#125;// Count the number of 1s in the binary representation.// e.g. count_ones(010100100) = 3inline int count_ones(uint64_t value) &#123;    return static_cast<int>(_mm_popcnt_u64(value));&#125;// Get the offset of a bit in a bitmap.// word_id is the word id of the bit in a bitmap// bit is the word that contains the bitinline int get_offset(int word_id, uint64_t bit) &#123;    // 返回该位在位图中的偏移量    return (word_id << 6) + count_ones(bit - 1);&#125;// https://stackoverflow.com/questions/364985/algorithm-for-finding-the-smallest-power-of-two-thats-greater-or-equal-to-a-giv// 返回大于或等于x的最小2的幂次方数inline int pow_2_round_up(int x) &#123;    --x;    x |= x >> 1;    x |= x >> 2;    x |= x >> 4;    x |= x >> 8;    x |= x >> 16;    return x + 1;&#125;// https://stackoverflow.com/questions/994593/how-to-do-an-integer-log2-in-c// 返回小于或等于x的最大2的幂次方数的指数inline int log_2_round_down(int x) &#123;    int res = 0;    while (x >>= 1) ++res;    return res;&#125;</code></pre><h5 id="统计方法"><a href="#统计方法" class="headerlink" title="统计方法"></a>统计方法</h5><pre class=" language-c++"><code class="language-c++">/*** Cost model weights ***/// Intra-node cost weightsconstexpr double kExpSearchIterationsWeight = 20;constexpr double kShiftsWeight = 0.5;// TraverseToLeaf cost weightsconstexpr double kNodeLookupsWeight = 20;constexpr double kModelSizeWeight = 5e-7;/*** Stat Accumulators ***/// 统计数据节点的搜索迭代次数和位移数量struct DataNodeStats &#123;    double num_search_iterations = 0;    double num_shifts = 0;&#125;;// Used when stats are computed using a sample// 用于在计算统计数据时使用样本struct SampleDataNodeStats &#123;    // 样本大小的对数    double log2_sample_size = 0;    // 搜索迭代次数    double num_search_iterations = 0;    // 位移数量的对数    double log2_num_shifts = 0;&#125;;// Accumulates stats that are used in the cost model, based on the actual vs// predicted position of a keyclass StatAccumulator &#123;    public:    virtual ~StatAccumulator() = default;    // 用于根据实际位置和预测位置来累积统计信息    virtual void accumulate(int actual_position, int predicted_position) = 0;    // 用于获取当前的统计信息    virtual double get_stat() = 0;    // 重置统计信息    virtual void reset() = 0;&#125;;</code></pre><p><strong>计算预期的指数搜索迭代次数</strong>：</p><pre class=" language-c++"><code class="language-c++">// Mean log error represents the expected number of exponential search iterations when doing a lookupclass ExpectedSearchIterationsAccumulator : public StatAccumulator &#123;    public:    // 计算预期的指数搜索迭代次数    void accumulate(int actual_position, int predicted_position) override &#123;        // 计算实际位置和预测位置之间的差的绝对值加1的对数        cumulative_log_error_ +=            std::log2(std::abs(predicted_position - actual_position) + 1);        count_++;    &#125;    double get_stat() override &#123;        if (count_ == 0) return 0;        // 返回当前的平均对数误差        return cumulative_log_error_ / count_;    &#125;    void reset() override &#123;        cumulative_log_error_ = 0;        count_ = 0;    &#125;    public:    double cumulative_log_error_ = 0;    int count_ = 0;&#125;;</code></pre><p><strong>计算预期的插入操作中的移位次数</strong>：</p><pre class=" language-c++"><code class="language-c++">// Mean shifts represents the expected number of shifts when doing an insertclass ExpectedShiftsAccumulator : public StatAccumulator &#123;public:    explicit ExpectedShiftsAccumulator(int data_capacity)        : data_capacity_(data_capacity) &#123;&#125;    // A dense region of n keys will contribute a total number of expected shifts    // of approximately    // ((n-1)/2)((n-1)/2 + 1) = n^2/4 - 1/4    // This is exact for odd n and off by 0.25 for even n.    // Therefore, we track n^2/4.    void accumulate(int actual_position, int) override &#123;        if (actual_position > last_position_ + 1) &#123;            long long dense_region_length = last_position_ - dense_region_start_idx_ + 1;            num_expected_shifts_ += (dense_region_length * dense_region_length) / 4;            dense_region_start_idx_ = actual_position;        &#125;        last_position_ = actual_position;        count_++;    &#125;    double get_stat() override &#123;        if (count_ == 0) return 0;        // first need to accumulate statistics for current packed region        long long dense_region_length = last_position_ - dense_region_start_idx_ + 1;        long long cur_num_expected_shifts =            num_expected_shifts_ + (dense_region_length * dense_region_length) / 4;        return cur_num_expected_shifts / static_cast<double>(count_);    &#125;    void reset() override &#123;        last_position_ = -1;        dense_region_start_idx_ = 0;        num_expected_shifts_ = 0;        count_ = 0;    &#125;public:    int last_position_ = -1;            // 最后一个位置    int dense_region_start_idx_ = 0;    // 密集区域的起始索引    long long num_expected_shifts_ = 0; // 预期的移位次数    int count_ = 0;    int data_capacity_ = -1;            // 节点的容量&#125;;</code></pre><hr><h4 id="alex-node-h"><a href="#alex-node-h" class="headerlink" title="alex_node.h"></a>alex_node.h</h4><p><strong>宏定义，控制编译条件</strong>：</p><pre class=" language-c++"><code class="language-c++">// Whether we store key and payload arrays separately in data nodes// By default, we store them separately#define ALEX_DATA_NODE_SEP_ARRAYS 1#if ALEX_DATA_NODE_SEP_ARRAYS#define ALEX_DATA_NODE_KEY_AT(i) key_slots_[i]#define ALEX_DATA_NODE_PAYLOAD_AT(i) payload_slots_[i]#else#define ALEX_DATA_NODE_KEY_AT(i) data_slots_[i].first#define ALEX_DATA_NODE_PAYLOAD_AT(i) data_slots_[i].second#endif// Whether we use lzcnt and tzcnt when manipulating a bitmap (e.g., when finding the closest gap).// If your hardware does not support lzcnt/tzcnt (e.g., your Intel CPU is pre-Haswell), set this to 0.#define ALEX_USE_LZCNT 1</code></pre><h5 id="AlexNode"><a href="#AlexNode" class="headerlink" title="AlexNode"></a>AlexNode</h5><pre class=" language-c++"><code class="language-c++">// A parent class for both types of ALEX nodestemplate<class T, class P>class AlexNode &#123;public:    // Whether this node is a leaf (data) node    bool is_leaf_ = false;    // Power of 2 to which the pointer to this node is duplicated in its parent model node    // For example, if duplication_factor_ is 3, then there are 8 redundant    // pointers to this node in its parent    // 指向这个节点的指针在其父模型节点中被复制的次数。    // 例如，如果duplication_factor_为3，那么在其父节点中有8个指向这个节点的冗余指针    uint8_t duplication_factor_ = 0;    // Node's level in the RMI. Root node is level 0    short level_ = 0;    // Both model nodes and data nodes nodes use models    LinearModel<T> model_;    // Could be either the expected or empirical cost, depending on how this field is used    double cost_ = 0.0;    AlexNode() = default;    explicit AlexNode(short level) : level_(level) &#123;&#125;    AlexNode(short level, bool is_leaf) : is_leaf_(is_leaf), level_(level) &#123;&#125;    virtual ~AlexNode() = default;    // The size in bytes of all member variables in this class    virtual long long node_size() const = 0;&#125;;</code></pre><h5 id="AlexModelNode"><a href="#AlexModelNode" class="headerlink" title="AlexModelNode"></a>AlexModelNode</h5><pre class=" language-c++"><code class="language-c++">template<class T, class P, class Alloc = std::allocator<std::pair<T, P>>>class AlexModelNode : public AlexNode<T, P> &#123;public:    // 类型别名: 当前类的类型    typedef AlexModelNode<T, P, Alloc> self_type;    // 定义一个类型别名 alloc_type,它是 Alloc 分配器类针对 self_type 类型的特化版本的类型    typedef typename Alloc::template rebind<self_type>::other alloc_type;    typedef typename Alloc::template rebind<AlexNode<T, P> *>::other pointer_alloc_type;    const Alloc &allocator_;    // Number of logical children. Must be a power of 2    int num_children_ = 0;    // Array of pointers to children    AlexNode<T, P> **children_ = nullptr;    // 指向子节点的指针数组    // 构造函数1：使用默认分配器初始化对象    explicit AlexModelNode(const Alloc &alloc = Alloc())            : AlexNode<T, P>(0, false), allocator_(alloc) &#123;&#125;    // 构造函数2：使用指定的层级和默认分配器初始化对象    explicit AlexModelNode(short level, const Alloc &alloc = Alloc())            : AlexNode<T, P>(level, false), allocator_(alloc) &#123;&#125;    ~AlexModelNode() &#123;        if (children_ == nullptr) &#123;            return;        &#125;        // 调用 pointer_allocator() 函数获取分配器对象,        // 并使用其 deallocate 方法释放 children_ 指向的内存空间. num_children_ 参数指定要释放的多少        pointer_allocator().deallocate(children_, num_children_);    &#125;    AlexModelNode(const self_type &other) : AlexNode<T, P>(other), allocator_(other.allocator_),                                            num_children_(other.num_children_) &#123;        // 使用分配器对象pointer_allocator()分配一块内存空间,大小为other.num_children_个指针的大小,        // 并将返回的指针赋值给children_        children_ = new(pointer_allocator().allocate(other.num_children_))                AlexNode<T, P> *[other.num_children_];        // 将other对象的子节点指针数组复制到当前对象的子节点指针数组中        std::copy(other.children_, other.children_ + other.num_children_, children_);    &#125;    // Given a key, traverses to the child node responsible for that key    // 根据给定的键（key）获取对应的子节点指针    inline AlexNode<T, P> *get_child_node(const T &key) &#123;        int bucketID = this->model_.predict(key);        bucketID = std::min<int>(std::max<int>(bucketID, 0), num_children_ - 1);        return children_[bucketID];    &#125;    pointer_alloc_type pointer_allocator() &#123;        return pointer_alloc_type(allocator_);    &#125;    // 计算节点的大小,包括当前对象的大小以及子节点指针数组的大小    long long node_size() const override &#123;        long long size = sizeof(self_type);        size += num_children_ * sizeof(AlexNode<T, P> *);  // pointers to children        return size;    &#125;&#125;;</code></pre><p><strong>内部节点扩展</strong>：</p><pre class=" language-c++"><code class="language-c++">// Expand by a power of 2 by creating duplicates of all existing child pointers.// Input is the base 2 log of the expansion factor, in order to guarantee expanding by a power of 2.// Returns the expansion factor.// 通过创建现有子指针的副本来将数据结构扩展到2的幂次方大小int expand(int log2_expansion_factor) &#123;    assert(log2_expansion_factor >= 0);    // 实际的扩展因子    int expansion_factor = 1 << log2_expansion_factor;    // 新的子节点数量    int num_new_children = num_children_ * expansion_factor;    auto new_children = new(pointer_allocator().allocate(num_new_children))        AlexNode<T, P> *[num_new_children];    int cur = 0;    // 遍历现有的子节点，并将每个子节点复制到新数组中相应的位置    while (cur < num_children_) &#123;        AlexNode<T, P> *cur_child = children_[cur];        // 复制的数量由当前子节点的duplication_factor_属性决定。        int cur_child_repeats = 1 << cur_child->duplication_factor_;        for (int i = expansion_factor * cur; i < expansion_factor * (cur + cur_child_repeats); i++) &#123;            new_children[i] = cur_child;        &#125;        // 更新当前子节点的duplication_factor_属性，以便记录其被复制的次数。        cur_child->duplication_factor_ += log2_expansion_factor;        cur += cur_child_repeats;    &#125;    pointer_allocator().deallocate(children_, num_children_);    children_ = new_children;    num_children_ = num_new_children;    this->model_.expand(expansion_factor);    return expansion_factor;&#125;</code></pre><h5 id="AlexDataNode"><a href="#AlexDataNode" class="headerlink" title="AlexDataNode"></a>AlexDataNode</h5><p><strong>封装类型别名</strong>：</p><pre class=" language-c++"><code class="language-c++">template<class T, class P, class Compare = AlexCompare, class Alloc = std::allocator<std::pair<T, P>>, bool allow_duplicates = true>class AlexDataNode : public AlexNode<T, P> &#123;    public:        // 类型别名V, 表示键值对的类型        typedef std::pair<T, P> V;        // 类型别名self_type, 表示当前类的类型        typedef AlexDataNode<T, P, Compare, Alloc, allow_duplicates> self_type;        // 类型别名alloc_type, 表示用于分配self_type对象的分配器类型        typedef typename Alloc::template rebind<self_type>::other alloc_type;        // 类型别名key_alloc_type, 表示用于分配键类型的分配器类型        typedef typename Alloc::template rebind<T>::other key_alloc_type;        // 类型别名payload_alloc_type, 表示用于分配值类型的分配器类型        typedef typename Alloc::template rebind<P>::other payload_alloc_type;        // 类型别名value_alloc_type, 表示用于分配键值对类型的分配器类型        typedef typename Alloc::template rebind<V>::other value_alloc_type;        // 类型别名bitmap_alloc_type, 表示用于分配位图类型的分配器类型        typedef typename Alloc::template rebind<uint64_t>::other bitmap_alloc_type;        // 常量引用key_less_, 表示用于比较键大小的比较函数对象        const Compare &key_less_;        // 常量引用allocator_, 表示用于分配内存的分配器对象        const Alloc &allocator_;        // Forward declaration        template<typename node_type = self_type, typename payload_return_type = P, typename value_return_type = V>        class Iterator;    // 迭代容器中的元素        // 类型别名iterator_type, 表示非const迭代器类型        typedef Iterator<> iterator_type;        // 类型别名const_iterator_type, 表示const迭代器类型        typedef Iterator<const self_type, const P, const V> const_iterator_type;        // 一个指向下一个叶子节点的指针next_leaf_, 初始值为nullptr        self_type *next_leaf_ = nullptr;        // 一个指向上一个叶子节点的指针prev_leaf_, 初始值为nullptr        self_type *prev_leaf_ = nullptr;&#125;;</code></pre><p>数据节点属性：</p><pre class=" language-c++"><code class="language-c++">#if ALEX_DATA_NODE_SEP_ARRAYS    T *key_slots_ = nullptr;        // holds keys    P *payload_slots_ = nullptr;    // holds payloads, must be same size as key_slots#else    V* data_slots_ = nullptr;  // holds key-payload pairs#endif// 键/数据槽数组的大小int data_capacity_ = 0;  // size of key/data_slots array// 已填充的键/数据槽的数量（与间隙相对）int num_keys_ = 0;  // number of filled key/data slots (as opposed to gaps)// Bitmap: each uint64_t represents 64 positions in reverse order// (i.e., each uint64_t is "read" from the right-most bit to the left-most bit)// bitmap_是一个位图，每个uint64_t代表64个位置，从右到左读取uint64_t *bitmap_ = nullptr;// 位图中的int64_t数量int bitmap_size_ = 0;  // number of int64_t in bitmap// Variables related to resizing (expansions and contractions)static constexpr double kMaxDensity_ = 0.8;  // density after contracting,// also determines the expansion // thresholdstatic constexpr double kInitDensity_ = 0.7;  // density of data nodes after bulk loadingstatic constexpr double kMinDensity_ = 0.6;  // density after expanding, also// determines the contraction // thresholddouble expansion_threshold_ = 1;  // expand after m_num_keys is >= this numberdouble contraction_threshold_ = 0;  // contract after m_num_keys is < this numberstatic constexpr int kDefaultMaxDataNodeBytes_ = 1 << 24;  // by default, maximum data node size is 16MB// 最大键/数据槽的数量，不能超过这个数量进行扩展int max_slots_ = kDefaultMaxDataNodeBytes_ / sizeof(V);  // cannot expand beyond this number of key/data slots// Counters used in cost modelslong long num_shifts_ = 0;                 // does not reset after resizinglong long num_exp_search_iterations_ = 0;  // does not reset after resizingint num_lookups_ = 0;                      // does not reset after resizingint num_inserts_ = 0;                      // does not reset after resizingint num_resizes_ = 0;  // technically not required, but nice to have// Variables for determining append-mostly behaviorT max_key_ = std::numeric_limits<T>::lowest();  // max key in node, updates after inserts but not erasesT min_key_ = std::numeric_limits<T>::max();  // min key in node, updates after// inserts but not erasesint num_right_out_of_bounds_inserts_ = 0;  // number of inserts that are larger than the max keyint num_left_out_of_bounds_inserts_ = 0;  // number of inserts that are smaller than the min key// Node is considered append-mostly if the fraction of inserts that are out of// bounds is above this threshold// Append-mostly nodes will expand in a manner that anticipates further// appendsstatic constexpr double kAppendMostlyThreshold = 0.9;// Purely for benchmark debugging purposesdouble expected_avg_exp_search_iterations_ = 0;double expected_avg_shifts_ = 0;// Placed at the end of the key/data slots if there are gaps after the max key// 一个哨兵值，放置在键/数据槽的末尾，用于标记最大键之后的任何间隙static constexpr T kEndSentinel_ = std::numeric_limits<T>::max();</code></pre><p><strong>构造函数</strong>：</p><pre class=" language-c++"><code class="language-c++"></code></pre><p><strong>迭代器</strong>：</p><pre class=" language-c++"><code class="language-c++"></code></pre><p><strong>插入</strong>：</p><pre class=" language-c++"><code class="language-c++"></code></pre><p><strong>查询</strong>：</p><pre class=" language-c++"><code class="language-c++"></code></pre>]]></content>
      
      
      <categories>
          
          <category> 学习型索引 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习型索引 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FINEdex</title>
      <link href="posts/323e.html"/>
      <url>posts/323e.html</url>
      
        <content type="html"><![CDATA[<h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><p>随着现有数据量的快速增长，人们考虑使用学习索引来替换传统的索引结构。但是传统的学习索引无法解决高并发需求，作者首先总结了现有的学习索引存在的短板：</p><ul><li>有限的可扩展性。针对插入更新操作，现有的学习索引很难拥有很好的性能。目前提出的一些学习索引结构只能单独的解决高并发的读，写，重训练。FITing-tree,ALEX,PGM-index都没有考虑数据一致性问题。XIndex通过把数据保存在不同的数据结构中来解决并发的数据一致性问题，这样会导致范围查询操作的性能低下。</li><li>高开销问题。XIndex和FITing-tree都是通过建立一个delta buffer来解决插入操作，该缓冲区是B+树或者Mass树。并且XIndex显示，当缓冲区过大时，会产生很大程度的性能下降。ALEX和PGM-index是通过在索引结构中保留了空槽来解决插入时产生的高开销问题。一旦发生多线程访问时，会发生多个线程的冲突访问情况，这会导致并发操作的性能下降。</li></ul><p>作者提出了FINEdex做出了如下贡献：</p><ol><li>高可伸缩性。提出了一种用于并发内存系统的非粒度学习索引方案，即FINEdex，它有效地满足了可伸缩性的要求。主要的贡献是通过充实的数据结构来减少数据依赖性，并同时在两个粒度中对模型进行重新训练。</li><li>低开销的索引操作和无阻塞的重训练操作。</li><li>系统应用以及评测。</li></ol><p><img src="https://yux20000304.github.io/2022/10/12/FINEdex%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/1.png" alt="img"></p><p>上图给出了目前现有的学习索引的功能。</p><h2 id="设计部分介绍"><a href="#设计部分介绍" class="headerlink" title="设计部分介绍"></a>设计部分介绍</h2><p>首先给出了数据结构设计部分。</p><p><img src="https://yux20000304.github.io/2022/10/12/FINEdex%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/2.png" alt="img"></p><p>作者通过对XIndex的设计结构进行修改，实现了一个高效的并发性数据索引结构。实现了一个两级的有序数组结构。该结构能够很好的避免数据依赖性并且能够保持数据的有序性。并且在插入的过程中，旧数据的访问不会受到影响。只有当缓冲区满了的时候，才会发生数据合并以及模型重训练操作。作者把整个设计分成了两个部分：</p><ol><li>模型部分：这些独立的模型可以通过并发的再训练来适应新的数据分布。</li><li>数据部分：通过层级结构，建立了两层有序数组，提供能快速重训练的方案。</li></ol><ul><li>模型部分，首先作者设计了一个方案来提高模型的准确率。学习探针算法LPA。通过在f(x)函数周围建立一个平行四边形囊括所有的训练数据，之后再使用线性回归模型进行拟合。传统的RMI模型，由于无法针对不同分布的数据来决定模型的个数，RMI模型的准确率一直是一个潜在的问题。LPA算法通过设置一个错误率阈值，来判断某一组数据是否为线性分布的。如果大于这个阈值，那么移除该数据集靠后的一部分数据，如果小于阈值，那么添加新的数据进入原来的数据集中再重新训练。这个过程一共需要提供如下几个参数：</li></ul><ol><li>threshold：错误率阈值。</li><li>learning_step：学习步长，用来决定学习速度的参数之一。</li><li>learning_rate：学习率，用来决定学习速度的参数之一。<br>整个过程是一个贪心算法。最终得到的输出是一串错误率为threshold的线性模型链。</li></ol><p>除了以上的几个参数设置，作者还提出了优化模型输出的方案。上面提到了LPA算法得到的是一串线性模型链，在执行查找等操作时，需要顺序遍历整个模型链。最终作者选择使用优化后的b树来存储这些模型&lt;key，model&gt;，key代表了一个模型的最大元素。</p><ul><li><p>数据部分，主要解决了并发性问题。每一个模型都有一个小型的缓冲区来解决修改和插入等操作。整个小型缓冲区被设计成了两层的b树结构，在训练好的模型中，每个数组中的元素都有一个bin指针，用来指向level bin缓冲区，该缓冲区分为root层和child层，首先插入的数据会被放到root层，随着root层满了，会创建child节点来存储新插入的节点，之后插入的节点会优先插入到靠前的child节点来节约空间使用。<strong>缺陷：在插入的过程中需要不断地调整树形结构，这可能会造成很大的性能开销</strong></p></li><li><p>紧接着，作者提出了在并发情况下的模型重训练设计思路。作者把重训练分成了两个部分：level-bin重训练和模型重训练。</p><ul><li>level-bin重训练<br>作者提出每当一个level-bin层满了之后，只需要对一个该层进行重训练，不需要影响其余的level-bin节点，产生的时间开销仅有27us</li><li>模型重训练<br>作者通过使用RCU技术保证了新模型的训练过程中，旧模型的访问操作不被影响。由于level-bins是通过指针指向的，我们只需要在复制的过程中使用新的指针指向level-bin即可。模型重训练会在模型需要重新训练一个更小的模型时被触发。</li></ul></li><li><p>并发：作者通过提出一系列方案来解决并发冲突</p><ul><li>写写冲突：当不同线程需要对同样的数据或者是bin进行修改会产生写写冲突。作者提出使用细粒度的锁分别对记录和bin进行上锁。对记录上锁的操作比较简单，对于bin上锁，需要根据情况进行上锁，有的时候可能只需要对child bin进行上锁，有的情况需要对root bin也进行上锁。</li><li>读写冲突：通过使用版本控制实现数据一致性。每次完成训练后，都会给模型和数据分配一个版本号。在一次访问过程中，首先会获取版本号，如果在读取完成后版本号没有发生改变，那么说明读取操作的数据是有效的，如果无效那么重新发起读请求。</li></ul></li></ul><h2 id="测试部分介绍"><a href="#测试部分介绍" class="headerlink" title="测试部分介绍"></a>测试部分介绍</h2><p>在redis中进行测试。只修改了基础的排序数据结构。一共提供了六个常见的api（train，get，put，update，remove，scan）。一共分配了4个线程来执行模型的重训练，访问过程一共分为三步：在模型中进行查找，计算区间，操作level-bin。使用24线程进行测试。使用了Masstree，Xindex，LI+Δ三个索引设计方案进行对比。没有采用redis原生的跳表结构进行对比，因为其与树形结构的性能比较类似。使用LI+Δ方案时，为了让其支持并发操作，使用了XIndex的delta buffer方案，使用masstree管理该缓冲区。FIT树由于不支持并发没有进行比较。</p><ul><li>配置部分：学习索引结构都使用了两层RMI模型，第二层最多容纳250k的模型数量（XIndex中得出的结论）。FINEdex的threshold我们设置成32，level-bin层的root设置成8，child bin大小设置为16。</li><li>使用的测试工具：<ol><li>YCSB测试工具：一共提供了六种不同的测试负载。</li><li>网络博客负载</li><li>文档id负载</li></ol></li></ul>]]></content>
      
      
      <categories>
          
          <category> 学习型索引 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习型索引 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LearnedIndex-Papers</title>
      <link href="posts/5da4.html"/>
      <url>posts/5da4.html</url>
      
        <content type="html"><![CDATA[<h2 id="学习型索引"><a href="#学习型索引" class="headerlink" title="学习型索引"></a>学习型索引</h2><h3 id="1-论文"><a href="#1-论文" class="headerlink" title="1. 论文"></a>1. 论文</h3><h4 id="1-1-年份"><a href="#1-1-年份" class="headerlink" title="1.1 年份"></a>1.1 年份</h4><h5 id="2018"><a href="#2018" class="headerlink" title="2018"></a>2018</h5><table><thead><tr><th>发表年份</th><th>原文地址</th><th>Code</th></tr></thead><tbody><tr><td><a href="papers/2018-SIGMOD-The_Case_for_Learned_Index_Structures.pdf">2018-SIGMOD-The Case for Learned Index Structures</a> : 开山之作</td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3183713.3196909">Sigmod</a></td><td><a href="https://github.com/learnedsystems/RMI">learnedsystems/RMI</a></td></tr></tbody></table><hr><h5 id="2019"><a href="#2019" class="headerlink" title="2019"></a>2019</h5><table><thead><tr><th>发表年份</th><th>原文地址</th><th>Code</th></tr></thead><tbody><tr><td><a href="papers/2019-arXiv-A_Benchmark_for_Learned_Indexes.pdf">2019-arXiv-SOSD: A Benchmark for Learned Indexes</a>  : 测试基准</td><td><a href="https://arxiv.org/pdf/1911.13014.pdf">arXiv</a></td><td><a href="https://github.com/learnedsystems/SOSD">learnedsystems/SOSD</a></td></tr><tr><td><a href="papers/2019-arXiv-A_Scalable_Learned_Index_Scheme_in_Storage_Systems.pdf">2019-arXiv-A Scalable Learned Index Scheme in Storage Systems</a>:  FINEdex 版之始</td><td><a href="https://arxiv.org/pdf/1905.06256.pdf">arXiv</a></td><td></td></tr><tr><td><a href="papers/2019-SIGMOD-FITing-Tree_A_Data-aware_Index_Structure.pdf">2019-SIGMOD-FITing-Tree_A Data-aware Index Structure</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3299869.3319860">Sigmod</a></td><td></td></tr><tr><td><a href="papers/2019-%E8%BD%AF%E4%BB%B6%E5%AD%A6%E6%8A%A5-%E5%9F%BA%E4%BA%8E%E4%B8%AD%E9%97%B4%E5%B1%82%E7%9A%84%E5%8F%AF%E6%89%A9%E5%B1%95%E5%AD%A6%E4%B9%A0%E7%B4%A2%E5%BC%95%E6%8A%80%E6%9C%AF.pdf">2019-软件学报-基于中间层的可扩展学习索引技术</a></td><td><a href="https://www.jos.org.cn/html/2020/3/5910.htm">软件学报</a></td><td></td></tr></tbody></table><hr><h5 id="2020"><a href="#2020" class="headerlink" title="2020"></a>2020</h5><table><thead><tr><th>发表年份</th><th>原文地址</th><th>Code</th></tr></thead><tbody><tr><td><a href="papers/2020-%E8%BD%AF%E4%BB%B6%E5%AD%A6%E6%8A%A5-%E5%AD%A6%E4%B9%A0%E7%B4%A2%E5%BC%95%EF%BC%9A%E7%8E%B0%E7%8A%B6%E4%B8%8E%E7%A0%94%E7%A9%B6%E5%B1%95%E6%9C%9B.pdf">2020-软件学报-学习索引：现状与研究展望</a></td><td><a href="https://www.jos.org.cn/html/2021/4/6168.htm">软件学报</a></td><td></td></tr><tr><td><a href="papers/2020-aiDM-RadixSpline_A_Single-Pass_Learned_Index.pdf">2020-aiDM-RadixSpline_A Single-Pass Learned Index</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3401071.3401659">aiDM</a></td><td></td></tr><tr><td><a href="papers/2020-APSys-SIndex_A_Scalable_Learned_Index_for_String_Keys.pdf">2020-APSys-SIndex_A Scalable Learned Index for String Keys</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3409963.3410496">APSys</a></td><td></td></tr><tr><td><a href="papers/2020-ICDEW-START_Self-Tuning_Adaptive_Radix_Tree.pdf">2020-ICDEW-START_Self-Tuning_Adaptive_Radix_Tree</a></td><td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9094133">ICDE</a></td><td></td></tr><tr><td><a href="papers/2020-NeurIPS-Learned_Indexes_for_a_Google-scale_Disk-based_Database.pdf">2020-NeurIPS-Learned Indexes for a Google-scale Disk-based Database</a></td><td><a href="https://arxiv.org/pdf/2012.12501.pdf">arXiv</a></td><td></td></tr><tr><td><a href="papers/2020-OSDI-Bourbon_learned_LSM.pdf">2020-OSDI-Bourbon_learned_LSM</a></td><td><a href="https://dl.acm.org/doi/pdf/10.5555/3488766.3488775">OSDI</a></td><td></td></tr><tr><td><a href="papers/2020-OSDI-Bourbon_learned_LSM_slides.pdf">2020-OSDI-Bourbon_learned_LSM_slides</a></td><td><a href="https://dl.acm.org/doi/pdf/10.5555/3488766.3488775">OSDI</a></td><td></td></tr><tr><td><a href="papers/2020-PPoPP-XIndex_A_Scalable_Learned_Index_for_Multicore_Data_Storage.pdf">2020-PPoPP-XIndex_A Scalable Learned Index for Multicore Data Storage</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3332466.3374547">PPoPP</a></td><td></td></tr><tr><td><a href="papers/2020-VLDB-PGM-index_fully-dynamic_compressed_worst-case_bounds.pdf">2020-VLDB-PGM-index_fully-dynamic_compressed_worst-case_bounds</a></td><td><a href="https://dl.acm.org/doi/pdf/10.14778/3389133.3389135">PVLDB</a></td><td></td></tr><tr><td><a href="papers/2020-SIGMOD-ALEX_Updatable_Adaptive_Learned_Index.pdf">2020-SIGMOD-ALEX_Updatable_Adaptive_Learned_Index</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3318464.3389711">Sigmod</a></td><td></td></tr><tr><td><a href="papers/2020-SIGMOD-CDFShop_Exploring_and_Optimizing_Learned_Index_Structures.pdf">2020-SIGMOD-CDFShop Exploring and Optimizing Learned Index Structures</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3318464.3384706">Sigmod</a></td><td></td></tr><tr><td><a href="papers/2020-SIGMOD-Order-Preserving_Key_Compression_for_In-Memory_Search_Trees.pdf">2020-SIGMOD-Order-Preserving Key Compression for In-Memory Search Trees</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3318464.3380583">Sigmod</a></td><td></td></tr><tr><td><a href="papers/2020-SIGMOD-Order-Preserving_Key_Compression_for_In-Memory_Search_Trees_slides.pdf">2020-SIGMOD-Order-Preserving Key Compression for In-Memory Search Trees_slides</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3318464.3380583">Sigmod</a></td><td></td></tr><tr><td><a href="papers/2020-SIGMOD-The_Case_for_a_Learned_Sorting_Algorithm.pdf">2020-SIGMOD-The Case for a Learned Sorting Algorithm</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3318464.3389752">Sigmod</a></td><td></td></tr></tbody></table><hr><h5 id="2021"><a href="#2021" class="headerlink" title="2021"></a>2021</h5><table><thead><tr><th>发表年份</th><th>原文地址</th><th>Code</th></tr></thead><tbody><tr><td><a href="papers/2021-AIDB-PLEX_RS+CHT.pdf">2021-AIDB-PLEX_RS+CHT</a></td><td><a href="https://arxiv.org/pdf/2108.05117.pdf">AIDB</a></td><td></td></tr><tr><td><a href="papers/2021-AIDB-RSS_Bounding_the_Last_Mile-Efficient_Learned_String_Indexing.pdf">2021-AIDB-RSS_Bounding_the_Last_Mile-Efficient_Learned_String_Indexing</a></td><td><a href="https://arxiv.org/pdf/2111.14905.pdf">AIDB</a></td><td></td></tr><tr><td><a href="papers/2021-aiDM-RUSLI_Real-time_Updatable_Spline_Learned_Index.pdf">2021-aiDM-RUSLI_Real-time_Updatable_Spline_Learned_Index</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3464509.3464886">aiDM</a></td><td></td></tr><tr><td><a href="papers/2021-aiDM-Tailored_Regression_Learned_Indexes-Logarithmic-Error-Regression.pdf">2021-aiDM-Tailored_Regression_Learned_Indexes-Logarithmic-Error-Regression</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3464509.3464891">aiDM</a></td><td><a href="https://github.com/umatin/LogarithmicErrorRegression">LogarithmicErrorRegression</a></td></tr><tr><td><a href="papers/2021-arXiv-Micro-architectural_Analysis_of_a_Learned_Index.pdf">2021-arXiv-Micro-architectural Analysis of a Learned Index</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3533702.3534917">arXiv</a></td><td></td></tr><tr><td><a href="papers/2021-arXiv-Pluggable_Learned_Index_Method_via_Sampling_and_Gap_Insertion.pdf">2021-arXiv-Pluggable_Learned_Index_Method_via_Sampling_and_Gap_Insertion</a></td><td><a href="https://arxiv.org/pdf/2101.00808.pdf">arXiv</a></td><td></td></tr><tr><td><a href="papers/2021-EDBT-Shift-Table_A_Low-latency_Learned_Index_for_Range_Queries_using_Model_Correction.pdf">2021-EDBT-Shift-Table A Low-latency Learned Index for Range Queries using Model Correction</a></td><td><a href="https://arxiv.org/pdf/2101.10457.pdf">EDBT</a></td><td></td></tr><tr><td><a href="papers/2021-ICDEW-Towards_a_Benchmark_for_Learned_Systems.pdf">2021-ICDEW-Towards a Benchmark for Learned Systems</a></td><td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9438803">ICDE</a></td><td></td></tr><tr><td><a href="papers/2021-PVLDB-Benchmarking_Learned_Indexes.pdf">2021-PVLDB-Benchmarking_Learned_Indexes</a></td><td><a href="https://dl.acm.org/doi/pdf/10.14778/3421424.3421425">PVLDB</a></td><td></td></tr><tr><td><a href="papers/2021-PVLDB-Endownment-learned_cardition.pdf">2021-PVLDB-Endownment-learned_cardition</a></td><td><a href="https://dl.acm.org/doi/pdf/10.14778/3461535.3461552">PVLDB</a></td><td></td></tr><tr><td><a href="papers/2021-PVLDB-FINEdex-Fine-grained_for_Scalable_Concurrent_Memory_Systems.pdf">2021-PVLDB-FINEdex-Fine-grained_for_Scalable_Concurrent_Memory_Systems</a></td><td><a href="https://dl.acm.org/doi/pdf/10.14778/3489496.3489512">PVLDB</a></td><td></td></tr><tr><td><a href="papers/2021-PVLDB-LIPP_Updatable_Learned_Index_Precise_Positions.pdf">2021-PVLDB-LIPP_Updatable_Learned_Index_Precise_Positions</a></td><td><a href="https://dl.acm.org/doi/pdf/10.14778/3457390.3457393">PVLDB</a></td><td><a href="https://github.com/Jiacheng-WU/lipp">lipp</a></td></tr></tbody></table><hr><h5 id="2022"><a href="#2022" class="headerlink" title="2022"></a>2022</h5><table><thead><tr><th>发表年份</th><th>原文地址</th><th>Code</th></tr></thead><tbody><tr><td><a href="papers/2022-aiDB-AutoIndex_Automatically_Finding_Optimal_Index_Structure.pdf">2022-aiDB-AutoIndex_Automatically_Finding_Optimal_Index_Structure</a></td><td><a href="https://arxiv.org/pdf/2208.03823.pdf">aiDB</a></td><td></td></tr><tr><td><a href="papers/2022-aiDM-LSI-Learned_Secondary_Index_Structure.pdf">2022-aiDM-LSI-Learned_Secondary_Index_Structure</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3533702.3534912">aiDM</a></td><td></td></tr><tr><td><a href="papers/2022-ICLR_learned_index_with_dynamic_eps.pdf">2022-ICLR_learned_index_with_dynamic_eps</a></td><td><a href="https://openreview.net/pdf?id=VyZRObZ19kt">ICLR</a></td><td></td></tr><tr><td><a href="papers/2022-learned_Similarity_Search.pdf">2022-learned_Similarity_Search</a></td><td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9891778">IEEE</a></td><td></td></tr><tr><td><a href="papers/2022-PVLDB-are_updatable_learned_index_ready.pdf">2022-PVLDB-are_updatable_learned_index_ready </a></td><td><a href="https://dl.acm.org/doi/pdf/10.14778/3551793.3551848">PVLDB</a></td><td></td></tr><tr><td><a href="papers/2022-PVLDB-Endowment-Can_Learned_Models_Replace_Hash_Functions.pdf">2022-PVLDB-Endowment-Can_Learned_Models_Replace_Hash_Functions</a></td><td><a href="https://dl.acm.org/doi/pdf/10.14778/3570690.3570702">PVLDB</a></td><td></td></tr><tr><td><a href="papers/2022-PVLDB-Endowment-FILM-for-Larger-than-Memory-Databases.pdf">2022-PVLDB-Endowment-FILM-for-Larger-than-Memory-Databases</a></td><td><a href="https://dl.acm.org/doi/pdf/10.14778/3570690.3570704">PVLDB</a></td><td></td></tr><tr><td><a href="papers/2022-TOS-The_Concurrent_Learned_Indexes_for_Multicore_Data_Storage.pdf">2022-TOS-The Concurrent Learned Indexes for Multicore Data Storage</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3478289">TOS</a></td><td></td></tr><tr><td><a href="papers/2022-VLDB-APEX_Learned_Index_PM.pdf">2022-VLDB-APEX_Learned_Index_PM</a></td><td><a href="https://dl.acm.org/doi/pdf/10.14778/3494124.3494141">VLDB</a></td><td></td></tr><tr><td><a href="papers/2022-VLDB-NFL_Learned_Index_Distribution_Transformation.pdf">2022-VLDB-NFL_Learned_Index_Distribution_Transformation</a>: it transforms keys distribution to make it more linear.</td><td><a href="https://dl.acm.org/doi/pdf/10.14778/3547305.3547322">VLDB</a></td><td></td></tr></tbody></table><hr><h5 id="2023"><a href="#2023" class="headerlink" title="2023"></a>2023</h5><table><thead><tr><th>发表年份</th><th>原文地址</th><th>Code</th></tr></thead><tbody><tr><td><a href="papers/2023-arXiv-DILI_A_Distribution-Driven_Learned_Index.pdf">2023-arXiv-DILI_A Distribution-Driven Learned Index</a>  Using more bulk loading time for better lookup.</td><td><a href="https://arxiv.org/pdf/2304.08817.pdf">arXiv</a></td><td></td></tr><tr><td><a href="papers/2023-arxiv-Updatable_Learned_Indexes_Disk-Resident_DBMS.pdf">2023-arXiv-Updatable_Learned_Indexes_Disk-Resident_DBMS</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3589284">arXiv</a></td><td></td></tr><tr><td><a href="papers/2023-ASPLOS-LeaFTL-leared_FTL_for_SSD.pdf">2023-ASPLOS-LeaFTL- A Learning-Based Flash Translation Layer for Solid-State Drives</a></td><td><a href="https://dl.acm.org/doi/pdf/10.1145/3575693.3575744">ASPLOS</a></td><td></td></tr><tr><td><a href="papers/2023-FAST-ROLEX.pdf">2023-FAST-ROLEX</a></td><td></td><td></td></tr><tr><td><a href="papers/2023-PVLDB-Learned_Index_A_Comprehensive_Experimental_Evaluation.pdf">2023-PVLDB-Learned Index_A Comprehensive Experimental Evaluation</a></td><td><a href="https://dl.acm.org/doi/pdf/10.14778/3594512.3594528">PVLDB</a></td><td></td></tr></tbody></table><hr><h4 id="1-2-类别"><a href="#1-2-类别" class="headerlink" title="1.2 类别"></a>1.2 类别</h4><h5 id="Survey-Benchmark-Tuning"><a href="#Survey-Benchmark-Tuning" class="headerlink" title="Survey/Benchmark/Tuning"></a>Survey/Benchmark/Tuning</h5><ol><li> <a href="papers/2019-arXiv-A_Benchmark_for_Learned_Indexes.pdf">2019-arXiv-A Benchmark for Learned Indexes</a></li><li> <a href="papers/2020-SIGMOD-CDFShop_Exploring_and_Optimizing_Learned_Index_Structures.pdf">2020-SIGMOD-demo-CDFShop-tuning_RMI</a></li><li> <a href="papers/2021-PVLDB-Benchmarking_Learned_Indexes.pdf">2021-PVLDB-Benchmarking_Learned_Indexes</a></li><li> <a href="papers/2021-aiDM-Tailored_Regression_Learned_Indexes-Logarithmic-Error-Regression.pdf">2021-aiDM-Tailored_Regression_Learned_Indexes-Logarithmic-Error-Regression</a></li><li> <a href="papers/2022-PVLDB-are_updatable_learned_index_ready.pdf">2022-are_updatable_learned_index_ready</a></li><li> <a href="papers/2023-PVLDB-Learned_Index_A_Comprehensive_Experimental_Evaluation.pdf">2023-PVLDB-Endowment-Comprehensive_Experimental_Evaluation</a></li></ol><h5 id="Read-only"><a href="#Read-only" class="headerlink" title="Read-only"></a>Read-only</h5><ol><li> <a href="papers/2018-SIGMOD-The_Case_for_Learned_Index_Structures.pdf">2018-SIGMOD-The Case for Learned Index Structures</a></li><li> <a href="papers/2020-aiDM-RadixSpline_A_Single-Pass_Learned_Index.pdf">2020-aiDM-RadixSpline_A Single-Pass Learned Index</a></li><li> <a href="papers/2020-NeurIPS-Learned_Indexes_for_a_Google-scale_Disk-based_Database.pdf">2020-NeurIPS-Learned Indexes for a Google-scale Disk-based Database</a></li><li> <a href="papers/2021-AIDB-PLEX_RS+CHT.pdf">2021-AIDB-PLEX_RS+CHT</a>: RadixSpine as the top + Compact Hist-Tree as the bottom</li><li> <a href="papers/2021-AIDB-RSS_Bounding_the_Last_Mile-Efficient_Learned_String_Indexing.pdf">2021-AIDB-RSS_Bounding_the_Last_Mile-Efficient_Learned_String_Indexing</a></li></ol><h5 id="Updatable"><a href="#Updatable" class="headerlink" title="Updatable"></a>Updatable</h5><ol><li> <a href="papers/2019-SIGMOD-FITing-Tree_A_Data-aware_Index_Structure.pdf">2019-SIGMOD-FITing-Tree_A Data-aware Index Structure</a></li><li> <a href="papers/2020-SIGMOD-ALEX_Updatable_Adaptive_Learned_Index.pdf">2020-SIGMOD-ALEX_Updatable_Adaptive_Learned_Index</a> Use gapped array for SMO</li><li> <a href="papers/2020-VLDB-PGM-index_fully-dynamic_compressed_worst-case_bounds.pdf">2020-VLDB-PGM-index_fully-dynamic_compressed_worst-case_bounds</a></li><li> <a href="papers/2021-PVLDB-LIPP_Updatable_Learned_Index_Precise_Positions.pdf">2021-PVLDB-LIPP_Updatable_Learned_Index_Precise_Positions</a></li><li> <a href="papers/2021-aiDM-RUSLI_Real-time_Updatable_Spline_Learned_Index.pdf">2021-aiDM-RUSLI_Real-time_Updatable_Spline_Learned_Index</a></li><li> <a href="papers/2022-TOS-The_Concurrent_Learned_Indexes_for_Multicore_Data_Storage.pdf">2022-TOS-Xindex-most-recent</a></li><li> <a href="papers/2023-FAST-ROLEX.pdf">2023-FAST-ROLEX</a></li><li> <a href="papers/2023-arXiv-DILI_A_Distribution-Driven_Learned_Index.pdf">2023-arxiv-DILI-A Distribution-Driven Learned Index</a> Using more bulk loading time for better lookup.</li></ol><h5 id="Secondary-Storage-Persistent-Memory-LSM"><a href="#Secondary-Storage-Persistent-Memory-LSM" class="headerlink" title="Secondary Storage/Persistent Memory/LSM"></a>Secondary Storage/Persistent Memory/LSM</h5><ol><li> <a href="papers/2019-arXiv-A_Scalable_Learned_Index_Scheme_in_Storage_Systems.pdf">2019-arXiv-A Scalable Learned Index Scheme in Storage Systems</a>: the initial version of FINEdex</li><li> <a href="papers/2020-NeurIPS-Learned_Indexes_for_a_Google-scale_Disk-based_Database.pdf">2020-NeurIPS-Learned Indexes for a Google-scale Disk-based Database</a></li><li> <a href="papers/2020-OSDI-Bourbon_learned_LSM.pdf">2020-OSDI-Bourbon_learned_LSM</a></li><li> <a href="papers/2020-OSDI-Bourbon_learned_LSM_slides.pdf">2020-OSDI-Bourbon_learned_LSM_slides</a></li><li> <a href="papers/2022-aiDM-LSI-Learned_Secondary_Index_Structure.pdf">2022-aiDM-LSI-Learned_Secondary_Index_Structure</a></li><li> <a href="papers/2022-VLDB-APEX_Learned_Index_PM.pdf">2022-VLDB-APEX_Learned_Index_PM</a></li><li> <a href="papers/2022-PVLDB-Endowment-FILM-for-Larger-than-Memory-Databases.pdf">2022-PVLDB-Endowment-FILM-for-Larger-than-Memory-Databases</a></li><li> <a href="papers/2023-arxiv-Updatable_Learned_Indexes_Disk-Resident_DBMS.pdf">2023-arxiv-Updatable_Learned_Indexes_Disk-Resident_DBMS</a></li></ol><h5 id="Radix-Spine-based"><a href="#Radix-Spine-based" class="headerlink" title="Radix-Spine based"></a>Radix-Spine based</h5><ol><li> <a href="papers/2020-aiDM-RadixSpline_A_Single-Pass_Learned_Index.pdf">2020-aiDM-Radix_Spline</a>: Using linear spine fits to a CDF, then a flat radix table as an appoximate index.</li><li> <a href="papers/2021-AIDB-RSS_Bounding_the_Last_Mile-Efficient_Learned_String_Indexing.pdf">2021-AIDB-RSS_Bounding_the_Last_Mile-Efficient_Learned_String_Indexing</a></li><li> <a href="papers/2021-AIDB-PLEX_RS+CHT.pdf">2021-AIDB-PLEX_RS+CHT</a></li><li> <a href="papers/2021-aiDM-RUSLI_Real-time_Updatable_Spline_Learned_Index.pdf">2021-aiDM-RUSLI_Real-time_Updatable_Spline_Learned_Index</a></li></ol><h5 id="Variable-length-string-keys"><a href="#Variable-length-string-keys" class="headerlink" title="Variable length string keys"></a>Variable length string keys</h5><ol><li> <a href="papers/2020-APSys-SIndex_A_Scalable_Learned_Index_for_String_Keys.pdf">2020-APSys-SIndex_Scalable_Learned_Index__String_Keys</a></li><li> <a href="papers/2021-AIDB-RSS_Bounding_the_Last_Mile-Efficient_Learned_String_Indexing.pdf">2021-AIDB-RSS_Bounding_the_Last_Mile-Efficient_Learned_String_Indexing</a></li><li> <a href="papers/2020-SIGMOD-Order-Preserving_Key_Compression_for_In-Memory_Search_Trees.pdf">2020-SIGMOD-HOPE</a>: not learned index, but an encoding schme; order persevering encoding for string; can be used for string learned indexes</li><li> <a href="papers/2020-SIGMOD-Order-Preserving_Key_Compression_for_In-Memory_Search_Trees_slides.pdf">2020-SIGMOD-HOPE_slides</a></li></ol><h5 id="Concurrency"><a href="#Concurrency" class="headerlink" title="Concurrency"></a>Concurrency</h5><ol><li> <a href="papers/2020-PPoPP-XIndex_A_Scalable_Learned_Index_for_Multicore_Data_Storage.pdf">2020-PPoPP-XIndex_Scalable_Learned_Index_for_Multicore_Data_Storage</a></li><li> <a href="papers/2020-APSys-SIndex_A_Scalable_Learned_Index_for_String_Keys.pdf">2020-APSys-SIndex_Scalable_Learned_Index_String_Keys</a></li><li> <a href="papers/2021-PVLDB-FINEdex-Fine-grained_for_Scalable_Concurrent_Memory_Systems.pdf">2021-PVLDB-FINEdex-Fine-grained_for_Scalable_Concurrent_Memory_Systems</a></li><li> <a href="papers/2022-TOS-The_Concurrent_Learned_Indexes_for_Multicore_Data_Storage.pdf">2022-TOS-Xindex-most-recent</a></li></ol><h5 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h5><ol><li> <a href="papers/2020-SIGMOD-The_Case_for_a_Learned_Sorting_Algorithm.pdf">2020-SIGMOD_The_Case_for_a_Learned_Sorting_Algorithm</a></li><li> <a href="papers/2022-PVLDB-Endowment-Can_Learned_Models_Replace_Hash_Functions.pdf">2022-PVLDB-Endowment-Can_Learned_Models_Replace_Hash_Functions </a></li><li> <a href="papers/2022-learned_Similarity_Search.pdf">2022-learned_Similarity_Search</a></li><li> <a href="papers/2023-ASPLOS-LeaFTL-leared_FTL_for_SSD.pdf">2023-ASPLOS-LeaFTL-Learning-Based Flash Translation Layer for Solid-State Drives</a> Learned index for SSD FTL page-level memory mapping.</li></ol><hr><h3 id="1-2-教程"><a href="#1-2-教程" class="headerlink" title="1.2 教程"></a>1.2 教程</h3><p><a href="https://blog.csdn.net/weixin_44026604/article/details/120776283">SIndex 论文笔记：A Scalable Learned Index for String Keys-CSDN博客</a></p><p><a href="https://zhuanlan.zhihu.com/p/277979207">OSDI20 - Bourbon: Learned Index for LSM - 知乎 (zhihu.com)</a></p>]]></content>
      
      
      <categories>
          
          <category> 论文整理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文整理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MyHexoBlog</title>
      <link href="posts/22ca.html"/>
      <url>posts/22ca.html</url>
      
        <content type="html"><![CDATA[<h2 id="MyHexoBlog"><a href="#MyHexoBlog" class="headerlink" title="MyHexoBlog"></a>MyHexoBlog</h2><p>首先，GitHub Pages 是由 GitHub 官方提供的一种免费的静态站点托管服务，让我们可以在 GitHub 仓库里托管和发布自己的静态网站页面。同时，Hexo 是一个快速、简洁且高效的静态博客框架，它基于 Node.js 运行，可以将我们撰写的 Markdown 文档解析渲染成静态的 HTML 网页。所以，我们可以先在本地撰写 Markdown 格式文章后，通过 Hexo 解析文档，渲染生成具有主题样式的 HTML 静态网页，再推送到 GitHub 上完成博文的发布。这样别人就能通过网址访问啦。</p><p><strong>优点：</strong>完全免费；静态站点，轻量快速；可按需求自由定制改造；托管在 GitHub，安全省心；迁移方便。</p><p><strong>不足：</strong>发文不便，依赖于本地环境；更适合个人博客使用；GitHub 在国内访问速度有点不快【gitee亦可】。</p><hr><h3 id="1-环境搭建"><a href="#1-环境搭建" class="headerlink" title="1. 环境搭建"></a>1. 环境搭建</h3><h4 id="下载安装-Node-js"><a href="#下载安装-Node-js" class="headerlink" title="下载安装 Node.js"></a>下载安装 Node.js</h4><p> 下载官网：<a href="https://nodejs.org/en">Node.js — Run JavaScript Everywhere (nodejs.org)</a>。 </p><p>安装参考教程：<a href="https://zhuanlan.zhihu.com/p/442215189">node.js安装及环境配置超详细教程【Windows系统安装包方式】 - 知乎 (zhihu.com)</a>。</p><p><strong>注意：可以变更缓存及包下载路径，但C盘够用，则不推荐变更</strong>。</p><p>① 打开安装的目录（默认安装情况下在C:\Program Files\nodejs）</p><pre class=" language-bash"><code class="language-bash">E:\EnvironmentSetting\NodeJs</code></pre><p>② 在安装目录下新建两个文件夹【node_global】和【node_cache】</p><p>③ 使用管理员权限打开 cmd 命令窗口，输入 <code>npm config set prefix “你的路径\node_global”</code> ：</p><pre class=" language-bash"><code class="language-bash">// 原位置<span class="token function">npm</span> config <span class="token keyword">set</span> prefix <span class="token string">"C:\Users\用户名\AppData\Roaming\npm\node_global"</span>// 新位置<span class="token function">npm</span> config <span class="token keyword">set</span> prefix <span class="token string">"E:\EnvironmentSetting\NodeJs\node_global"</span>// 查看位置<span class="token function">npm</span> root -g // 重置<span class="token function">npm</span> config delete prefix</code></pre><p>④ <code>npm config set cache “你的路径\node_cache” </code> ：</p><pre class=" language-bash"><code class="language-bash">// 原位置<span class="token function">npm</span> config <span class="token keyword">set</span> cache <span class="token string">"C:\Users\用户名\AppData\Roaming\node_cache"</span>// 新位置<span class="token function">npm</span> config <span class="token keyword">set</span> cache <span class="token string">"E:\EnvironmentSetting\NodeJs\node_cache"</span></code></pre><p>⑤设置环境变量，打开【系统属性】-【高级】-【环境变量】，在<code>系统变量</code>中新建</p><p>变量名：<code>NODE_PATH</code> </p><p>变量值：<code>E:\EnvironmentSetting\NodeJs\node_global\node_modules</code> </p><p>⑥ 编辑<code>用户变量（环境变量）</code>的 path，将默认的 C 盘下 <code>APPData\Roaming\npm</code> 修改成  </p><p><code>C:\Program Files\nodejs\node_global</code>，点击确定。</p><p>最后在系统 <code>Path</code>里面添加<code>%NODE_PATH%</code> 。执行 <code>npm root -g</code> 查看位置是否变更。</p><p>⑦ 测试：配置完成后，安装个module测试下，咱们就安装最经常使用的express模块，打开cmd窗口，输入以下命令进行模块的全局安装：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> express -g   // -g是全局安装的意思</code></pre><hr><h4 id="下载安装-Git"><a href="#下载安装-Git" class="headerlink" title="下载安装 Git"></a>下载安装 Git</h4><p>下载官网：<a href="https://git-scm.com/book/zh/v2/%E8%B5%B7%E6%AD%A5-%E5%AE%89%E8%A3%85-Git">Git - 安装 Git (git-scm.com)</a>。</p><p>安装参考教程：<a href="https://zhuanlan.zhihu.com/p/443527549">Git下载安装教程：git安装步骤手把手图文【超详细】 - 知乎 (zhihu.com)</a>。</p><p>安装参考教程：<a href="https://blog.csdn.net/mukes/article/details/115693833">Git 详细安装教程（详解 Git 安装过程的每一个步骤）_git安装-CSDN博客</a>。</p><p>安装完成后，Win+R 输入 cmd ，输入 <code>node -v</code>、<code>npm -v</code> 和 <code>git --version</code> 并回车，查看程序版本号。</p><hr><h4 id="配置-Github"><a href="#配置-Github" class="headerlink" title="配置 Github"></a>配置 Github</h4><p>使用邮箱注册 <a href="https://link.zhihu.com/?target=https://github.com/">GitHub</a> 账户，选择免费账户（Free），并完成邮件验证。</p><p>右键 -&gt; Git Bash Here，<strong>设置用户名和邮箱</strong>：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">git</span> config --global user.name <span class="token string">"GitHub 用户名"</span><span class="token function">git</span> config --global user.email <span class="token string">"GitHub 邮箱"</span><span class="token comment" spellcheck="true"># 查看用户名及邮箱</span><span class="token function">git</span> config user.name <span class="token function">git</span> config user.email</code></pre><p><strong>创建 SSH 密匙</strong>：</p><p>输入 <code>ssh-keygen -t rsa -C &quot;GitHub 邮箱&quot;</code>，然后一路回车。</p><pre class=" language-bash"><code class="language-bash">ssh-keygen -t rsa -C <span class="token string">"GitHub 邮箱"</span></code></pre><p><strong>添加密匙</strong>：</p><p>进入 [C:\Users\用户名.ssh] 目录（要勾选显示“隐藏的项目”），用记事本打开公钥 id_rsa.pub 文件并复制里面的内容。</p><p>登陆 GitHub ，进入 Settings 页面，选择左边栏的 SSH and GPG keys，点击 New SSH key。Title 随便取个名字，粘贴复制的 id_rsa.pub 内容到 Key 中，点击 Add SSH key 完成添加。</p><p><strong>验证连接</strong>：</p><p>打开 Git Bash，输入 <code>ssh -T git@github.com</code> 出现 “Are you sure……”，输入 yes 回车确认。</p><pre class=" language-bash"><code class="language-bash"><span class="token function">ssh</span> -T git@github.com</code></pre><p>显示 “Hi xxx! You’ve successfully……” 即连接成功。</p><h4 id="创建公有仓库"><a href="#创建公有仓库" class="headerlink" title="创建公有仓库"></a>创建公有仓库</h4><p>GitHub 主页右上角加号 -&gt; New repository：</p><ul><li>Repository name 中输入 <code>用户名.github.io</code> </li><li>勾选 “Initialize this repository with a README”</li><li>Description 选填</li></ul><p>填好后点击 Create repository 创建，创建后默认自动启用 HTTPS.</p><p>博客地址为：<code>https://用户名.github.io</code>。</p><p>gitee配置Page可参考博客：[在gitee上免费部署静态网站_在gitee上部署静态网站](<a href="https://blog.csdn.net/weixin_38705239/article/details/100161188#:~:text=%E4%BD%A0%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8">https://blog.csdn.net/weixin_38705239/article/details/100161188#:~:text=你可以使用</a> Gitee 来 部署静态 网页。 以下是详细的步骤： 1. 在,2. 将你的 静态 网页代码上传到该仓库中。 3. 在仓库的主页，点击右上角的 “Settings” 进入仓库设置。)。</p><hr><h3 id="2-Hexo"><a href="#2-Hexo" class="headerlink" title="2. Hexo"></a>2. Hexo</h3><p>新建一个文件夹用来存放 Hexo 程序文件，如 <code>MyHexoBlog</code>。cmd 进入该文件夹。</p><h4 id="2-1-安装-Hexo"><a href="#2-1-安装-Hexo" class="headerlink" title="2.1 安装 Hexo"></a>2.1 安装 Hexo</h4><p><strong>使用 npm 一键安装 Hexo 博客程序</strong>：【<strong>若失败，要么开启VPN，要么更换node的镜像源</strong>】</p><pre class=" language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> -g hexo-cli</code></pre><h4 id="2-2-初始化和本地预览"><a href="#2-2-初始化和本地预览" class="headerlink" title="2.2 初始化和本地预览"></a>2.2 初始化和本地预览</h4><p><strong>初始化并安装所需组件</strong>：</p><pre class=" language-bash"><code class="language-bash">hexo init      <span class="token comment" spellcheck="true"># 初始化</span><span class="token function">npm</span> <span class="token function">install</span>    <span class="token comment" spellcheck="true"># 安装组件</span></code></pre><p>若报<code>hexo</code> 未识别，可参考该篇博客：<a href="https://blog.csdn.net/Deng872347348/article/details/121646375">安装hexo时出现的问题：‘hexo‘ 不是内部或外部命令。</a>。</p><p>完成后依次输入下面命令，<strong>启动本地服务器进行预览</strong>：</p><pre class=" language-bash"><code class="language-bash">hexo g   <span class="token comment" spellcheck="true"># 生成页面</span>hexo s   <span class="token comment" spellcheck="true"># 启动预览</span></code></pre><p><strong>访问</strong> <code>http://localhost:4000</code>，出现 Hexo 默认页面，本地博客安装成功！</p><p><strong>Tips：</strong>如果出现页面加载不出来，可能是端口被占用了。Ctrl+C 关闭服务器，运行 <code>hexo server -p 5000</code> 更改端口号后重试。Hexo 博客文件夹目录结构如下：</p><p><img src="https://pic1.zhimg.com/80/v2-264c75c0e493ae8cc5f283567c64ff2c_720w.webp"></p><h4 id="2-3-部署到-GitHub-Pages"><a href="#2-3-部署到-GitHub-Pages" class="headerlink" title="2.3 部署到 GitHub Pages"></a>2.3 部署到 GitHub Pages</h4><p>本地博客测试成功后，就是上传到 GitHub 进行部署，使其能够在网络上访问。</p><p>首先<strong>安装 hexo-deployer-git</strong>：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> hexo-deployer-git --save</code></pre><p>然后<strong>修改 _config.yml</strong> 文件末尾的 Deployment 部分，修改成如下：</p><pre class=" language-bash"><code class="language-bash">deploy:  type: <span class="token function">git</span>  repository: git@github.com:flyboy716/flyboy716.github.io.git  branch: master</code></pre><p>完成后运行 <code>hexo d</code> 将网站上传部署到 GitHub Pages。</p><pre class=" language-bash"><code class="language-bash">hexo d</code></pre><p>完成！这时访问我们的 GitHub 域名 <code>https://用户名.github.io</code> 就可以看到 Hexo 网站了。</p><hr><h4 id="2-4-绑定域名-可选"><a href="#2-4-绑定域名-可选" class="headerlink" title="2.4 绑定域名-可选"></a>2.4 绑定域名-可选</h4><p>博客搭建完成使用的是 GitHub 的子域名（用户名.<a href="https://link.zhihu.com/?target=http://github.io">http://github.io</a>），我们可以为 Hexo 博客绑定自己的域名替换 GitHub 域名，更加个性化和专业，也利于 SEO。</p><p>我们使用 <a href="https://link.zhihu.com/?target=https://www.namesilo.com/?rid=d27fa32do">Namesilo</a> 进行注册，便宜好用没啥套路，使用优惠码 <code>okoff</code> 优惠一美元，com 域名大概 50 块一年。</p><p><strong>域名注册和解析</strong>：</p><ul><li>域名注册和解析教程：<a href="https://zhuanlan.zhihu.com/p/33921436">Namesilo 域名购买及使用教程</a></li></ul><p>按上面教程注册并解析域名，在 DNS 设置部分，删除自带的记录，然后添加 CNAME 记录将 www 域名解析指向 <code>用户名.github.io</code>。</p><p><strong>绑定域名到 Hexo 博客</strong>：</p><p>进入本地博客文件夹的 source 目录，打开记事本，里面输入自己的域名，如 <a href="http://www.example.com,保存名称为/">http://www.example.com，保存名称为</a> “CNAME”，格式为 “所有文件”（无 .txt 后缀）。</p><p>清除缓存等文件并重新发布网站：</p><pre class=" language-bash"><code class="language-bash">hexo clean   <span class="token comment" spellcheck="true"># 清除缓存文件等</span>hexo g       <span class="token comment" spellcheck="true"># 生成页面</span>hexo s       <span class="token comment" spellcheck="true"># 启动预览</span></code></pre><p>现在就可以使用自己的域名访问 Hexo 博客了。</p><p><strong>开启 HTTPS</strong>：配置自己的域名后，需要我们手动开启 HTTPS。打开博客所在 GitHub 仓库，Settings -&gt; 下拉找到 GitHub Pages -&gt; 勾选 Enforce HTTPS。</p><p>HTTPS 证书部署成功需要一定时间，等大概几分钟再访问域名，就可以看到域名前面的小绿锁了，HTTPS 配置完成！</p><hr><h4 id="2-5-开始使用"><a href="#2-5-开始使用" class="headerlink" title="2.5 开始使用"></a>2.5 开始使用</h4><p><strong>发布文章</strong>：</p><p>进入博客所在目录，右键打开 Git Bash Here，创建博文：</p><pre class=" language-bash"><code class="language-bash">hexo new <span class="token string">"My New Post"</span></code></pre><p>然后 source 文件夹中会出现一个 My New Post.md 文件，就可以使用 Markdown 编辑器在该文件中撰写文章了。</p><p>写完后运行下面代码将文章渲染并部署到 GitHub Pages 上完成发布。<strong>以后每次发布文章都是这两条命令</strong>。</p><pre class=" language-bash"><code class="language-bash">hexo g   <span class="token comment" spellcheck="true"># 生成页面</span>hexo d   <span class="token comment" spellcheck="true"># 部署发布</span></code></pre><hr><p>也可以不使用命令自己创建 .md 文件，只需在文件开头手动加入如下格式 Front-matter 即可，写完后运行 <code>hexo g</code> 和 <code>hexo d</code> 发布。</p><pre class=" language-bash"><code class="language-bash">---title: Hello World <span class="token comment" spellcheck="true"># 标题</span>date: 2019/3/26 hh:mm:ss <span class="token comment" spellcheck="true"># 时间</span>categories: <span class="token comment" spellcheck="true"># 分类</span>- Diarytags: <span class="token comment" spellcheck="true"># 标签</span>- PS3- Games---摘要<span class="token operator">&lt;</span><span class="token operator">!</span>--more--<span class="token operator">></span>正文</code></pre><p><strong>网站设置</strong>：</p><p>包括网站名称、描述、作者、链接样式等，全部在网站目录下的 _config.yml 文件中，参考<a href="https://link.zhihu.com/?target=https://hexo.io/zh-cn/docs/configuration">官方文档</a>按需要编辑。注意：冒号后要加一个空格！</p><p><strong>更换主题</strong>：</p><p>在 <a href="https://link.zhihu.com/?target=https://hexo.io/themes/">Themes | Hexo</a> 选择一个喜欢的主题，比如 <a href="https://link.zhihu.com/?target=http://theme-next.iissnan.com/getting-started.html">NexT</a>，进入网站目录打开 Git Bash Here 下载主题：</p><pre class=" language-text"><code class="language-text">git clone https://github.com/iissnan/hexo-theme-next themes/next</code></pre><p>然后修改 _config.yml 中的 theme 为新主题名称 next，发布。（有的主题需要将 _config.yml 替换为主题自带的，参考主题说明。）</p><h4 id="2-6-常用命令"><a href="#2-6-常用命令" class="headerlink" title="2.6 常用命令"></a>2.6 常用命令</h4><pre class=" language-text"><code class="language-text">hexo new "name"       # 新建文章hexo new page "name"  # 新建页面hexo g                # 生成页面hexo d                # 部署hexo g -d             # 生成页面并部署hexo s                # 本地预览hexo clean            # 清除缓存和已生成的静态文件hexo help             # 帮助</code></pre><h4 id="2-7-常见问题"><a href="#2-7-常见问题" class="headerlink" title="2.7 常见问题"></a>2.7 常见问题</h4><p><strong>1、Hexo 设置显示文章摘要，首页不显示全文</strong>。</p><p>Hexo 主页文章列表默认会显示文章全文，浏览时很不方便，可以在文章中插入 <code>&lt;!--more--&gt;</code> 进行分段。</p><p>该代码前面的内容会作为摘要显示，而后面的内容会替换为 “Read More” 隐藏起来。</p><p><strong>2、设置网站图标</strong>。</p><p>进入 themes/主题 文件夹，打开 _config.yml 配置文件，找到 favicon 修改，一般格式为：<code>favicon: 图标地址</code>。（不同主题可能略有差别）</p><p><strong>3、修改并部署后没有效果</strong>。</p><p>使用 <code>hexo clean</code> 清理后重新部署。</p><p><strong>4、开启 HTTPS 后访问网站显示连接不安全？</strong></p><p>证书还未部署生效，等待一会儿，清除浏览器缓存再试。</p><p><strong>5、Mac 安装 Hexo 报错无法安装</strong>。</p><p>Mac 用户需要管理员权限运行，使用 <code>sudo npm install -g hexo-cli</code> 命令安装。</p><p><strong>6、npm 下载速度慢，甚至完全没反应</strong>。</p><p>使用 npm 安装程序等待很久也没反应，或者下载速度很慢，可以更换 npm 源为国内 npm 镜像。</p><p>临时更换方法：在 npm 安装命令后面加上：</p><pre class=" language-text"><code class="language-text">--registry https://registry.npm.taobao.org </code></pre><hr><h4 id="2-8-结语"><a href="#2-8-结语" class="headerlink" title="2.8 结语"></a>2.8 结语</h4><p>Hexo 是一种纯静态的博客，我们必须要在本地完成文章的编辑再部署到 GitHub 上，依赖于本地环境。不能像 WordPress 或 Typecho 那样的动态博客一样能直接在浏览器中完成撰文和发布。</p><p>可以说是一种比较极客的写博客方式，但是优势也是明显的——免费稳定省心，比较适合爱折腾研究的用户，或者没有在线发文需求的朋友。</p><hr><h3 id="3-主题"><a href="#3-主题" class="headerlink" title="3. 主题"></a>3. 主题</h3><h4 id="3-1-Matery"><a href="#3-1-Matery" class="headerlink" title="3.1 Matery"></a>3.1 Matery</h4><p>官方参考地址：<a href="https://github.com/blinkfox/hexo-theme-matery/blob/develop/README_CN.md">hexo-theme-matery/README_CN.md</a>。</p><p><a href="https://blinkfox.github.io/2018/09/28/qian-duan/hexo-bo-ke-zhu-ti-zhi-hexo-theme-matery-de-jie-shao/#searchModal">Hexo博客主题之hexo-theme-matery的介绍 | 闪烁之狐 </a>。</p><hr>]]></content>
      
      
      <categories>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 博客 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
